%___________________________________________________________________________________________
%
 \section{Funci\'on Generadora de Probabilidades}
%___________________________________________________________________________________________

\begin{Teo}[Teorema de Continuidad]
Sup\'ongase que $\left\{X_{n},n=1,2,3,\ldots\right\}$ son variables aleatorias finitas, no negativas con valores enteros tales que $P\left(X_{n}=k\right)=p_{k}^{(n)}$, para $n=1,2,3,\ldots$, $k=0,1,2,\ldots$, con $\sum_{k=0}^{\infty}p_{k}^{(n)}=1$, para $n=1,2,3,\ldots$. Sea $g_{n}$ la PGF para la variable aleatoria $X_{n}$. Entonces existe una sucesi\'on $\left\{p_{k}\right\}$ tal que \begin{eqnarray*}
lim_{n\rightarrow\infty}p_{k}^{(n)}=p_{k}\textrm{ para }0<s<1.
\end{eqnarray*}

En este caso, $g\left(s\right)=\sum_{k=0}^{\infty}s^{k}p_{k}$. Adem\'as
\begin{eqnarray*}
\sum_{k=0}^{\infty}p_{k}=1\textrm{ si y s\'olo si
}lim_{s\uparrow1}g\left(s\right)=1
\end{eqnarray*}
\end{Teo}

\begin{Teo}
Sea $N$ una variable aleatoria con valores enteros no negativos finita tal que $P\left(N=k\right)=p_{k}$, para $k=0,1,2,\ldots$, y $\sum_{k=0}^{\infty}p_{k}=P\left(N<\infty\right)=1$. Sea $\Phi$ la PGF de $N$ tal que
$g\left(s\right)=\esp\left[s^{N}\right]=\sum_{k=0}^{\infty}s^{k}p_{k}$ con $g\left(1\right)=1$. Si $0\leq p_{1}\leq1$ y $\esp\left[N\right]=g^{'}\left(1\right)\leq1$, entonces no existe soluci\'on  de la ecuaci\'on $g\left(s\right)=s$ en el intervalo $\left[0,1\right)$. Si $\esp\left[N\right]=g^{'}\left(1\right)>1$, lo cual implica que $0\leq p_{1}<1$, entonces existe una \'unica soluci\'on de la ecuaci\'on $g\left(s\right)=s$ en el intervalo $\left[0,1\right)$.
\end{Teo}


\begin{Teo}
Si $X$ y $Y$ tienen PGF $G_{X}$ y $G_{Y}$ respectivamente, entonces,\[G_{X}\left(s\right)=G_{Y}\left(s\right)\] para tod $s$, si y s\'olo si \[P\left(X=k\right))=P\left(Y=k\right)\] para toda $k=0,1,\ldots,$., es decir, si y s\'olo si $X$ y $Y$ tienen la misma distribuci\'on de probabilidad.
\end{Teo}


\begin{Teo}
Para cada $n$ fijo, sea la sucesi\'oin de probabilidades $\left\{a_{0,n},a_{1,n},\ldots,\right\}$, tales que $a_{k,n}\geq0$ para toda $k=0,1,2,\ldots,$ y $\sum_{k\geq0}a_{k,n}=1$, y sea $G_{n}\left(s\right)$ la correspondiente funci\'on generadora, $G_{n}\left(s\right)=\sum_{k\geq0}a_{k,n}s^{k}$. De modo que para cada valor fijo de $k$
\begin{eqnarray*}
lim_{n\rightarrow\infty}a_{k,n}=a_{k},
\end{eqnarray*}
es decir converge en distribuci\'on, es necesario y suficiente que para cada valor fijo $s\in\left[0,\right)$,

\begin{eqnarray*}
lim_{n\rightarrow\infty}G_{n}\left(s\right)=G\left(s\right),
\end{eqnarray*}
donde $G\left(s\right)=\sum_{k\geq0}p_{k}s^{k}$, para cualquier la funci\'on generadora del l\'imite de la sucesi\'on.
\end{Teo}

\begin{Teo}[Teorema de Abel]
Sea $G\left(s\right)=\sum_{k\geq0}a_{k}s^{k}$ para cualquier $\left\{p_{0},p_{1},\ldots,\right\}$, tales que $p_{k}\geq0$ para toda $k=0,1,2,\ldots,$. Entonces $G\left(s\right)$ es continua por la derecha en $s=1$, es decir
\begin{eqnarray*}
lim_{s\uparrow1}G\left(s\right)=\sum_{k\geq0}p_{k}=G\left(\right),
\end{eqnarray*}
sin importar si la suma es finita o no.
\end{Teo}

\begin{Note}
El radio de Convergencia para cualquier PGF es $R\geq1$, entonces, el Teorema de Abel nos dice que a\'un en el peor escenario, cuando $R=1$, a\'un se puede confiar en que la PGF ser\'a continua en $s=1$, en contraste, no se puede asegurar que la PGF ser\'a continua en el l\'imite inferior $-R$, puesto que la PGF es sim\'etrica alrededor del cero: la PGF converge para todo $s\in\left(-R,R\right)$, y no lo hace para $s<-R$ o $s>R$. Adem\'as nos dice que podemos escribir $G_{X}\left(1\right)$ como una abreviaci\'on de $lim_{s\uparrow1}G_{X}\left(s\right)$.
\end{Note}

Entonces si suponemos que la diferenciaci\'on t\'ermino a t\'ermino est\'a permitida, entonces

\begin{eqnarray*}
G_{X}^{'}\left(s\right)&=&\sum_{x=1}^{\infty}xs^{x-1}p_{x}
\end{eqnarray*}

el Teorema de Abel nos dice que
\begin{eqnarray*}
\esp\left(X\right]&=&\lim_{s\uparrow1}G_{X}^{'}\left(s\right):\\
\esp\left[X\right]&=&=\sum_{x=1}^{\infty}xp_{x}=G_{X}^{'}\left(1\right)\\
&=&\lim_{s\uparrow1}G_{X}^{'}\left(s\right),
\end{eqnarray*}
dado que el Teorema de Abel se aplica a
\begin{eqnarray*}
G_{X}^{'}\left(s\right)&=&\sum_{x=1}^{\infty}xs^{x-1}p_{x},
\end{eqnarray*}
estableciendo as\'i que $G_{X}^{'}\left(s\right)$ es continua en $s=1$. Sin el Teorema de Abel no se podr\'ia asegurar que el l\'imite de $G_{X}^{'}\left(s\right)$ conforme $s\uparrow1$ sea la respuesta correcta para $\esp\left[X\right]$.

\begin{Note}
La PGF converge para todo $|s|<R$, para alg\'un $R$. De hecho la PGF converge absolutamente si $|s|<R$. La PGF adem\'as converge uniformemente en conjuntos de la forma $\left\{s:|s|<R^{'}\right\}$, donde $R^{'}<R$, es decir, $\forall\epsilon>0, \exists n_{0}\in\ent$ tal que $\forall s$, con $|s|<R^{'}$, y $\forall n\geq n_{0}$,
\begin{eqnarray*}
|\sum_{x=0}^{n}s^{x}\prob\left(X=x\right)-G_{X}\left(s\right)|<\epsilon.
\end{eqnarray*}
De hecho, la convergencia uniforme es la que nos permite diferenciar t\'ermino a t\'ermino:
\begin{eqnarray*}
G_{X}\left(s\right)=\esp\left[s^{X}\right]=\sum_{x=0}^{\infty}s^{x}\prob\left(X=x\right),
\end{eqnarray*}
y sea $s<R$.
\begin{enumerate}
\item
\begin{eqnarray*}
G_{X}^{'}\left(s\right)&=&\frac{d}{ds}\left(\sum_{x=0}^{\infty}s^{x}\prob\left(X=x\right)\right)=\sum_{x=0}^{\infty}\frac{d}{ds}\left(s^{x}\prob\left(X=x\right)\right)\\
&=&\sum_{x=0}^{n}xs^{x-1}\prob\left(X=x\right).
\end{eqnarray*}

\item\begin{eqnarray*}
\int_{a}^{b}G_{X}\left(s\right)ds&=&\int_{a}^{b}\left(\sum_{x=0}^{\infty}s^{x}\prob\left(X=x\right)\right)ds=\sum_{x=0}^{\infty}\left(\int_{a}^{b}s^{x}\prob\left(X=x\right)ds\right)\\
&=&\sum_{x=0}^{\infty}\frac{s^{x+1}}{x+1}\prob\left(X=x\right),
\end{eqnarray*}
para $-R<a<b<R$.
\end{enumerate}
\end{Note}

\begin{Teo}[Teorema de Convergencia Mon\'otona para PGF] Sean $X$ y $X_{n}$ variables aleatorias no negativas, con valores en los enteros, finitas, tales que
\begin{eqnarray*}
lim_{n\rightarrow\infty}G_{X_{n}}\left(s\right)&=&G_{X}\left(s\right)
\end{eqnarray*}
para $0\leq s\leq1$, entonces
\begin{eqnarray*}
lim_{n\rightarrow\infty}P\left(X_{n}=k\right)=P\left(X=k\right),
\end{eqnarray*}
para $k=0,1,2,\ldots.$
\end{Teo}

El teorema anterior requiere del siguiente lema

\begin{Lemma}
Sean $a_{n,k}\in\ent^{+}$, $n\in\nat$ constantes no negativas con
$\sum_{k\geq0}a_{k,n}\leq1$. Sup\'ongase que para $0\leq s\leq1$,
se tiene
\begin{eqnarray*}
a_{n}\left(s\right)&=&\sum_{k=0}^{\infty}a_{k,n}s^{k}\rightarrow
a\left(s\right)=\sum_{k=0}^{\infty}a_{k}s^{k}.
\end{eqnarray*}
Entonces
\begin{eqnarray*}
a_{0,n}\rightarrow a_{0}.
\end{eqnarray*}
\end{Lemma}

Consideremos un sistema que consta de \'unicamente un servidor y una sola cola, a la cual los usuarios arriban conforme a un proceso poisson cuya tasa promedio de llegada es $1/\lambda$; la tasa promedio con la cual el servidor da servicio es $1/\mu$, adem\'as los tiempos entre arribos y los tiempos de servicio son independientes entre s\'i.

Se define la carga de tr\'afico $\rho:=\frac{\lambda}{\mu}$, para este modelo existe un teorema que nos dice la relaci\'on que hay entre el valor de $\rho$ y la estabilidad de la cola:

\begin{Prop}
La cola $M/M/1$ con carga de tr\'afico $\rho$, es estable si y s\'olo si $\rho<1$.
\end{Prop}

Este teorema nos permite determinar las principales medidas de desempe\~no: Tiempo de espera en el sistema, $W$, el n\'umero esperado de clientes en el sistema, $L$, adem\'as de los tiempos promedio e espera tanto en la cola como de servicio, $s$ representa el tiempo de servicio para un cliente:

\begin{eqnarray}
 L&=&\frac{\rho}{1-\rho},\\
W&=&\frac{1}{\mu-\lambda},\\
W_{q}&=&\esp\left[s\right]\frac{\rho}{1-\rho}\textrm{,  y }\\
L_{q}&=&\frac{\rho^{2}}{1-\rho}.
\end{eqnarray}

Esta es la idea general, poder determinar la principales medidas de desempe\~no para un sistema de colas o sistema de visitas, para este fin es necesario realizar los siguientes supuestos. En teor\'ia de colas hay casos particulares, para los cuales es posible determinar espec\'ificamente medidas de desempe\~no del sistema bajo condiciones de estabilidad, tales como los tiempos promedio de espera y de servicio, tanto en el sistema como en cada
una de las colas.


En teor\'ia de colas hay casos particulares, para los cuales es posible determinar espec\'ificamente medidas de desempe\~no del sistema bajo condiciones de estabilidad, tales como los tiempos promedio de espera y de servicio, tanto en el sistema como en cada
una de las colas. Se considerar\'an intervalos de tiempo de la forma $\left[t,t+1\right]$. Los usuarios arriban por paquetes de manera independiente del resto de las colas. Se define el grupo de usuarios que llegan a cada una de las colas del sistema 1, caracterizadas por $Q_{1}$ y $Q_{2}$ respectivamente, en el intervalo de tiempo $\left[t,t+1\right]$ por $X_{1}\left(t\right),X_{2}\left(t\right)$.

Para cada uno de los procesos anteriores se define su Funci\'on Generadora de Probabilidades (PGF):

\begin{eqnarray*}
\begin{array}{cc}
P_{1}\left(z_{1}\right)=\esp\left[z_{1}^{X_{1}\left(t\right)}\right], & P_{2}\left(z_{2}\right)=\esp\left[z_{2}^{X_{2}\left(t\right)}\right].\\
\end{array}
\end{eqnarray*}

Con primer momento definidos por

\begin{eqnarray*}
\mu_{1}&=&\esp\left[X_{1}\left(t\right)\right]=P_{1}^{(1)}\left(1\right),\\
\mu_{2}&=&\esp\left[X_{2}\left(t\right)\right]=P_{2}^{(1)}\left(1\right).
\end{eqnarray*}


En lo que respecta al servidor, en t\'erminos de los tiempos de visita a cada una de las colas, se denotar\'an por $\tau_{1},\tau_{2}$ para $Q_{1},Q_{2}$ respectivamente; y a los
tiempos en que el servidor termina de atender en las colas $Q_{1},Q_{2}$, se les denotar\'a por $\overline{\tau}_{1},\overline{\tau}_{2}$ respectivamente. Entonces, los tiempos de servicio est\'an dados por las diferencias $\overline{\tau}_{1}-\tau_{1},\overline{\tau}_{2}-\tau_{2}$ para $Q_{1},Q_{2}$. An\'alogamente los tiempos de traslado del servidor desde el momento en que termina de atender a una cola y llega a la siguiente para comenzar a dar servicio est\'an dados por $\tau_{2}-\overline{\tau}_{1},\tau_{1}-\overline{\tau}_{2}$.


La FGP para estos tiempos de traslado est\'an dados por

\begin{eqnarray*}
\begin{array}{cc}
R_{1}\left(z_{1}\right)=\esp\left[z_{1}^{\tau_{2}-\overline{\tau}_{1}}\right],
&
R_{2}\left(z_{2}\right)=\esp\left[z_{2}^{\tau_{1}-\overline{\tau}_{2}}\right],
\end{array}
\end{eqnarray*}

y al igual que como se hizo con anterioridad

\begin{eqnarray*}
\begin{array}{cc}
r_{1}=R_{1}^{(1)}\left(1\right)=\esp\left[\tau_{2}-\overline{\tau}_{1}\right],
&
r_{2}=R_{2}^{(1)}\left(1\right)=\esp\left[\tau_{1}-\overline{\tau}_{2}\right],\\
\end{array}
\end{eqnarray*}
Sean $\alpha_{1},\alpha_{2}$ el n\'umero de usuarios que arriban
en grupo a la cola $Q_{1}$ y $Q_{2}$ respectivamente. Sus PGF's
est\'an definidas como

\begin{eqnarray*}
\begin{array}{cc}
A_{1}\left(z\right)=\esp\left[z^{\alpha_{1}\left(t\right)}\right],&
A_{2}\left(z\right)=\esp\left[z^{\alpha_{2}\left(t\right)}\right].\\
\end{array}
\end{eqnarray*}

Su primer momento est\'a dado por

\begin{eqnarray*}
\begin{array}{cc}
\lambda_{1}=\esp\left[\alpha_{1}\left(t\right)\right]=A_{1}^{(1)}\left(1\right),&
\lambda_{2}=\esp\left[\alpha_{2}\left(t\right)\right]=A_{2}^{(1)}\left(1\right).\\
\end{array}
\end{eqnarray*}

Sean $\beta_{1},\beta_{2}$ el n\'umero de usuarios que arriban en el grupo $\alpha_{1},\alpha_{2}$ a la cola $Q_{1}$ y $Q_{2}$, respectivamente, de igual manera se definen sus PGF's

\begin{eqnarray*}
\begin{array}{cc}
B_{1}\left(z\right)=\esp\left[z^{\beta_{1}\left(t\right)}\right],&
B_{2}\left(z\right)=\esp\left[z^{\beta_{2}\left(t\right)}\right],\\
\end{array}
\end{eqnarray*}

con

\begin{eqnarray*}
\begin{array}{cc}
b_{1}=\esp\left[\beta_{1}\left(t\right)\right]=B_{1}^{(1)}\left(1\right),&
b_{2}=\esp\left[\beta_{2}\left(t\right)\right]=B_{2}^{(1)}\left(1\right).\\
\end{array}
\end{eqnarray*}

La distribuci\'on para el n\'umero de grupos que arriban al sistema en cada una de las colas se definen por:

\begin{eqnarray*}
\begin{array}{cc}
P_{1}\left(z_{1}\right)=A_{1}\left[B_{1}\left(z_{1}\right)\right]=\esp\left[B_{1}\left(z_{1}\right)^{\alpha_{1}\left(t\right)}\right],& P_{2}\left(z_{1}\right)=A_{1}\left[B_{1}\left(z_{1}\right)\right]=\esp\left[B_{1}\left(z_{1}\right)^{\alpha_{1}\left(t\right)}\right],\\
\end{array}
\end{eqnarray*}

entonces

\begin{eqnarray*}
P_{1}^{(1)}\left(1\right)&=&\esp\left[\alpha_{1}\left(t\right)B_{1}^{(1)}\left(1\right)\right]=B_{1}^{(1)}\left(1\right)\esp\left[\alpha_{1}\left(t\right)\right]=\lambda_{1}b_{1}\\
P_{2}^{(1)}\left(1\right)&=&\esp\left[\alpha_{2}\left(t\right)B_{2}^{(1)}\left(1\right)\right]=B_{2}^{(1)}\left(1\right)\esp\left[\alpha_{2}\left(t\right)\right]=\lambda_{2}b_{2}.\\
\end{eqnarray*}

De lo desarrollado hasta ahora se tiene lo siguiente

\begin{eqnarray*}
&&\esp\left[z_{1}^{L_{1}\left(\overline{\tau}_{1}\right)}z_{2}^{L_{2}\left(\overline{\tau}_{1}\right)}\right]=\esp\left[z_{2}^{L_{2}\left(\overline{\tau}_{1}\right)}\right]=\esp\left[z_{2}^{L_{2}\left(\tau_{1}\right)+X_{2}\left(\overline{\tau}_{1}-\tau_{1}\right)}\right]\\
&=&\esp\left[\left\{z_{2}^{L_{2}\left(\tau_{1}\right)}\right\}\left\{z_{2}^{X_{2}\left(\overline{\tau}_{1}-\tau_{1}\right)}\right\}\right]=\esp\left[\left\{z_{2}^{L_{2}\left(\tau_{1}\right)}\right\}\left\{P_{2}\left(z_{2}\right)\right\}^{\overline{\tau}_{1}-\tau_{1}}\right]\\
&=&\esp\left[\left\{z_{2}^{L_{2}\left(\tau_{1}\right)}\right\}\left\{\theta_{1}\left(P_{2}\left(z_{2}\right)\right)\right\}^{L_{1}\left(\tau_{1}\right)}\right]=F_{1}\left(\theta_{1}\left(P_{2}\left(z_{2}\right)\right),z_{2}\right)
\end{eqnarray*}

es decir 
\begin{equation}\label{Eq.base.F1}
\esp\left[z_{1}^{L_{1}\left(\overline{\tau}_{1}\right)}z_{2}^{L_{2}\left(\overline{\tau}_{1}\right)}\right]=F_{1}\left(\theta_{1}\left(P_{2}\left(z_{2}\right)\right),z_{2}\right).
\end{equation}

Procediendo de manera an\'aloga para $\overline{\tau}_{2}$:

\begin{eqnarray*}
\esp\left[z_{1}^{L_{1}\left(\overline{\tau}_{2}\right)}z_{2}^{L_{2}\left(\overline{\tau}_{2}\right)}\right]&=&\esp\left[z_{1}^{L_{1}\left(\overline{\tau}_{2}\right)}\right]=\esp\left[z_{1}^{L_{1}\left(\tau_{2}\right)+X_{1}\left(\overline{\tau}_{2}-\tau_{2}\right)}\right]=\esp\left[\left\{z_{1}^{L_{1}\left(\tau_{2}\right)}\right\}\left\{z_{1}^{X_{1}\left(\overline{\tau}_{2}-\tau_{2}\right)}\right\}\right]\\
&=&\esp\left[\left\{z_{1}^{L_{1}\left(\tau_{2}\right)}\right\}\left\{P_{1}\left(z_{1}\right)\right\}^{\overline{\tau}_{2}-\tau_{2}}\right]=\esp\left[\left\{z_{1}^{L_{1}\left(\tau_{2}\right)}\right\}\left\{\theta_{2}\left(P_{1}\left(z_{1}\right)\right)\right\}^{L_{2}\left(\tau_{2}\right)}\right]\\
&=&F_{2}\left(z_{1},\theta_{2}\left(P_{1}\left(z_{1}\right)\right)\right)
\end{eqnarray*}
por tanto
\begin{equation}\label{Eq.PGF.Conjunta.Tau2}
\esp\left[z_{1}^{L_{1}\left(\overline{\tau}_{2}\right)}z_{2}^{L_{2}\left(\overline{\tau}_{2}\right)}\right]=F_{2}\left(z_{1},\theta_{2}\left(P_{1}\left(z_{1}\right)\right)\right)
\end{equation}

Ahora, para el intervalo de tiempo
$\left[\overline{\tau}_{1},\tau_{2}\right]$ y $\left[\overline{\tau}_{2},\tau_{1}\right]$, los arribos de los usuarios modifican el n\'umero de usuarios que llegan a las colas, es decir, los procesos
$L_{1}\left(t\right)$ y $L_{2}\left(t\right)$. La PGF para el n\'umero de arribos a todas las estaciones durante el intervalo $\left[\overline{\tau}_{1},\tau_{2}\right]$  cuya distribuci\'on est\'a especificada por la distribuci\'on compuesta $R_{1}\left(\mathbf{z}\right),R_{2}\left(\mathbf{z}\right)$:

\begin{eqnarray*}
R_{1}\left(\mathbf{z}\right)=R_{1}\left(\prod_{i=1}^{2}P\left(z_{i}\right)\right)=\esp\left[\left\{\prod_{i=1}^{2}P\left(z_{i}\right)\right\}^{\tau_{2}-\overline{\tau}_{1}}\right]\\
R_{2}\left(\mathbf{z}\right)=R_{2}\left(\prod_{i=1}^{2}P\left(z_{i}\right)\right)=\esp\left[\left\{\prod_{i=1}^{2}P\left(z_{i}\right)\right\}^{\tau_{1}-\overline{\tau}_{2}}\right]\\
\end{eqnarray*}

Dado que los eventos en
$\left[\tau_{1},\overline{\tau}_{1}\right]$ y $\left[\overline{\tau}_{1},\tau_{2}\right]$ son independientes, la
PGF conjunta para el n\'umero de usuarios en el sistema al tiempo $t=\tau_{2}$ la PGF conjunta para el n\'umero de usuarios en el sistema est\'an dadas por

\begin{eqnarray*}
F_{1}\left(\mathbf{z}\right)&=&R_{2}\left(\prod_{i=1}^{2}P\left(z_{i}\right)\right)F_{2}\left(z_{1},\theta_{2}\left(P_{1}\left(z_{1}\right)\right)\right)\\
F_{2}\left(\mathbf{z}\right)&=&R_{1}\left(\prod_{i=1}^{2}P\left(z_{i}\right)\right)F_{1}\left(\theta_{1}\left(P_{2}\left(z_{2}\right)\right),z_{2}\right)\\
\end{eqnarray*}

Entonces debemos de determinar las siguientes expresiones:

\begin{eqnarray*}
\begin{array}{cc}
f_{1}\left(1\right)=\frac{\partial F_{1}\left(\mathbf{z}\right)}{\partial z_{1}}|_{\mathbf{z}=1}, & f_{1}\left(2\right)=\frac{\partial F_{1}\left(\mathbf{z}\right)}{\partial z_{2}}|_{\mathbf{z}=1},\\
f_{2}\left(1\right)=\frac{\partial F_{2}\left(\mathbf{z}\right)}{\partial z_{1}}|_{\mathbf{z}=1}, & f_{2}\left(2\right)=\frac{\partial F_{2}\left(\mathbf{z}\right)}{\partial z_{2}}|_{\mathbf{z}=1},\\
\end{array}
\end{eqnarray*}

calculando las derivadas parciales 
\begin{eqnarray*}
\frac{\partial R_{1}\left(\mathbf{z}\right)}{\partial
z_{1}}|_{\mathbf{z}=1}&=&R_{1}^{(1)}\left(1\right)P_{1}^{(1)}\left(1\right)\\
\frac{\partial R_{1}\left(\mathbf{z}\right)}{\partial
z_{2}}|_{\mathbf{z}=1}&=&R_{1}^{(1)}\left(1\right)P_{2}^{(1)}\left(1\right)\\
\frac{\partial R_{2}\left(\mathbf{z}\right)}{\partial
z_{1}}|_{\mathbf{z}=1}&=&R_{2}^{(1)}\left(1\right)P_{1}^{(1)}\left(1\right)\\
\frac{\partial R_{2}\left(\mathbf{z}\right)}{\partial
z_{2}}|_{\mathbf{z}=1}&=&R_{2}^{(1)}\left(1\right)P_{2}^{(1)}\left(1\right)\\
\end{eqnarray*}

igualando a cero

\begin{eqnarray*}
\frac{\partial}{\partial
z_{1}}F_{1}\left(\theta_{1}\left(P_{2}\left(z_{2}\right)\right),z_{2}\right)&=&0\\
\frac{\partial}{\partial
z_{2}}F_{1}\left(\theta_{1}\left(P_{2}\left(z_{2}\right)\right),z_{2}\right)&=&\frac{\partial
F_{1}}{\partial z_{2}}+\frac{\partial F_{1}}{\partial
z_{1}}\theta_{1}^{(1)}P_{2}^{(1)}\left(1\right)\\
\frac{\partial}{\partial
z_{1}}F_{2}\left(z_{1},\theta_{2}\left(P_{1}\left(z_{1}\right)\right)\right)&=&\frac{\partial
F_{2}}{\partial z_{1}}+\frac{\partial F_{2}}{\partial
z_{2}}\theta_{2}^{(1)}P_{1}^{(1)}\left(1\right)\\
\frac{\partial}{\partial
z_{2}}F_{2}\left(z_{1},\theta_{2}\left(P_{1}\left(z_{1}\right)\right)\right)&=&0.
\end{eqnarray*}


Por lo tanto de las dos secciones anteriores se tiene que:


\begin{eqnarray*}
\frac{\partial F_{1}}{\partial z_{1}}&=&\frac{\partial
R_{2}}{\partial z_{1}}|_{\mathbf{z}=1}+\frac{\partial F_{2}}{\partial z_{1}}|_{\mathbf{z}=1}=R_{2}^{(1)}\left(1\right)P_{1}^{(1)}\left(1\right)+f_{2}\left(1\right)+f_{2}\left(2\right)\theta_{2}^{(1)}\left(1\right)P_{1}^{(1)}\left(1\right)\\
\frac{\partial F_{1}}{\partial z_{2}}&=&\frac{\partial
R_{2}}{\partial z_{2}}|_{\mathbf{z}=1}+\frac{\partial F_{2}}{\partial z_{2}}|_{\mathbf{z}=1}=R_{2}^{(1)}\left(1\right)P_{2}^{(1)}\left(1\right)\\
\frac{\partial F_{2}}{\partial z_{1}}&=&\frac{\partial
R_{1}}{\partial z_{1}}|_{\mathbf{z}=1}+\frac{\partial F_{1}}{\partial z_{1}}|_{\mathbf{z}=1}=R_{1}^{(1)}\left(1\right)P_{1}^{(1)}\left(1\right)\\
\frac{\partial F_{2}}{\partial z_{2}}&=&\frac{\partial
R_{1}}{\partial z_{2}}|_{\mathbf{z}=1}+\frac{\partial F_{1}}{\partial z_{2}}|_{\mathbf{z}=1}
=R_{1}^{(1)}\left(1\right)P_{2}^{(1)}\left(1\right)+f_{1}\left(1\right)\theta_{1}^{(1)}\left(1\right)P_{2}^{(1)}\left(1\right)\\
\end{eqnarray*}


El cual se puede escribir en forma equivalente:
\begin{eqnarray*}
f_{1}\left(1\right)&=&r_{2}\mu_{1}+f_{2}\left(1\right)+f_{2}\left(2\right)\frac{\mu_{1}}{1-\mu_{2}}\\
f_{1}\left(2\right)&=&r_{2}\mu_{2}\\
f_{2}\left(1\right)&=&r_{1}\mu_{1}\\
f_{2}\left(2\right)&=&r_{1}\mu_{2}+f_{1}\left(2\right)+f_{1}\left(1\right)\frac{\mu_{2}}{1-\mu_{1}}\\
\end{eqnarray*}

De donde:
\begin{eqnarray*}
f_{1}\left(1\right)&=&\mu_{1}\left[r_{2}+\frac{f_{2}\left(2\right)}{1-\mu_{2}}\right]+f_{2}\left(1\right)\\
f_{2}\left(2\right)&=&\mu_{2}\left[r_{1}+\frac{f_{1}\left(1\right)}{1-\mu_{1}}\right]+f_{1}\left(2\right)\\
\end{eqnarray*}

Resolviendo para $f_{1}\left(1\right)$:
\begin{eqnarray*}
f_{1}\left(1\right)&=&r_{2}\mu_{1}+f_{2}\left(1\right)+f_{2}\left(2\right)\frac{\mu_{1}}{1-\mu_{2}}=r_{2}\mu_{1}+r_{1}\mu_{1}+f_{2}\left(2\right)\frac{\mu_{1}}{1-\mu_{2}}\\
&=&\mu_{1}\left(r_{2}+r_{1}\right)+f_{2}\left(2\right)\frac{\mu_{1}}{1-\mu_{2}}=\mu_{1}\left(r+\frac{f_{2}\left(2\right)}{1-\mu_{2}}\right),\\
\end{eqnarray*}

entonces

\begin{eqnarray*}
f_{2}\left(2\right)&=&\mu_{2}\left(r_{1}+\frac{f_{1}\left(1\right)}{1-\mu_{1}}\right)+f_{1}\left(2\right)=\mu_{2}\left(r_{1}+\frac{f_{1}\left(1\right)}{1-\mu_{1}}\right)+r_{2}\mu_{2}\\
&=&\mu_{2}\left[r_{1}+r_{2}+\frac{f_{1}\left(1\right)}{1-\mu_{1}}\right]=\mu_{2}\left[r+\frac{f_{1}\left(1\right)}{1-\mu_{1}}\right]\\
&=&\mu_{2}r+\mu_{1}\left(r+\frac{f_{2}\left(2\right)}{1-\mu_{2}}\right)\frac{\mu_{2}}{1-\mu_{1}}\\
&=&\mu_{2}r+\mu_{2}\frac{r\mu_{1}}{1-\mu_{1}}+f_{2}\left(2\right)\frac{\mu_{1}\mu_{2}}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}\\
&=&\mu_{2}\left(r+\frac{r\mu_{1}}{1-\mu_{1}}\right)+f_{2}\left(2\right)\frac{\mu_{1}\mu_{2}}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}\\
&=&\mu_{2}\left(\frac{r}{1-\mu_{1}}\right)+f_{2}\left(2\right)\frac{\mu_{1}\mu_{2}}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}\\
\end{eqnarray*}
entonces
\begin{eqnarray*}
f_{2}\left(2\right)-f_{2}\left(2\right)\frac{\mu_{1}\mu_{2}}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}&=&\mu_{2}\left(\frac{r}{1-\mu_{1}}\right)\\
f_{2}\left(2\right)\left(1-\frac{\mu_{1}\mu_{2}}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}\right)&=&\mu_{2}\left(\frac{r}{1-\mu_{1}}\right)\\
f_{2}\left(2\right)\left(\frac{1-\mu_{1}-\mu_{2}+\mu_{1}\mu_{2}-\mu_{1}\mu_{2}}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}\right)&=&\mu_{2}\left(\frac{r}{1-\mu_{1}}\right)\\
f_{2}\left(2\right)\left(\frac{1-\mu}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}\right)&=&\mu_{2}\left(\frac{r}{1-\mu_{1}}\right)\\
\end{eqnarray*}
por tanto
\begin{eqnarray*}
f_{2}\left(2\right)&=&\frac{r\frac{\mu_{2}}{1-\mu_{1}}}{\frac{1-\mu}{\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}}=\frac{r\mu_{2}\left(1-\mu_{1}\right)\left(1-\mu_{2}\right)}{\left(1-\mu_{1}\right)\left(1-\mu\right)}\\
&=&\frac{\mu_{2}\left(1-\mu_{2}\right)}{1-\mu}r=r\mu_{2}\frac{1-\mu_{2}}{1-\mu}.
\end{eqnarray*}
es decir

\begin{eqnarray}
f_{2}\left(2\right)&=&r\mu_{2}\frac{1-\mu_{2}}{1-\mu}.
\end{eqnarray}

Entonces

\begin{eqnarray*}
f_{1}\left(1\right)&=&\mu_{1}r+f_{2}\left(2\right)\frac{\mu_{1}}{1-\mu_{2}}=\mu_{1}r+\left(\frac{\mu_{2}\left(1-\mu_{2}\right)}{1-\mu}r\right)\frac{\mu_{1}}{1-\mu_{2}}\\
&=&\mu_{1}r+\mu_{1}r\left(\frac{\mu_{2}}{1-\mu}\right)=\mu_{1}r\left[1+\frac{\mu_{2}}{1-\mu}\right]\\
&=&r\mu_{1}\frac{1-\mu_{1}}{1-\mu}\\
\end{eqnarray*}

%______________________________________________________________________
\section{Ecuaciones Centrales}
%______________________________________________________________________

\begin{Prop}
Supongamos

\begin{equation}\label{Eq.1}
f_{i}\left(i\right)-f_{j}\left(i\right)=\mu_{i}\left[\sum_{k=j}^{i-1}r_{k}+\sum_{k=j}^{i-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]
\end{equation}

\begin{equation}\label{Eq.2}
f_{i+1}\left(i\right)=r_{i}\mu_{i},
\end{equation}

Demostrar que

\begin{eqnarray*}
f_{i}\left(i\right)&=&\mu_{i}\left[\sum_{k=1}^{N}r_{k}+\sum_{k=1,k\neq i}^{N}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right].
\end{eqnarray*}

En la Ecuaci\'on (\ref{Eq.2}) hagamos $j=i+1$, entonces se tiene $f_{j}=r_{i}\mu_{i}$, lo mismo para (\ref{Eq.1})

\begin{eqnarray*}
f_{i}\left(i\right)&=&r_{i}\mu_{i}+\mu_{i}\left[\sum_{k=j}^{i-1}r_{k}+\sum_{k=j}^{i-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
&=&\mu_{i}\left[\sum_{k=j}^{i}r_{k}+\sum_{k=j}^{i-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
\end{eqnarray*}

entonces, tomando sobre todo valor de $1,\ldots,N$, tanto para antes de $i$ como para despu\'es de $i$, entonces

\begin{eqnarray*}
f_{i}\left(i\right)&=&\mu_{i}\left[\sum_{k=1}^{N}r_{k}+\sum_{k=1,k\neq i}^{N}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right].
\end{eqnarray*}
\end{Prop}

Ahora, supongamos nuevamente la ecuaci\'on (\ref{Eq.1})

\begin{eqnarray*}
f_{i}\left(i\right)-f_{j}\left(i\right)&=&\mu_{i}\left[\sum_{k=j}^{i-1}r_{k}+\sum_{k=j}^{i-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
&\Leftrightarrow&\\
f_{j}\left(j\right)-f_{i}\left(j\right)&=&\mu_{j}\left[\sum_{k=i}^{j-1}r_{k}+\sum_{k=i}^{j-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
f_{i}\left(j\right)&=&f_{j}\left(j\right)-\mu_{j}\left[\sum_{k=i}^{j-1}r_{k}+\sum_{k=i}^{j-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
&=&\mu_{j}\left(1-\mu_{j}\right)\frac{r}{1-\mu}-\mu_{j}\left[\sum_{k=i}^{j-1}r_{k}+\sum_{k=i}^{j-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
&=&\mu_{j}\left[\left(1-\mu_{j}\right)\frac{r}{1-\mu}-\sum_{k=i}^{j-1}r_{k}-\sum_{k=i}^{j-1}\frac{f_{k}\left(k\right)}{1-\mu_{k}}\right]\\
&=&\mu_{j}\left[\left(1-\mu_{j}\right)\frac{r}{1-\mu}-\sum_{k=i}^{j-1}r_{k}-\frac{r}{1-\mu}\sum_{k=i}^{j-1}\mu_{k}\right]\\
&=&\mu_{j}\left[\frac{r}{1-\mu}\left(1-\mu_{j}-\sum_{k=i}^{j-1}\mu_{k}\right)-\sum_{k=i}^{j-1}r_{k}\right]\\
&=&\mu_{j}\left[\frac{r}{1-\mu}\left(1-\sum_{k=i}^{j}\mu_{k}\right)-\sum_{k=i}^{j-1}r_{k}\right].\\
\end{eqnarray*}

Ahora,

\begin{eqnarray*}
1-\sum_{k=i}^{j}\mu_{k}&=&1-\sum_{k=1}^{N}\mu_{k}+\sum_{k=j+1}^{i-1}\mu_{k}\\
&\Leftrightarrow&\\
\sum_{k=i}^{j}\mu_{k}&=&\sum_{k=1}^{N}\mu_{k}-\sum_{k=j+1}^{i-1}\mu_{k}\\
&\Leftrightarrow&\\
\sum_{k=1}^{N}\mu_{k}&=&\sum_{k=i}^{j}\mu_{k}+\sum_{k=j+1}^{i-1}\mu_{k}\\
\end{eqnarray*}

Por tanto
\begin{eqnarray*}
f_{i}\left(j\right)&=&\mu_{j}\left[\frac{r}{1-\mu}\sum_{k=j+1}^{i-1}\mu_{k}+\sum_{k=j}^{i-1}r_{k}\right].
\end{eqnarray*}

\begin{Teo}[Teorema de Continuidad]
Sup\'ongase que $\left\{X_{n},n=1,2,3,\ldots\right\}$ son variables aleatorias finitas, no negativas con valores enteros tales que $P\left(X_{n}=k\right)=p_{k}^{(n)}$, para $n=1,2,3,\ldots$, $k=0,1,2,\ldots$, con $\sum_{k=0}^{\infty}p_{k}^{(n)}=1$, para $n=1,2,3,\ldots$. Sea $g_{n}$ la PGF para la variable aleatoria $X_{n}$. Entonces existe una sucesi\'on $\left\{p_{k}\right\}$ tal que \begin{eqnarray*}
lim_{n\rightarrow\infty}p_{k}^{(n)}=p_{k}\textrm{ para }0<s<1.
\end{eqnarray*}
En este caso, $g\left(s\right)=\sum_{k=0}^{\infty}s^{k}p_{k}$. Adem\'as
\begin{eqnarray*}
\sum_{k=0}^{\infty}p_{k}=1\textrm{ si y s\'olo si
}lim_{s\uparrow1}g\left(s\right)=1
\end{eqnarray*}
\end{Teo}

\begin{Teo}
Sea $N$ una variable aleatoria con valores enteros no negativos finita tal que $P\left(N=k\right)=p_{k}$, para $k=0,1,2,\ldots$, y $\sum_{k=0}^{\infty}p_{k}=P\left(N<\infty\right)=1$. Sea $\Phi$ la PGF de $N$ tal que $g\left(s\right)=\esp\left[s^{N}\right]=\sum_{k=0}^{\infty}s^{k}p_{k}$ con $g\left(1\right)=1$. Si $0\leq p_{1}\leq1$ y $\esp\left[N\right]=g^{'}\left(1\right)\leq1$, entonces no existe soluci\'on  de la ecuaci\'on $g\left(s\right)=s$ en el intervalo $\left[0,1\right)$. Si $\esp\left[N\right]=g^{'}\left(1\right)>1$, lo cual implica que $0\leq p_{1}<1$, entonces existe una \'unica soluci\'on de la ecuaci\'on $g\left(s\right)=s$ en el intervalo
$\left[0,1\right)$.
\end{Teo}

\begin{Teo}
Si $X$ y $Y$ tienen PGF $G_{X}$ y $G_{Y}$ respectivamente, entonces,\[G_{X}\left(s\right)=G_{Y}\left(s\right)\] para toda $s$, si y s\'olo si \[P\left(X=k\right))=P\left(Y=k\right)\] para toda $k=0,1,\ldots,$., es decir, si y s\'olo si $X$ y $Y$ tienen la misma distribuci\'on de probabilidad.
\end{Teo}


\begin{Teo}
Para cada $n$ fijo, sea la sucesi\'oin de probabilidades $\left\{a_{0,n},a_{1,n},\ldots,\right\}$, tales que $a_{k,n}\geq0$ para toda $k=0,1,2,\ldots,$ y $\sum_{k\geq0}a_{k,n}=1$, y sea $G_{n}\left(s\right)$ la correspondiente funci\'on generadora, $G_{n}\left(s\right)=\sum_{k\geq0}a_{k,n}s^{k}$. De modo que para cada valor fijo de $k$
\begin{eqnarray*}
lim_{n\rightarrow\infty}a_{k,n}=a_{k},
\end{eqnarray*}
es decir converge en distribuci\'on, es necesario y suficiente que para cada valor fijo $s\in\left[0,\right)$,
\begin{eqnarray*}
lim_{n\rightarrow\infty}G_{n}\left(s\right)=G\left(s\right),
\end{eqnarray*}
donde $G\left(s\right)=\sum_{k\geq0}p_{k}s^{k}$, para cualquier la funci\'on generadora del l\'imite de la sucesi\'on.
\end{Teo}

\begin{Teo}[Teorema de Abel]
Sea $G\left(s\right)=\sum_{k\geq0}a_{k}s^{k}$ para cualquier $\left\{p_{0},p_{1},\ldots,\right\}$, tales que $p_{k}\geq0$ para toda $k=0,1,2,\ldots,$. Entonces $G\left(s\right)$ es continua por la derecha en $s=1$, es decir
\begin{eqnarray*}
lim_{s\uparrow1}G\left(s\right)=\sum_{k\geq0}p_{k}=G\left(\right),
\end{eqnarray*}
sin importar si la suma es finita o no.
\end{Teo}
\begin{Note}
El radio de Convergencia para cualquier PGF es $R\geq1$, entonces, el Teorema de Abel nos dice que a\'un en el peor escenario, cuando $R=1$, a\'un se puede confiar en que la PGF ser\'a continua en $s=1$, en contraste, no se puede asegurar que la PGF ser\'a continua en el l\'imite inferior $-R$, puesto que la PGF es sim\'etrica alrededor del cero: la PGF converge para todo $s\in\left(-R,R\right)$, y no lo hace para $s<-R$ o $s>R$. Adem\'as nos dice que podemos escribir $G_{X}\left(1\right)$ como una abreviaci\'on de $lim_{s\uparrow1}G_{X}\left(s\right)$.
\end{Note}

Entonces si suponemos que la diferenciaci\'on t\'ermino a t\'ermino est\'a permitida, entonces

\begin{eqnarray*}
G_{X}^{'}\left(s\right)&=&\sum_{x=1}^{\infty}xs^{x-1}p_{x}
\end{eqnarray*}

el Teorema de Abel nos dice que
\begin{eqnarray*}
\esp\left(X\right]&=&\lim_{s\uparrow1}G_{X}^{'}\left(s\right):\\
\esp\left[X\right]&=&=\sum_{x=1}^{\infty}xp_{x}=G_{X}^{'}\left(1\right)\\
&=&\lim_{s\uparrow1}G_{X}^{'}\left(s\right),
\end{eqnarray*}
dado que el Teorema de Abel se aplica a
\begin{eqnarray*}
G_{X}^{'}\left(s\right)&=&\sum_{x=1}^{\infty}xs^{x-1}p_{x},
\end{eqnarray*}
estableciendo as\'i que $G_{X}^{'}\left(s\right)$ es continua en $s=1$. Sin el Teorema de Abel no se podr\'ia asegurar que el l\'imite de $G_{X}^{'}\left(s\right)$ conforme $s\uparrow1$ sea la respuesta correcta para $\esp\left[X\right]$.

\begin{Note}
La PGF converge para todo $|s|<R$, para alg\'un $R$. De hecho la PGF converge absolutamente si $|s|<R$. La PGF adem\'as converge uniformemente en conjuntos de la forma $\left\{s:|s|<R^{'}\right\}$, donde $R^{'}<R$, es decir, $\forall\epsilon>0, \exists n_{0}\in\ent$ tal que $\forall s$, con $|s|<R^{'}$, y $\forall n\geq n_{0}$,
\begin{eqnarray*}
|\sum_{x=0}^{n}s^{x}\prob\left(X=x\right)-G_{X}\left(s\right)|<\epsilon.
\end{eqnarray*}
De hecho, la convergencia uniforme es la que nos permite diferenciar t\'ermino a t\'ermino:
\begin{eqnarray*}
G_{X}\left(s\right)=\esp\left[s^{X}\right]=\sum_{x=0}^{\infty}s^{x}\prob\left(X=x\right),
\end{eqnarray*}
y sea $s<R$.
\begin{enumerate}
\item
\begin{eqnarray*}
G_{X}^{'}\left(s\right)&=&\frac{d}{ds}\left(\sum_{x=0}^{\infty}s^{x}\prob\left(X=x\right)\right)=\sum_{x=0}^{\infty}\frac{d}{ds}\left(s^{x}\prob\left(X=x\right)\right)\\
&=&\sum_{x=0}^{n}xs^{x-1}\prob\left(X=x\right).
\end{eqnarray*}

\item\begin{eqnarray*}
\int_{a}^{b}G_{X}\left(s\right)ds&=&\int_{a}^{b}\left(\sum_{x=0}^{\infty}s^{x}\prob\left(X=x\right)\right)ds=\sum_{x=0}^{\infty}\left(\int_{a}^{b}s^{x}\prob\left(X=x\right)ds\right)\\
&=&\sum_{x=0}^{\infty}\frac{s^{x+1}}{x+1}\prob\left(X=x\right),
\end{eqnarray*}
para $-R<a<b<R$.
\end{enumerate}
\end{Note}

\begin{Teo}[Teorema de Convergencia Mon\'otona para PGF]
Sean $X$ y $X_{n}$ variables aleatorias no negativas, con valores en los enteros, finitas, tales que
\begin{eqnarray*}
lim_{n\rightarrow\infty}G_{X_{n}}\left(s\right)&=&G_{X}\left(s\right)
\end{eqnarray*}
para $0\leq s\leq1$, entonces
\begin{eqnarray*}
lim_{n\rightarrow\infty}P\left(X_{n}=k\right)=P\left(X=k\right),
\end{eqnarray*}
para $k=0,1,2,\ldots.$
\end{Teo}

El teorema anterior requiere del siguiente lema

\begin{Lemma}
Sean $a_{n,k}\in\ent^{+}$, $n\in\nat$ constantes no negativas con $\sum_{k\geq0}a_{k,n}\leq1$. Sup\'ongase que para $0\leq s\leq1$,
se tiene
\begin{eqnarray*}
a_{n}\left(s\right)&=&\sum_{k=0}^{\infty}a_{k,n}s^{k}\rightarrow
a\left(s\right)=\sum_{k=0}^{\infty}a_{k}s^{k}.
\end{eqnarray*}
Entonces
\begin{eqnarray*}
a_{0,n}\rightarrow a_{0}.
\end{eqnarray*}
\end{Lemma}

%_________________________________________________________________________
\section{Redes de Jackson}
%_________________________________________________________________________
Cuando se considera la cantidad de
usuarios que llegan a cada uno de los nodos desde fuera del
sistema m\'as los que provienen del resto de los nodos, se dice
que la red es abierta y recibe el nombre de {\em Red de Jackson Abierta}.\\

Si denotamos por $Q_{1}\left(t\right),Q_{2}\left(t\right),\ldots,Q_{K}\left(t\right)$ el n\'umero de usuarios presentes en la cola $1,2,\ldots,K$ respectivamente al tiempo $t$, entonces se tiene la colecci\'on de colas $\left\{Q_{1},Q_{2},\ldots,Q_{K}\right\}$, donde despu\'es de que el usuario es atendido en la cola $i$, se traslada a la cola $j$ con probabilidad $p_{ij}$. En caso de que un usuario decida volver a ser atendido en $i$, este permanecer\'a en la misma cola con probabilidad $p_{ii}$. Para considerar a los usuarios que entran al sistema por primera vez por $i$, m\'as aquellos que provienen de otra cola, es necesario considerar un estado adicional $0$, con probabilidad de transici\'on $p_{00}=0$, $p_{0j}\geq0$ y $p_{j0}\geq0$, para $j=1,2,\ldots,K$, entonces en general la probabilidad de transici\'on de una cola a otra puede representarse por $P=\left(p_{ij}\right)_{i,j=0}^{K}$.\\

Para el caso espec\'ifico en el que en cada una de las colas los tiempos entre arribos y los tiempos de servicio sean exponenciales con par\'ametro de intensidad $\lambda$ y media $\mu$, respectivamente, con $m$ servidores y sin restricciones en la capacidad de almacenamiento en cada una de las colas, en Chee-Hook y Boon-Hee \cite{HookHee}, cap. 6, se muestra que el n\'umero de
usuarios en las $K$ colas, en el caso estacionario, puede determinarse por la ecuaci\'on (\ref{Eq.7.5.1})  que a
continuaci\'on se presenta, adem\'as de que la distribuci\'on l\'imite de la misma es (\ref{Eq.7.5.2}).\\

El n\'umero de usuarios en las $K$ colas en su estado estacionario, ver \cite{Bhat}, se define como
\begin{equation}\label{Eq.7.5.1}
p_{q_{1}q_{2}\cdots
q_{K}}=P\left[Q_{1}=q_{1},Q_{2}=q_{2},\ldots,Q_{K}=q_{K}\right].
\end{equation}

Jackson (1957), demostr\'o que la distribuci\'on l\'imite
$p_{q_{1}q_{2}\cdots q_{K}}$ de (\ref{Eq.7.5.1}) es

\begin{equation}\label{Eq.7.5.2}
p_{q_{1}q_{2}\cdots
q_{K}}=P_{1}\left(q_{1}\right)P_{2}\left(q_{2}\right)\cdots
P_{K}\left(q_{K}\right),
\end{equation}

donde
\begin{equation}\label{Eq.7.5.3}
p_{i}\left(r\right)=\left\{\begin{array}{cc}
 p_{i}\left(0\right)\frac{\left(\gamma_{i}/\mu_{i}\right)^{r}}{r!},  & r=0,1,2,\ldots,m, \\
 p_{i}\left(0\right)\frac{\left(\gamma_{i}/\mu_{i}\right)^{r}}{m!m^{r-m}}, & r=m,m+1,\ldots .\\
\end{array}\right.
\end{equation}

y

\begin{equation}\label{Eq.7.5.4}
\gamma_{i}=\lambda_{i}+\sum p_{ji}\gamma_{j},\textrm{
}i=1,2,\ldots,K.
\end{equation}

La relaci\'on (\ref{Eq.7.5.4}) es importante puesto que considera no solamente los arribos externos si no que adem\'as permite considerar intercambio de clientes entre las distintas colas que conforman el sistema.\\

Dados $\lambda_{i}$ y $p_{ij}$, la cantidad $\gamma_{i}$ puede determinarse a partir de la ecuaci\'on (\ref{Eq.7.5.4}) de manera recursiva. Adem\'as $p_{i}\left(0\right)$ puede determinarse utilizando la condici\'on de normalidad
\[\sum_{q_{1}}\sum_{q_{2}}\cdots\sum_{q_{K}}p_{q_{1}q_{2}\cdots q_{K}}=1.\]

Sin embargo las Redes de Jackson tienen el inconveniente de que no consideran el caso en que existan tiempos de traslado entre las colas. 
