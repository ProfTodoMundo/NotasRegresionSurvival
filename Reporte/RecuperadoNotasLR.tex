\documentclass[a4paper]{report} % Especificar el tama\~no del papel directamente aqu\'i
\usepackage[utf8]{inputenc} % Ya no es necesario en LaTeX moderno (si usas una versi\'on reciente, puedes eliminar esta l\'inea)
\usepackage[T1]{fontenc} % Mejora la codificaci\'on de fuentes
\usepackage{amsmath, amssymb} % Combinar los paquetes matemáticos en una sola l\'inea
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec} % Para modificar t\'itulos de secciones
\usepackage[spanish]{babel} % Definir el idioma
\usepackage{listings} % Incluir c\'odigo fuente
\usepackage{graphicx} % Incluir gráficos
\usepackage{hyperref}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\title{Curso Elemental de Regresi\'on Log\'istica y An\'alisis de Supervivencia}
\author{Carlos E. Mart\'inez-Rodr\'iguez}
\date{Julio 2024}

% Configuraci\'on de geometr\'ia
\geometry{
  left=25mm,
  right=25mm,
  top=30mm,
  bottom=30mm,
}

% Configuraci\'on de encabezados y pies de p\'agina
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\rightmark}
\fancyfoot[L]{Carlos E. Mart\'inez-Rodr\'iguez}

% Configuraci\'on para la inclusi\'on de c\'odigo fuente en R
\lstset{
    language=R,
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    title=\lstname
}

% Definiciones de nuevos entornos
\newtheorem{Algthm}{Algoritmo}[chapter]
\newtheorem{Def}{Definici\'on}[chapter]
\newtheorem{Ejem}{Ejemplo}[chapter]
\newtheorem{Teo}{Teorema}[chapter]
\newtheorem{Dem}{Demostraci\'on}[chapter]
\newtheorem{Note}{Nota}[chapter]
\newtheorem{Sol}{Soluci\'on}[chapter]
\newtheorem{Prop}{Proposici\'on}[chapter]
\newtheorem{Coro}{Corolario}[chapter]
\newtheorem{Lemma}{Lema}[chapter]
\newtheorem{Sup}{Supuestos}[chapter]
\newtheorem{Remark}{Observaci\'on}[chapter]
\newtheorem{Condition}{Condici\'on}[chapter]
\newtheorem{Theorem}{Teorema}[chapter]
\newtheorem{Corollary}{Corolario}[chapter]
\newtheorem{Obs}{Observaci\'on}[chapter]


% Definiciones de comandos para s\'imbolos matem\'aticos
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\ent}{\mathbb{Z}}
\newcommand{\rea}{\mathbb{R}}
\newcommand{\Eb}{\mathbf{E}}
\newcommand{\esp}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\indora}{\mbox{$1$\hspace{-0.8ex}$1$}}
\newcommand{\ER}{\left(E,\mathcal{E}\right)}
\newcommand{\KM}{\left(P_{s,t}\right)}
\newcommand{\Xt}{\left(X_{t}\right)_{t\in I}}
\newcommand{\PE}{\left(X_{t}\right)_{t\in I}}
\newcommand{\SG}{\left(P_{t}\right)}
\newcommand{\CM}{\mathbf{P}^{x}}

%______________________________________________________________________


\begin{document}

\maketitle

\tableofcontents
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Introducci\'on}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Introducci\'on}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La Estad\'istica es una ciencia formal que estudia la recolecci\'on, an\'alisis e interpretaci\'on de datos de una muestra representativa, ya sea para ayudar en la toma de decisiones o para explicar condiciones regulares o irregulares de alg\'un fen\'omeno o estudio aplicado, de ocurrencia en forma aleatoria o condicional. Sin embargo, la estad\'istica es m\'as que eso, es decir, es transversal a una amplia variedad de disciplinas, desde la f\'isica hasta las ciencias sociales, desde las ciencias de la salud hasta el control de calidad. Se usa para la toma de decisiones en \'areas de negocios o instituciones gubernamentales. Ahora bien, las t\'ecnicas estad\'isticas se aplican de manera amplia en mercadotecnia, contabilidad, control de calidad y en otras actividades; estudios de consumidores; an\'alisis de resultados en deportes; administradores de instituciones; en la educaci\'on; organismos pol\'iticos; m\'edicos; y por otras personas que intervienen en la toma de decisiones.

\begin{Def}
    La Estad\'istica es la ciencia cuyo objetivo es reunir una informaci\'on cuantitativa concerniente a individuos, grupos, series de hechos, etc. y deducir de ello gracias al an\'alisis de estos datos unos significados precisos o unas previsiones para el futuro.
\end{Def}

La estad\'istica, en general, es la ciencia que trata de la recopilaci\'on, organizaci\'on presentaci\'on, an\'alisis e interpretaci\'on de datos num\'ericos con el fin de realizar una toma de decisi\'on m\'as efectiva. Los m\'etodos estad\'isticos tradicionalmente se utilizan para prop\'ositos descriptivos, para organizar y resumir datos num\'ericos. La estad\'istica descriptiva, por ejemplo trata de la tabulaci\'on de datos, su presentaci\'on en forma gr\'afica o ilustrativa y el c\'alculo de medidas descriptivas.

%---------------------------------------------------------
\subsection{Historia de la Estad\'istica}
%---------------------------------------------------------
Es dif\'icil conocer los or\'igenes de la Estad\'istica. Desde los comienzos de la civilizaci\'on han existido formas sencillas de estad\'istica, pues ya se utilizaban representaciones gr\'aficas y otros s\'imbolos en pieles, rocas, palos de madera y paredes de cuevas para contar el n\'umero de personas, animales o ciertas cosas. Su origen empieza posiblemente en la isla de Cerde\~na, donde existen monumentos prehist\'oricos pertenecientes a los Nuragas, las primeros habitantes de la isla; estos monumentos constan de bloques de basalto superpuestos sin mortero y en cuyas paredes de encontraban grabados toscos signos que han sido interpretados con mucha verosimilidad como muescas que serv\'ian para llevar la cuenta del ganado y la caza. Los babilonios usaban ya peque\~nas tablillas de arcilla para recopilar datos en tablas sobre la producci\'on agr\'icola y los g\'eneros vendidos o cambiados mediante trueque. Otros vestigios pueden ser hallados en el antiguo Egipto, cuyos faraones lograron recopilar, hacia el a\~no 3050 antes de Cristo, prolijos datos relativos a la poblaci\'on y la riqueza del pa\'is. De acuerdo al historiador griego Her\'odoto, dicho registro de riqueza y poblaci\'on se hizo con el objetivo de preparar la construcci\'on de las pir\'amides. En el mismo Egipto, Rams\'es II hizo un censo de las tierras con el objeto de verificar un nuevo reparto. En el antiguo Israel la Biblia da referencias, en el libro de los N\'umeros, de los datos estad\'isticos obtenidos en dos recuentos de la poblaci\'on hebrea. El rey David por otra parte, orden\'o a Joab, general del ej\'ercito hacer un censo de Israel con la finalidad de conocer el n\'umero de la poblaci\'on. Tambi\'en los chinos efectuaron censos hace m\'as de cuarenta siglos. Los griegos efectuaron censos peri\'odicamente con fines tributarios, sociales (divisi\'on de tierras) y militares (c\'alculo de recursos y hombres disponibles). La investigaci\'on hist\'orica revela que se realizaron 69 censos para calcular los impuestos, determinar los derechos de voto y ponderar la potencia guerrera. \medskip

Fueron los romanos, maestros de la organizaci\'on pol\'itica, quienes mejor supieron emplear los recursos de la estad\'istica. Cada cinco a\~nos realizaban un censo de la poblaci\'on y sus funcionarios p\'ublicos ten\'ian la obligaci\'on de anotar nacimientos, defunciones y matrimonios, sin olvidar los recuentos peri\'odicos del ganado y de las riquezas contenidas en las tierras conquistadas. Para el nacimiento de Cristo suced\'ia uno de estos empadronamientos de la poblaci\'on bajo la autoridad del imperio. Durante los mil a\~nos siguientes a la ca\'ida del imperio Romano se realizaron muy pocas operaciones Estad\'isticas, con la notable excepci\'on de las relaciones de tierras pertenecientes a la Iglesia, compiladas por Pipino el Breve en el 758 y por Carlomagno en el 762 DC. Durante el siglo IX se realizaron en Francia algunos censos parciales de siervos. En Inglaterra, Guillermo el Conquistador recopil\'o el Domesday Book o libro del Gran Catastro para el a\~no 1086, un documento de la propiedad, extensi\'on y valor de las tierras de Inglaterra. Esa obra fue el primer compendio estad\'istico de Inglaterra.  Aunque Carlomagno, en Francia; y Guillermo el Conquistador, en Inglaterra, trataron de revivir la t\'ecnica romana, los m\'etodos estad\'isticos permanecieron casi olvidados durante la Edad Media.


Durante los siglos XV, XVI, y XVII, hombres como Leonardo de Vinci, Nicol\'as Cop\'ernico, Galileo, Neper, William Harvey, Sir Francis Bacon y Ren\'e Descartes, hicieron grandes operaciones al m\'etodo cient\'ifico, de tal forma que cuando se crearon los Estados Nacionales y surgi\'o como fuerza el comercio internacional exist\'ia ya un m\'etodo capaz de aplicarse a los datos econ\'omicos. Para el a\~no 1532 empezaron a registrarse en Inglaterra las defunciones debido al temor que Enrique VII ten\'ia por la peste.  M\'as o menos por la misma \'epoca, en Francia la ley exigi\'o a los cl\'erigos registrar los bautismos, fallecimientos y matrimonios. Durante un brote de peste que apareci\'o a fines de la d\'ecada de 1500, el gobierno ingl\'es comenz\'o a publicar estad\'istica semanales de los decesos. Esa costumbre continu\'o muchos a\~nos, y en 1632 estos Bills of Mortality (Cuentas de Mortalidad) conten\'ian los nacimientos y fallecimientos por sexo. En 1662, el capit\'an John Graunt us\'o documentos que abarcaban treinta a\~nos y efectu\'o predicciones sobre el n\'umero de personas que morir\'ian de varias enfermedades y sobre las proporciones de nacimientos de varones y mujeres que cabr\'ia esperar. El trabajo de Graunt, condensado en su obra \textit{Natural and Political Observations...Made upon the Bills of Mortality}, fue un esfuerzo innovador en el an\'alisis estad\'istico. Por el a\~no 1540 el alem\'an Sebasti\'an Muster realiz\'o una compilaci\'on estad\'istica de los recursos nacionales, comprensiva de datos sobre organizaci\'on pol\'itica, instrucciones sociales, comercio y poder\'io militar. 

Los eruditos del siglo XVII demostraron especial inter\'es por la Estad\'istica Demogr\'afica como resultado de la especulaci\'on sobre si la poblaci\'on aumentaba, decrec\'ia o permanec\'ia est\'atica. En los tiempos modernos tales m\'etodos fueron resucitados por algunos reyes que necesitaban conocer las riquezas monetarias y el potencial humano de sus respectivos pa\'ises. El primer empleo de los datos estad\'isticos para fines ajenos a la pol\'itica tuvo lugar en 1691 y estuvo a cargo de Gaspar Neumann, un profesor alem\'an que viv\'ia en Breslau. Este investigador se propuso destruir la antigua creencia popular de que en los a\~nos terminados en siete mor\'ia m\'as gente que en los restantes, y para lograrlo hurg\'o pacientemente en los archivos parroquiales de la ciudad. Despu\'es de revisar miles de partidas de defunci\'on pudo demostrar que en tales a\~nos no fallec\'ian m\'as personas que en los dem\'as. Los procedimientos de Neumann fueron conocidos por el astr\'onomo ingl\'es Halley, descubridor del cometa que lleva su nombre, quien los aplic\'o al estudio de la vida humana. 

Durante el siglo XVII y principios del XVIII, matem\'aticos como Bernoulli, Francis Maseres, Lagrange y Laplace desarrollaron la teor\'ia de probabilidades. No obstante durante cierto tiempo, la teor\'ia de las probabilidades limit\'o su aplicaci\'on a los juegos de azar y hasta el siglo XVIII no comenz\'o a aplicarse a los grandes problemas cient\'ificos. Godofredo Achenwall, profesor de la Universidad de Gotinga, acu\~n\'o en 1760 la palabra estad\'istica, que extrajo del t\'ermino italiano statista (estadista). Cre\'ia, y con sobrada raz\'on, que los datos de la nueva ciencia ser\'ian el aliado m\'as eficaz del gobernante consciente. La ra\'iz remota de la palabra se halla, por otra parte, en el t\'ermino latino status, que significa estado o situaci\'on; Esta etimolog\'ia aumenta el valor intr\'inseco de la palabra, por cuanto la estad\'istica revela el sentido cuantitativo de las m\'as variadas situaciones. Jacques Qu\'etelect es quien aplica las Estad\'isticas a las ciencias sociales. Este interpret\'o la teor\'ia de la probabilidad para su uso en las ciencias sociales y resolver la aplicaci\'on del principio de promedios y de la variabilidad a los fen\'omenos sociales. Qu\'etelect fue el primero en realizar la aplicaci\'on pr\'actica de todo el m\'etodo Estad\'istico, entonces conocido, a las diversas ramas de la ciencia. Entretanto, en el per\'iodo del 1800 al 1820 se desarrollaron dos conceptos matem\'aticos fundamentales para la teor\'ia Estad\'istica; la teor\'ia de los errores de observaci\'on, aportada por Laplace y Gauss; y la teor\'ia de los m\'inimos cuadrados desarrollada por Laplace, Gauss y Legendre. A finales del siglo XIX, Sir Francis Gaston ide\'o el m\'etodo conocido por Correlaci\'on, que ten\'ia por objeto medir la influencia relativa de los factores sobre las variables. De aqu\'i parti\'o el desarrollo del coeficiente de correlaci\'on creado por Karl Pearson y otros cultivadores de la ciencia biom\'etrica como J. Pease Norton, R. H. Hooker y G. Udny Yule, que efectuaron amplios estudios sobre la medida de las relaciones.

La historia de la estad\'istica est\'a resumida en tres grandes etapas o fases.

\begin{itemize}
    \item \textbf{Fase 1: Los Censos:} Desde el momento en que se constituye una autoridad pol\'itica, la idea de inventariar de una forma m\'as o menos regular la poblaci\'on y las riquezas existentes en el territorio est\'a ligada a la conciencia de soberan\'ia y a los primeros esfuerzos administrativos.
    \item \textbf{Fase 2: De la Descripci\'on de los Conjuntos a la Aritm\'etica Pol\'itica:} Las ideas mercantilistas extra\~nan una intensificaci\'on de este tipo de investigaci\'on. Colbert multiplica las encuestas sobre art\'iculos manufacturados, el comercio y la poblaci\'on: los intendentes del Reino env\'ian a Par\'is sus memorias. Vauban, m\'as conocido por sus fortificaciones o su Dime Royale, que es la primera propuesta de un impuesto sobre los ingresos, se se\~nala como el verdadero precursor de los sondeos. M\'as tarde, Buf\'on se preocupa de esos problemas antes de dedicarse a la historia natural. La escuela inglesa proporciona un nuevo progreso al superar la fase puramente descriptiva.

Sus tres principales representantes son Graunt, Petty y Halley. El pen\'ultimo es autor de la famosa Aritm\'etica Pol\'itica. Chaptal, ministro del interior franc\'es, publica en 1801 el primer censo general de poblaci\'on, desarrolla los estudios industriales, de las producciones y los cambios, haci\'endose sistem\'aticos durantes las dos terceras partes del siglo XIX.

\item \textbf{Fase 3: Estad\'istica y C\'alculo de Probabilidades:} El c\'alculo de probabilidades se incorpora r\'apidamente como un instrumento de an\'alisis extremadamente poderoso para el estudio de los fen\'omenos econ\'omicos y sociales y en general para el estudio de fen\'omenos cuyas causas son demasiados complejas para conocerlos totalmente y hacer posible su an\'alisis.

\end{itemize}


La Estad\'istica para su mejor estudio se ha dividido en dos grandes ramas: \textbf{la Estad\'istica Descriptiva y la Estad\'istica Inferencial}.

\begin{itemize}
    \item \textbf{Descriptiva:} consiste sobre todo en la presentaci\'on de datos en forma de tablas y gr\'aficas. Esta comprende cualquier actividad relacionada con los datos y est\'a dise\~nada para resumir o describir los mismos sin factores pertinentes adicionales; esto es, sin intentar inferir nada que vaya m\'as all\'a de los datos, como tales.
    \item \textbf{Inferencial:} se deriva de muestras, de observaciones hechas s\'olo acerca de una parte de un conjunto numeroso de elementos y esto implica que su an\'alisis requiere de generalizaciones que van m\'as all\'a de los datos. Como consecuencia, la caracter\'istica m\'as importante del reciente crecimiento de la estad\'istica ha sido un cambio en el \'enfasis de los m\'etodos que describen a m\'etodos que sirven para hacer generalizaciones. La Estad\'istica Inferencial investiga o analiza una poblaci\'on partiendo de una muestra tomada.
\end{itemize}

%---------------------------------------------------------
\subsection*{Estad\'istica Inferencial}
%---------------------------------------------------------

Los m\'etodos b\'asicos de la estad\'istica inferencial son la estimaci\'on y el contraste de hip\'otesis, que juegan un papel fundamental en la investigaci\'on. Por tanto, algunos de los objetivos que se persiguen son:

\begin{itemize}
    \item Calcular los par\'ametros de la distribuci\'on de medias o proporciones muestrales de tama\~no $n$, extra\'idas de una poblaci\'on de media y varianza conocidas.
    \item Estimar la media o la proporci\'on de una poblaci\'on a partir de la media o proporci\'on muestral.
    \item Utilizar distintos tama\~nos muestrales para controlar la confianza y el error admitido.
    \item Contrastar los resultados obtenidos a partir de muestras.
    \item Visualizar gr\'aficamente, mediante las respectivas curvas normales, las estimaciones realizadas.
\end{itemize}

En definitiva, la idea es, a partir de una poblaci\'on se extrae una muestra por algunos de los m\'etodos existentes, con la que se generan datos num\'ericos que se van a utilizar para generar estad\'isticos con los que realizar estimaciones o contrastes poblacionales. Existen dos formas de estimar par\'ametros: la \textit{estimaci\'on puntual} y la \textit{estimaci\'on por intervalo de confianza}. En la primera se busca, con base en los datos muestrales, un \'unico valor estimado para el par\'ametro. Para la segunda, se determina un intervalo dentro del cual se encuentra el valor del par\'ametro, con una probabilidad determinada.

Si el objetivo del tratamiento estad\'istico inferencial, es efectuar generalizaciones acerca de la estructura, composici\'on o comportamiento de las poblaciones no observadas, a partir de una parte de la poblaci\'on, ser\'a necesario que la proporci\'on de poblaci\'on examinada sea representativa del total. Por ello, la selecci\'on de la muestra requiere unos requisitos que lo garanticen, debe ser representativa y aleatoria. 

Adem\'as, la cantidad de elementos que integran la muestra (el tama\~no de la muestra) depende de m\'ultiples factores, como el dinero y el tiempo disponibles para el estudio, la importancia del tema analizado, la confiabilidad que se espera de los resultados, las caracter\'isticas propias del fen\'omeno analizado, etc\'etera. 

As\'i, a partir de la muestra seleccionada se realizan algunos c\'alculos y se estima el valor de los par\'ametros de la poblaci\'on tales como la media, la varianza, la desviaci\'on est\'andar, o la forma de la distribuci\'on, etc.


El conjunto de los m\'etodos que se utilizan para medir las caracter\'isticas de la informaci\'on, para resumir los valores individuales, y para analizar los datos a fin de extraerles el m\'aximo de informaci\'on, es lo que se llama \textit{m\'etodos estad\'isticos}. Los m\'etodos de an\'alisis para la informaci\'on cuantitativa se pueden dividir en los siguientes seis pasos:

\begin{itemize}
    \item Definici\'on del problema.
    \item Recopilaci\'on de la informaci\'on existente.
    \item Obtenci\'on de informaci\'on original.
    \item Clasificaci\'on.
    \item Presentaci\'on.
    \item An\'alisis.
\end{itemize}

El centro de gravedad de la metodolog\'ia estad\'istica se empieza a desplazar t\'ecnicas de computaci\'on intensiva aplicadas a grandes masas de datos, y se empieza a considerar el m\'etodo estad\'istico como un proceso iterativo de b\'usqueda del modelo ideal. Las aplicaciones en este periodo de la Estad\'istica a la Econom\'ia conducen a una disciplina con contenido propio: la Econometr\'ia. La investigaci\'on estad\'istica en problemas militares durante la segunda guerra mundial y los nuevos m\'etodos de programaci\'on matem\'atica, dan lugar a la Investigaci\'on Operativa. El tratamiento de los datos de la investigaci\'on cient\'ifica tiene varias etapas:

\begin{itemize}
    \item En la etapa de recolecci\'on de datos del m\'etodo cient\'ifico, se define a la poblaci\'on de inter\'es y se selecciona una muestra o conjunto de personas representativas de la misma, se realizan experimentos o se emplean instrumentos ya existentes o de nueva creaci\'on, para medir los atributos de inter\'es necesarios para responder a las preguntas de investigaci\'on. Durante lo que es llamado trabajo de campo se obtienen los datos en crudo, es decir las respuestas directas de los sujetos uno por uno, se codifican (se les asignan valores a las respuestas), se capturan y se verifican para ser utilizados en las siguientes etapas.
    \item En la etapa de recuento, se organizan y ordenan los datos obtenidos de la muestra. Esta ser\'a descrita en la siguiente etapa utilizando la estad\'istica descriptiva, todas las investigaciones utilizan estad\'istica descriptiva, para conocer de manera organizada y resumida las caracter\'isticas de la muestra.
    \item En la etapa de an\'alisis se utilizan las pruebas estad\'isticas (estad\'istica inferencial) y en la interpretaci\'on se acepta o rechaza la hip\'otesis nula.
\end{itemize}
%---------------------------------------------------------
\subsection*{Niveles de medici\'on y tipos de variables}
%---------------------------------------------------------
Para poder emplear el m\'etodo estad\'istico en un estudio es necesario medir las variables. 

\begin{itemize}
    \item Medir: es asignar valores a las propiedades de los objetos bajo ciertas reglas, esas reglas son los niveles de medici\'on.
    \item Cuantificar: es asignar valores a algo tomando un patr\'on de referencia. Por ejemplo, cuantificar es ver cu\'antos hombres y cu\'antas mujeres hay.
\end{itemize}

\textbf{Variable:} es una caracter\'istica o propiedad que asume diferentes valores dentro de una poblaci\'on de inter\'es y cuya variaci\'on es susceptible de medirse.

Las variables pueden clasificarse de acuerdo al tipo de valores que puede tomar como:

\begin{itemize}
\item \textbf{Discretas o categ\'oricas} en las que los valores se relacionan a nombres, etiquetas o categor\'ias, no existe un significado num\'erico directo.
\item \textbf{Continuas} los valores tienen un correlato num\'erico directo, son continuos y susceptibles de fraccionarse y de poder utilizarse en operaciones aritm\'eticas.
\item \textbf{Dicot\'omica} s\'olo tienen dos valores posibles, la caracter\'istica est\'a ausente o presente.
\end{itemize}

En cuanto a una clasificaci\'on estad\'istica, las varibles pueden ser:

\begin{itemize}
\item \textbf{Aleatoria} Aquella en la cual desconocemos el valor porque fluct\'ua de acuerdo a un evento debido al azar.
\item \textbf{Determin\'istica} Aquella variable de la que se conoce el valor.
\item \textbf{Independiente} aquellas variables que son manipuladas por el investigador. Define los grupos.
\item \textbf{Dependiente} son mediciones que ocurren durante el experimento o tratamiento (resultado de la independiente), es la que se mide y compara entre los grupos.
\end{itemize}

En lo que tiene que ver con los \textbf{Niveles de Medici\'on} tenemoss distintos tipos de variable

\begin{itemize}
\item \textbf{Nominal:} Las propiedades de la medici\'on nominal son:
\begin{itemize}
\item Exhaustiva: implica a todas las opciones.
\item A los sujetos se les asignan categor\'ias, por lo que son mutuamente excluyentes. Es decir, la variable est\'a presente o no; tiene o no una caracter\'istica.
\end{itemize}
\item \textbf{Ordinal:} Las propiedades de la medici\'on ordinal son:
\begin{itemize}
\item El nivel ordinal posee transitividad, por lo que se tiene la capacidad de identificar que es mejor o mayor que otra, en ese sentido se pueden establecer jerarqu\'ias.
\item Las distancias entre un valor y otro no son iguales.
\end{itemize}
\item \textbf{Intervalo:} 
\begin{itemize}
\item El nivel de medici\'on intervalar requiere distancias iguales entre cada valor. Por lo general utiliza datos cuantitativos. Por ejemplo: temperatura, atributos psicol\'ogicos (CI, nivel de autoestima, pruebas de conocimientos, etc.)
\item Las unidades de calificaci\'on son equivalentes en todos los puntos de la escala. Una escala de intervalos implica: clasificaci\'on, magnitud y unidades de tama\~nos iguales (Brown, 2000).
\item Se pueden hacer operaciones aritm\'eticas.
\item Cuando se le pide al sujeto que califique una situaci\'on del 0 al 10 puede tomarse como un nivel de medici\'on de intervalo, siempre y cuando se incluya el 0.
\end{itemize}
\item \textbf{Raz\'on:} 
\begin{itemize}
\item La escala empieza a partir del 0 absoluto, por lo tanto incluye s\'olo los n\'umeros por su valor en s\'i, por lo que no pueden existir los n\'umeros con signo negativo. Por ejemplo: Peso corporal en kg., edad en a\~nos, estatura en cm.
\end{itemize}
\end{itemize}
%---------------------------------------------------------
\subsection*{Definiciones adicionales}
%---------------------------------------------------------
\begin{itemize}
    \item \textbf{Variable:} Consideraciones que una variable son una caracter\'istica o fen\'omeno que puede tomar distintos valores.
    \item \textbf{Dato:} Mediciones o cualidades que han sido recopiladas como resultado de observaciones.
    \item \textbf{Poblaci\'on:} Se considera el \'area de la cual son extra\'idos los datos. Es decir, es el conjunto de elementos o individuos que poseen una caracter\'istica com\'un y medible acerca de lo cual se desea informaci\'on. Es tambi\'en llamado Universo.
    \item \textbf{Muestra:} Es un subconjunto de la poblaci\'on, seleccionado de acuerdo a una regla o alg\'un plan de muestreo.
    \item \textbf{Censo:} Recopilaci\'on de todos los datos (de inter\'es para la investigaci\'on) de la poblaci\'on.
    \item \textbf{Estad\'istica:} Es una funci\'on o f\'ormula que depende de los datos de la muestra (es variable).
    \item \textbf{Par\'ametro:} Caracter\'istica medible de la poblaci\'on. Es un resumen num\'erico de alguna variable observada de la poblaci\'on. Los par\'ametros normales que se estudian son: \textit{La media poblacional, Proporci\'on.}
    \item \textbf{Estimador:} Un estimador de un par\'ametro es un estad\'istico que se emplea para conocer el par\'ametro desconocido.
    \item \textbf{Estad\'istico:} Es una funci\'on de los valores de la muestra. Es una variable aleatoria, cuyos valores dependen de la muestra seleccionada. Su distribuci\'on de probabilidad, se conoce como \textit{Distribuci\'on muestral del estad\'istico}.
    \item \textbf{Estimaci\'on:} Este t\'ermino indica que a partir de lo observado en una muestra (un resumen estad\'istico con las medidas que conocemos de Descriptiva) se extrapola o generaliza dicho resultado muestral a la poblaci\'on total, de modo que lo estimado es el valor generalizado a la poblaci\'on. Consiste en la b\'usqueda del valor de los par\'ametros poblacionales objeto de estudio. Puede ser puntual o por intervalo de confianza:
    \begin{itemize}
        \item \textbf{\textit{Puntual:}} cuando buscamos un valor concreto. Un estimador de un par\'ametro poblacional es una funci\'on de los datos muestrales. En pocas palabras, es una f\'ormula que depende de los valores obtenidos de una muestra, para realizar estimaciones. Lo que se pretende obtener es el valor exacto de un par\'ametro.
    \item \textbf{\textit{Intervalo de confianza:}} cuando determinamos un intervalo, dentro del cual se supone que va a estar el valor del par\'ametro que se busca con una cierta probabilidad. El intervalo de confianza est\'a determinado por dos valores dentro de los cuales afirmamos que est\'a el verdadero par\'ametro con cierta probabilidad. Son unos l\'imites o margen de variabilidad que damos al valor estimado, para poder afirmar, bajo un criterio de probabilidad, que el verdadero valor no los rebasar\'a.

Este intervalo contiene al par\'ametro estimado con una determinada certeza o nivel de confianza. 
\end{itemize}

En la estimaci\'on por intervalos se usan los siguientes conceptos:

\item \textbf{Variabilidad del par\'ametro:} Si no se conoce, puede obtenerse una aproximaci\'on en los datos o en un estudio piloto. Tambi\'en hay m\'etodos para calcular el tama\~no de la muestra que prescinden de este aspecto. Habitualmente se usa como medida de esta variabilidad la desviaci\'on t\'ipica poblacional.
\item \textbf{Error de la estimaci\'on:} Es una medida de su precisi\'on que se corresponde con la amplitud del intervalo de confianza. Cuanta m\'as precisi\'on se desee en la estimaci\'on de un par\'ametro, m\'as estrecho deber\'a ser el intervalo de confianza y, por tanto, menor el error, y m\'as sujetos deber\'an incluirse en la muestra estudiada. 
\item \textbf{Nivel de confianza:} Es la probabilidad de que el verdadero valor del par\'ametro estimado en la poblaci\'on se sit\'ue en el intervalo de confianza obtenido. El nivel de confianza se denota por $1-\alpha$
\item \textbf{$p$-value:} Tambi\'en llamado nivel de significaci\'on. Es la probabilidad (en tanto por uno) de fallar en nuestra estimaci\'on, esto es, la diferencia entre la certeza (1) y el nivel de confianza $1-\alpha$. 
\item \textbf{Valor cr\'itico:} Se representa por $Z_{\alpha/2}$. Es el valor de la abscisa en una determinada distribuci\'on que deja a su derecha un \'area igual a 1/2, siendo $1-\alpha$ el nivel de confianza. Normalmente los valores cr\'iticos est\'an tabulados o pueden calcularse en funci\'on de la distribuci\'on de la poblaci\'on.

\end{itemize}

Para un tama\~no fijo de la muestra, los conceptos de error y nivel de confianza van relacionados. Si admitimos un error mayor, esto es, aumentamos el tama\~no del intervalo de confianza, tenemos tambi\'en una mayor probabilidad de \'exito en nuestra estimaci\'on, es decir, un mayor nivel de confianza. Por tanto, un aspecto que debe de tenerse en cuenta es el tama\~no muestral, ya que para disminuir el error que se comente habr\'a que aumentar el tama\~no muestral. Esto se resolver\'a, para un intervalo de confianza cualquiera, despejando el tama\~no de la muestra en cualquiera de las formulas de los intervalos de confianza que veremos a continuaci\'on, a partir del error m\'aximo permitido. Los intervalos de confianza pueden ser unilaterales o bilaterales:

\begin{itemize}
    \item \textbf{Contraste de Hip\'otesis:} Consiste en determinar si es aceptable, partiendo de datos muestrales, que la caracter\'istica o el par\'ametro poblacional estudiado tome un determinado valor o est\'e dentro de unos determinados valores.
    \item \textbf{Nivel de Confianza:} Indica la proporci\'on de veces que acertar\'iamos al afirmar que el par\'ametro est\'a dentro del intervalo al seleccionar muchas muestras.
\end{itemize}
%---------------------------------------------------------
\subsection{Muestreo:} 
%---------------------------------------------------------
\textbf{Muestreo:} Una muestra es representativa en la medida que es imagen de la poblaci\'on. En general, podemos decir que el tama\~no de una muestra depender\'a principalmente de: \textit{Nivel de precisi\'on deseado, Recursos disponibles, Tiempo involucrado en la investigaci\'on.} Adem\'as el plan de muestreo debe considerar \textit{La poblaci\'on, Par\'ametros a medir}. Existe una gran cantidad de tipos de muestreo, en la pr\'actica los m\'as utilizados son los siguientes:


\begin{itemize}
    \item \textbf{MUESTREO ALEATORIO SIMPLE:} Es un m\'etodo de selecci\'on de $n$ unidades extra\'idas de $N$, de tal manera que cada una de las posibles muestras tiene la misma probabilidad de ser escogida. (En la pr\'actica, se enumeran las unidades de 1 a $N$, y a continuaci\'on se seleccionan $n$ n\'umeros aleatorios entre 1 y $N$, ya sea de tablas o de alguna urna con fichas numeradas).
    \item \textbf{MUESTREO ESTRATIFICADO ALEATORIO:} Se usa cuando la poblaci\'on est\'a agrupada en pocos estratos, cada uno de ellos son muchas entidades. Este muestreo consiste en sacar una muestra aleatoria simple de cada uno de los estratos. (Generalmente, de tama\~no proporcional al estrato).
    \item \textbf{MUESTREO SISTEM\'ATICO:} Se utiliza cuando las unidades de la poblaci\'on est\'an de alguna manera totalmente ordenadas. Para seleccionar una muestra de $n$ unidades, se divide la poblaci\'on en $n$ subpoblaciones de tama\~no $K = N/n$ y se toma al azar una unidad de la $K$ primeras y de ah\'i en adelante cada $K$-\'esima unidad.
    \item \textbf{MUESTREO POR CONGLOMERADO:} Se emplea cuando la poblaci\'on est\'a dividida en grupos o conglomerados peque\~nos. Consiste en obtener una muestra aleatoria simple de conglomerados y luego CENSAR cada uno de \'estos.
    \item \textbf{MUESTREO EN DOS ETAPAS (Biet\'apico):} En este caso la muestra se toma en dos pasos:
    \begin{itemize}
        \item Seleccionar una muestra de unidades primarias, y 
        \item Seleccionar una muestra de elementos a partir de cada unidad primaria escogida.
        \item \textit{Observaci\'on:} En la realidad es posible encontrarse con situaciones en las cuales no es posible aplicar libremente un tipo de muestreo, incluso estaremos obligados a mezclarlas en ocasiones.
    \end{itemize}
\end{itemize}

%---------------------------------------------------------
\subsection{Errores Estad\'isticos Comunes}
%---------------------------------------------------------

El prop\'osito de esta secci\'on es solamente indicar los malos usos comunes de datos estad\'isticos, sin incluir el uso de m\'etodos estad\'isticos complicados. Un estudiante deber\'ia estar alerta en relaci\'on con estos malos usos y deber\'ia hacer un gran esfuerzo para evitarlos a fin de ser un verdadero estad\'istico.

\textbf{Datos estad\'isticos inadecuados:} Los datos estad\'isticos son usados como la materia prima para un estudio estad\'istico. Cuando los datos son inadecuados, la conclusi\'on extra\'ida del estudio de los datos se vuelve obviamente inv\'alida. Por ejemplo, supongamos que deseamos encontrar el ingreso familiar t\'ipico del a\~no pasado en la ciudad Y de 50,000 familias y tenemos una muestra consistente del ingreso de solamente tres familias: 1 mill\'on, 2 millones y no ingreso. Si sumamos el ingreso de las tres familias y dividimos el total por 3, obtenemos un promedio de 1 mill\'on. Entonces, extraemos una conclusi\'on basada en la muestra de que el ingreso familiar promedio durante el a\~no pasado en la ciudad fue de 1 mill\'on. Es obvio que la conclusi\'on es falsa, puesto que las cifras son extremas y el tama\~no de la muestra es demasiado peque\~no; por lo tanto la muestra no es representativa. 

Hay muchas otras clases de datos inadecuados. Por ejemplo, algunos datos son respuestas inexactas de una encuesta, porque las preguntas usadas en la misma son vagas o enga\~nosas, algunos datos son toscas estimaciones porque no hay disponibles datos exactos o es demasiado costosa su obtenci\'on, y algunos datos son irrelevantes en un problema dado, porque el estudio estad\'istico no est\'a bien planeado. Al momento de recopilar los datos que ser\'an procesados se es susceptible de cometer errores as\'i como durante los c\'omputos de los mismos. No obstante, hay otros errores que no tienen nada que ver con la digitaci\'on y que no son tan f\'acilmente identificables. Algunos de \'estos errores son:

\begin{itemize}
    \item \textbf{Sesgo:} Es imposible ser completamente objetivo o no tener ideas preconcebidas antes de comenzar a estudiar un problema, y existen muchas maneras en que una perspectiva o estado mental pueda influir en la recopilaci\'on y en el an\'alisis de la informaci\'on. En estos casos se dice que hay un sesgo cuando el individuo da mayor peso a los datos que apoyan su opini\'on que a aquellos que la contradicen. Un caso extremo de sesgo ser\'ia la situaci\'on donde primero se toma una decisi\'on y despu\'es se utiliza el an\'alisis estad\'istico para justificar la decisi\'on ya tomada.
    \item \textbf{Datos No Comparables:} el establecer comparaciones es una de las partes m\'as importantes del an\'alisis estad\'istico, pero es extremadamente importante que tales comparaciones se hagan entre datos que sean comparables.
    \item \textbf{Proyecci\'on descuidada de tendencias:} la proyecci\'on simplista de tendencias pasadas hacia el futuro es uno de los errores que m\'as ha desacreditado el uso del an\'alisis estad\'istico.
    \item \textbf{Muestreo Incorrecto:} en la mayor\'ia de los estudios sucede que el volumen de informaci\'on disponible es tan inmenso que se hace necesario estudiar muestras, para derivar conclusiones acerca de la poblaci\'on a que pertenece la muestra. Si la muestra se selecciona correctamente, tendr\'a b\'asicamente las mismas propiedades que la poblaci\'on de la cual fue extra\'ida; pero si el muestreo se realiza incorrectamente, entonces puede suceder que los resultados no signifiquen nada.
\end{itemize}

\textbf{Sesgo} significa que un usuario d\'e los datos perjudicialmente de m\'as \'enfasis a los hechos, los cuales son empleados para mantener su predeterminada posici\'on u opini\'on. Los estad\'isticos son frecuentemente degradados por lemas tales como: \textit{Hay tres clases de mentiras: mentiras, mentiras reprobables y estad\'istica, y Las cifras no mienten, pero los mentirosos piensan}. Hay dos clases de sesgos: conscientes e inconscientes. Ambos son comunes en el an\'alisis estad\'istico. Hay numerosos ejemplos de sesgos conscientes. Un anunciante frecuentemente usa la estad\'istica para probar que su producto es muy superior al producto de su competidor. Un pol\'itico prefiere usar la estad\'istica para sostener su punto de vista. Gerentes y l\'ideres de trabajadores pueden simult\'aneamente situar sus respectivas cifras estad\'isticas sobre la misma tabla de trato para mostrar que sus rechazos o peticiones son justificadas. Es casi imposible que un sesgo inconsciente est\'e completamente ausente en un trabajo estad\'istico. En lo que respecta al ser humano, es dif\'icil obtener una actitud completamente objetiva al abordar un problema, aun cuando un cient\'ifico deber\'ia tener una mente abierta. Un estad\'istico deber\'ia estar enterado del hecho de que su interpretaci\'on de los resultados del an\'alisis estad\'istico est\'a influenciado por su propia experiencia, conocimiento y antecedentes con relaci\'on al problema dado.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Estad\'istica Descriptiva}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La estad\'istica descriptiva resume y describe las caracter\'isticas de un conjunto de datos. Incluye medidas de tendencia central, medidas de dispersi\'on y medidas de forma.
%---------------------------------------------------------
\subsection{Medidas de Tendencia Central}
%---------------------------------------------------------
Las medidas de tendencia central incluyen la media, la mediana y la moda.
%---------------------------------------------------------
\subsubsection{Media}
%---------------------------------------------------------
La media aritm\'etica es la suma de los valores dividida por el n\'umero de valores:
\begin{eqnarray*}
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{eqnarray*}
donde $x_i$ son los valores de la muestra y $n$ es el tama\~no de la muestra.
%---------------------------------------------------------
\subsubsection{Mediana}
%---------------------------------------------------------
La mediana es el valor medio cuando los datos est\'an ordenados. Si el n\'umero de valores es impar, la mediana es el valor central. Si es par, es el promedio de los dos valores centrales.
%---------------------------------------------------------
\subsubsection{Moda}
%---------------------------------------------------------
La moda es el valor que ocurre con mayor frecuencia en un conjunto de datos.
%---------------------------------------------------------
\subsection{Medidas de Dispersi\'on}
%---------------------------------------------------------
Las medidas de dispersi\'on incluyen el rango, la varianza y la desviaci\'on est\'andar.
%---------------------------------------------------------
\subsubsection{Rango}
%---------------------------------------------------------
El rango es la diferencia entre el valor m\'aximo y el valor m\'inimo de los datos:
\begin{eqnarray*}
Rango = x_{\text{max}} - x_{\text{min}}
\end{eqnarray*}
%---------------------------------------------------------
\subsubsection{Varianza}
%---------------------------------------------------------
La varianza es la media de los cuadrados de las diferencias entre los valores y la media:
\begin{eqnarray*}
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{eqnarray*}
%---------------------------------------------------------
\subsubsection{Desviaci\'on Est\'andar}
%---------------------------------------------------------
La desviaci\'on est\'andar es la ra\'iz cuadrada de la varianza:
\begin{eqnarray*}
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{eqnarray*}


%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Probabilidad}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
%---------------------------------------------------------
\subsection{Espacio Muestral y Eventos}
%---------------------------------------------------------
%---------------------------------------------------------
\subsection{Definiciones de Probabilidad}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Probabilidad Cl\'asica}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Probabilidad Frecuentista}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Probabilidad Bayesiana}
%---------------------------------------------------------
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Estad\'istica Bayesiana}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
%---------------------------------------------------------
\subsection{Prior y Posterior}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Distribuci\'on Prior}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Verosimilitud}
%---------------------------------------------------------
La verosimilitud (likelihood) es la probabilidad de observar los datos dados los par\'ametros. Es una funci\'on de los par\'ametros $\theta$ dada una muestra de datos $X$:
\begin{eqnarray*}
L(\theta; X) = P(X|\theta)
\end{eqnarray*}
donde $X$ son los datos observados y $\theta$ son los par\'ametros del modelo.
%---------------------------------------------------------
\subsubsection{Distribuci\'on Posterior}
%---------------------------------------------------------
La distribuci\'on posterior (a posteriori) combina la informaci\'on de la prior y la verosimilitud utilizando el teorema de Bayes. Representa nuestra creencia sobre los par\'ametros despu\'es de observar los datos:
\begin{eqnarray*}
P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
\end{eqnarray*}
donde $P(\theta|X)$ es la distribuci\'on posterior, $P(X|\theta)$ es la verosimilitud, $P(\theta)$ es la prior y $P(X)$ es la probabilidad marginal de los datos.

La probabilidad marginal de los datos $P(X)$ se puede calcular como:
\begin{eqnarray*}
P(X) = \int_{\Theta} P(X|\theta)P(\theta) d\theta
\end{eqnarray*}
donde $\Theta$ es el espacio de todos los posibles valores del par\'ametro $\theta$.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Distribuciones de Probabilidad}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
%---------------------------------------------------------
\subsection{Distribuciones Discretas}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Distribuci\'on Binomial}
%---------------------------------------------------------
La distribuci\'on binomial describe el n\'umero de \'exitos en una serie de ensayos de Bernoulli independientes y con la misma probabilidad de \'exito. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
\end{eqnarray*}
donde $X$ es el n\'umero de \'exitos, $n$ es el n\'umero de ensayos, $p$ es la probabilidad de \'exito en cada ensayo, y $\binom{n}{k}$ es el coeficiente binomial.

La funci\'on generadora de momentos (MGF) para la distribuci\'on binomial es:
\begin{eqnarray*}
M_X(t) = \left( 1 - p + pe^t \right)^n
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria binomial son:
\begin{eqnarray*}
E(X) &=& np \\
\text{Var}(X) &=& np(1-p)
\end{eqnarray*}
%---------------------------------------------------------
\subsubsection{Distribuci\'on de Poisson}
%---------------------------------------------------------
La distribuci\'on de Poisson describe el n\'umero de eventos que ocurren en un intervalo de tiempo fijo o en un \'area fija. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
\end{eqnarray*}
donde $X$ es el n\'umero de eventos, $\lambda$ es la tasa media de eventos por intervalo, y $k$ es el n\'umero de eventos observados.

La funci\'on generadora de momentos (MGF) para la distribuci\'on de Poisson es:
\begin{eqnarray*}
M_X(t) = e^{\lambda (e^t - 1)}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria de Poisson son:
\begin{eqnarray*}
E(X) &=& \lambda \\
\text{Var}(X) &=& \lambda
\end{eqnarray*}
%---------------------------------------------------------
\subsection{Distribuciones Continuas}
%---------------------------------------------------------
%---------------------------------------------------------
\subsubsection{Distribuci\'on Normal}
%---------------------------------------------------------
La distribuci\'on normal, tambi\'en conocida como distribuci\'on gaussiana, es una de las distribuciones m\'as importantes en estad\'istica. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{eqnarray*}
donde $x$ es un valor de la variable aleatoria, $\mu$ es la media, y $\sigma$ es la desviaci\'on est\'andar.

La funci\'on generadora de momentos (MGF) para la distribuci\'on normal es:
\begin{eqnarray*}
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria normal son:
\begin{eqnarray*}
E(X) &=& \mu \\
\text{Var}(X) &=& \sigma^2
\end{eqnarray*}

\subsubsection{Distribuci\'on Exponencial}

La distribuci\'on exponencial describe el tiempo entre eventos en un proceso de Poisson. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \lambda e^{-\lambda x}
\end{eqnarray*}
donde $x$ es el tiempo entre eventos y $\lambda$ es la tasa media de eventos.

La funci\'on generadora de momentos (MGF) para la distribuci\'on exponencial es:
\begin{eqnarray*}
M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{para } t < \lambda
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria exponencial son:
\begin{eqnarray*}
E(X) &=& \frac{1}{\lambda} \\
\text{Var}(X) &=& \frac{1}{\lambda^2}
\end{eqnarray*}



%---------------------------------------------------------
\subsection{Pruebas de Hip\'otesis}
%---------------------------------------------------------

\begin{itemize}
    \item Una hip\'otesis estad\'istica es una afirmaci\'on acerca de la distribuci\'on de probabilidad de una variable aleatoria, a menudo involucran uno o m\'as par\'ametros de la distribuci\'on.
    \item Las hip\'otesis son afirmaciones respecto a la poblaci\'on o distribuci\'on bajo estudio, no en torno a la muestra.
    \item La mayor\'ia de las veces, la prueba de hip\'otesis consiste en determinar si la situaci\'on experimental ha cambiado.
    \item El inter\'es principal es decidir sobre la veracidad o falsedad de una hip\'otesis, a este procedimiento se le llama \textit{prueba de hip\'otesis}.
    \item Si la informaci\'on es consistente con la hip\'otesis, se concluye que esta es verdadera, de lo contrario que con base en la informaci\'on, es falsa.
\end{itemize}

Una prueba de hip\'otesis est\'a formada por cinco partes:
\begin{itemize}
    \item La hip\'otesis nula, denotada por $H_{0}$.
    \item La hip\'otesis alternativa, denotada por $H_{1}$.
    \item El estad\'istico de prueba y su valor $p$.
    \item La regi\'on de rechazo.
    \item La conclusi\'on.
\end{itemize}

\begin{Def}
Las dos hip\'otesis en competencia son la \textbf{hip\'otesis alternativa $H_{1}$}, usualmente la que se desea apoyar, y la \textbf{hip\'otesis nula $H_{0}$}, opuesta a $H_{1}$.
\end{Def}
En general, es m\'as f\'acil presentar evidencia de que $H_{1}$ es cierta, que demostrar que $H_{0}$ es falsa, es por eso que por lo regular se comienza suponiendo que $H_{0}$ es cierta, luego se utilizan los datos de la muestra para decidir si existe evidencia a favor de $H_{1}$, m\'as que a favor de $H_{0}$, as\'i se tienen dos conclusiones:
\begin{itemize}
    \item Rechazar $H_{0}$ y concluir que $H_{1}$ es verdadera.
    \item Aceptar, no rechazar, $H_{0}$ como verdadera.
\end{itemize}

\begin{Ejem}
Se desea demostrar que el salario promedio por hora en cierto lugar es distinto de $19$ usd, que es el promedio nacional. Entonces $H_{1}:\mu \neq 19$, y $H_{0}:\mu = 19$.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de dos colas}.

\begin{Ejem}
Un determinado proceso produce un promedio de $5\%$ de piezas defectuosas. Se est\'a interesado en demostrar que un simple ajuste en una m\'aquina reducir\'a $p$, la proporci\'on de piezas defectuosas producidas en este proceso. Entonces se tiene $H_{0}: p < 0.3$ y $H_{1}: p = 0.03$. Si se puede rechazar $H_{0}$, se concluye que el proceso ajustado produce menos del $5\%$ de piezas defectuosas.
\end{Ejem}
A esta se le denomina \textbf{Prueba de hip\'otesis de una cola}.

La decisi\'on de rechazar o aceptar la hip\'otesis nula est\'a basada en la informaci\'on contenida en una muestra proveniente de la poblaci\'on de inter\'es. Esta informaci\'on tiene estas formas:
\begin{itemize}
    \item \textbf{Estad\'istico de prueba:} un s\'olo n\'umero calculado a partir de la muestra.
    \item \textbf{$p$-value:} probabilidad calculada a partir del estad\'istico de prueba.
\end{itemize}

\begin{Def}
El $p$-value es la probabilidad de observar un estad\'istico de prueba tanto o m\'as alejado del valor observado, si en realidad $H_{0}$ es verdadera. Valores grandes del estad\'istico de prueba y valores peque\~nos de $p$ significan que se ha observado un evento muy poco probable, si $H_{0}$ en realidad es verdadera.
\end{Def}

Todo el conjunto de valores que puede tomar el estad\'istico de prueba se divide en dos regiones. Un conjunto, formado de valores que apoyan la hip\'otesis alternativa y llevan a rechazar $H_{0}$, se denomina \textbf{regi\'on de rechazo}. El otro, conformado por los valores que sustentan la hip\'otesis nula, se le denomina \textbf{regi\'on de aceptaci\'on}. Cuando la regi\'on de rechazo est\'a en la cola izquierda de la distribuci\'on, la prueba se denomina \textbf{prueba lateral izquierda}. Una prueba con regi\'on de rechazo en la cola derecha se le llama \textbf{prueba lateral derecha}. Si el estad\'istico de prueba cae en la regi\'on de rechazo, entonces se rechaza $H_{0}$. Si el estad\'istico de prueba cae en la regi\'on de aceptaci\'on, entonces la hip\'otesis nula se acepta o la prueba se juzga como no concluyente. Dependiendo del nivel de confianza que se desea agregar a las conclusiones de la prueba, y el \textbf{nivel de significancia $\alpha$}, el riesgo que est\'a dispuesto a correr si se toma una decisi\'on incorrecta.

\begin{Def}
Un \textbf{error de tipo I} para una prueba estad\'istica es el error que se tiene al rechazar la hip\'otesis nula cuando es verdadera. El \textbf{nivel de significancia} para una prueba estad\'istica de hip\'otesis es
\begin{eqnarray*}
\alpha &=& P\left\{\textrm{error tipo I}\right\} = P\left\{\textrm{rechazar equivocadamente } H_{0}\right\} \\
&=& P\left\{\textrm{rechazar } H_{0} \textrm{ cuando } H_{0} \textrm{ es verdadera}\right\}
\end{eqnarray*}
\end{Def}
Este valor $\alpha$ representa el valor m\'aximo de riesgo tolerable de rechazar incorrectamente $H_{0}$. Una vez establecido el nivel de significancia, la regi\'on de rechazo se define para poder determinar si se rechaza $H_{0}$ con un cierto nivel de confianza.

%---------------------------------------------------------
\subsection{Muestras grandes: una media poblacional}
%---------------------------------------------------------


\begin{Def}
El \textbf{valor de $p$} (\textbf{$p$-value}) o nivel de significancia observado de un estad\'istico de prueba es el valor m\'as peque\~no de $\alpha$ para el cual $H_{0}$ se puede rechazar. El riesgo de cometer un error tipo $I$, si $H_{0}$ es rechazada con base en la informaci\'on que proporciona la muestra.
\end{Def}

\begin{Note}
Valores peque\~nos de $p$ indican que el valor observado del estad\'istico de prueba se encuentra alejado del valor hipot\'etico de $\mu$, es decir se tiene evidencia de que $H_{0}$ es falsa y por tanto debe de rechazarse.
\end{Note}

\begin{Note}
Valores grandes de $p$ indican que el estad\'istico de prueba observado no est\'a alejado de la media hipot\'etica y no apoya el rechazo de $H_{0}$.
\end{Note}

\begin{Def}
Si el valor de $p$ es menor o igual que el nivel de significancia $\alpha$, determinado previamente, entonces $H_{0}$ es rechazada y se puede concluir que los resultados son estad\'isticamente significativos con un nivel de confianza del $100 (1-\alpha)\%$.
\end{Def}
Es usual utilizar la siguiente clasificaci\'on de resultados:


\begin{center}
\begin{tabular}{|c||c|l|}\hline
$p$ & $H_{0}$ & Significativa \\ \hline
$p\leq 0.01$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. altamente significativos  y en contra de $H_{0}$\end{tabular} \\ \hline
$p\leq 0.05$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. Estad\'isticamente significativos  y en contra de $H_{0}$\end{tabular} \\ \hline
$p\leq 0.10$ & rechazada & \begin{tabular}[c]{@{}l@{}}Result. posiblemente significativos con Tendencia estad\'istica \\ y en contra de $H_{0}$\end{tabular} \\ \hline
$p> 0.10$ & no rechazada & \begin{tabular}[c]{@{}l@{}}Result.  estad\'isticamente no significativos y no rechazar $H_{0}$\end{tabular} \\ \hline
\end{tabular}
\end{center}

\begin{Note}
Para determinar el valor de $p$, encontrar el \'area en la cola despu\'es del estad\'istico de prueba. Si la prueba es de una cola, este es el valor de $p$. Si es de dos colas, \'este valor encontrado es la mitad del valor de $p$. Rechazar $H_{0}$ cuando el valor de $p<\alpha$.
\end{Note}

Hay dos tipos de errores al realizar una prueba de hip\'otesis
\begin{center}
\begin{tabular}{c|cc}
& $H_{0}$ es Verdadera & $H_{0}$ es Falsa\\\hline\hline
Rechazar $H_{0}$ & Error tipo I & $\surd$\\
Aceptar $H_{0}$ & $\surd$ & Error tipo II
\end{tabular}
\end{center}
\begin{Def}
La probabilidad de cometer el error tipo II se define por $\beta$ donde
\begin{eqnarray*}
\beta&=&P\left\{\textrm{error tipo II}\right\}=P\left\{\textrm{Aceptar equivocadamente }H_{0}\right\}\\
&=&P\left\{\textrm{Aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}
\end{eqnarray*}
\end{Def}

\begin{Note}
Cuando $H_{0}$ es falsa y $H_{1}$ es verdadera, no siempre es posible especificar un valor exacto de $\mu$, sino m\'as bien un rango de posibles valores.\medskip
En lugar de arriesgarse a tomar una decisi\'on incorrecta, es mejor conlcuir que \textit{no hay evidencia suficiente para rechazar $H_{0}$}, es decir en lugar de aceptar $H_{0}$, \textit{no rechazar $H_{0}$}.

\end{Note}
La bondad de una prueba estad\'istica se mide por el tama\~ no de $\alpha$ y $\beta$, ambas deben de ser peque\~ nas. Una manera muy efectiva de medir la potencia de la prueba es calculando el complemento del error tipo $II$:
\begin{eqnarray*}
1-\beta&= &P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}\\
&=&P\left\{\textrm{Rechazar }H_{0}\textrm{ cuando }H_{1}\textrm{ es verdadera}\right\}
\end{eqnarray*}
\begin{Def}
La \textbf{potencia de la prueba}, $1-\beta$, mide la capacidad de que la prueba funciona como se necesita.
\end{Def}

\begin{Ejem}
La producci\'on diariade una planta qu\'imica local ha promediado 880 toneladas en los \'ultimos a\~nos. A la gerente de control de calidad le gustar\'ia saber si este promedio ha cambiado en meses recientes. Ella selecciona al azar 50 d\'ias de la base de datos computarizada y calcula el promedio y la desviaci\'on est\'andar de las $n=50$  producciones como $\overline{x}=871$ toneladas y $s=21$ toneladas, respectivamente. Pruebe la hip\'otesis  apropiada usando $\alpha=0.05$.

La hip\'otesis nula apropiada es:
\begin{eqnarray*}
H_{0}&:& \mu=880\\
&&\textrm{ y la hip\'otesis alternativa }H_{1}\textrm{ es }\\
H_{1}&:& \mu\neq880
\end{eqnarray*}
el estimador puntual para $\mu$ es $\overline{x}$, entonces el estad\'istico de prueba es\medskip
\begin{eqnarray*}
z&=&\frac{\overline{x}-\mu_{0}}{s/\sqrt{n}}\\
&=&\frac{871-880}{21/\sqrt{50}}=-3.03
\end{eqnarray*}

Para esta prueba de  dos colas, hay que determinar los dos valores de $z_{\alpha/2}$, es decir,  $z_{\alpha/2}=\pm1.96$, como $z>z_{\alpha/2}$, $z$ cae en la zona de rechazo, por lo tanto  la gerente puede rechazar la hip\'otesis nula y concluir que el promedio efectivamente ha cambiado. La probabilidad de rechazar $H_{0}$ cuando esta es verdadera es de  $0.05$. Recordemos que el valor observado del estad\'istico de prueba es $z=-3.03$, la regi\'on de rechazo m\'as peque\~na que puede usarse y todav\'ia seguir rechazando $H_{0}$ es $|z|>3.03$, entonces $p=2(0.012)=0.0024$, que a su vez es menor que el nivel de significancia $\alpha$ asignado inicialmente, y adem\'as los resultados son  \textbf{altamente significativos}. Finalmente determinemos la potencia de la prueba cuando $\mu$ en realidad es igual a $870$ toneladas.

Recordar que la regi\'on de aceptaci\'on est\'a entre $-1.96$ y $1.96$, para $\mu=880$, equivalentemente $$874.18<\overline{x}<885.82$$
$\beta$ es la probabilidad de aceptar $H_{0}$ cuando $\mu=870$, calculemos los valores de $z$ correspondientes a $874.18$ y $885.82$ \medskip
Entonces
\begin{eqnarray*}
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{874.18-870}{21/\sqrt{50}}=1.41\\
z_{1}&=&\frac{\overline{x}-\mu}{s/\sqrt{n}}=\frac{885.82-870}{21/\sqrt{50}}=5.33
\end{eqnarray*}
por lo tanto
\begin{eqnarray*}
\beta&=&P\left\{\textrm{aceptar }H_{0}\textrm{ cuando }H_{0}\textrm{ es falsa}\right\}=P\left\{874.18<\mu<885.82\textrm{ cuando }\mu=870\right\}\\
&=&P\left\{1.41<z<5.33\right\}=P\left\{1.41<z\right\}=1-0.9207=0.0793
\end{eqnarray*}
entonces, la potencia de la prueba es
$$1-\beta=1-0.0793=0.9207$$ que es la probabilidad de rechazar correctamente $H_{0}$ cuando $H_{0}$ es falsa.
\end{Ejem}

%---------------------------------------------------------
\subsubsection{Prueba de hip\'otesis para la diferencia entre dos medias poblacionales}
%---------------------------------------------------------

El estad\'istico que resume la informaci\'on muestral respecto a la diferencia en medias poblacionales $\left(\mu_{1}-\mu_{2}\right)$ es la diferencia de las medias muestrales $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, por tanto al probar la difencia entre las medias muestrales se verifica que la diferencia real entre las medias poblacionales difiere de un valor especificado, $\left(\mu_{1}-\mu_{2}\right)=D_{0}$, se puede usar el error est\'andar de $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, es decir

\begin{eqnarray}\sqrt{\frac{\sigma^{2}_{1}}{n_{1}}+\frac{\sigma^{2}_{2}}{n_{2}}}\end{eqnarray}
cuyo estimador est\'a dado por
\begin{eqnarray}SE=\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}\end{eqnarray}
El procedimiento para muestras grandes es:

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,
donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}z=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

\begin{Ejem}
Para determinar si ser propietario de un autom\'ovil afecta el rendimiento acad\'emico de un estudiante, se tomaron dos muestras aleatorias de 100 estudiantes varones. El promedio de calificaciones para los $n_{1}=100$ no propietarios de un auto tuvieron un promedio y varianza de $\overline{x}_{1}=2.7$ y $s_{1}^{2}=0.36$, respectivamente, mientras que para para la segunda muestra con $n_{2}=100$ propietarios de un auto, se tiene $\overline{x}_{2}=2.54$ y $s_{2}^{2}=0.4$. Los datos presentan suficiente evidencia para indicar una diferencia en la media en el rendimiento acad\'emico entre propietarios y no propietarios de un autom\'ovil? Hacer pruebas para $\alpha=0.01,0.05$ y $\alpha=0.1$.

\begin{itemize}
\item Soluci\'on utilizando la t\'ecnica de regiones de rechazo:\medskip
realizando las operaciones
$z=1.84$, determinar si excede los valores de $z_{\alpha/2}$.
\item Soluci\'on utilizando el $p$-value:\medskip
Calcular el valor de $p$, la probabilidad de que $z$ sea mayor que $z=1.84$ o menor que $z=-1.84$, se tiene que $p=0.0658$. 

\item Si el intervalo de confianza que se construye contiene el valor del par\'ametro especificado por $H_{0}$, entonces ese valor es uno de los posibles valores del par\'ametro y $H_{0}$ no debe ser rechazada.

\item Si el valor hipot\'etico se encuentra fuera de los l\'imites de confianza, la hip\'otesis nula es rechazada al nivel de significancia $\alpha$.
\end{itemize}
\end{Ejem}

%---------------------------------------------------------
\subsubsection*{Prueba de Hip\'otesis para una Proporci\'on Binomial}
%---------------------------------------------------------

Para una muestra aleatoria de $n$ intentos id\'enticos, de una poblaci\'on binomial, la proporci\'on muesrtal $\hat{p}$ tiene una distribuci\'on aproximadamente normal cuando $n$ es grande, con media $p$ y error est\'andar
\begin{eqnarray}SE=\sqrt{\frac{pq}{n}}.\end{eqnarray}
La prueba de hip\'otesis de la forma
\begin{eqnarray*}
H_{0}&:&p=p_{0}\\
H_{1}&:&p>p_{0}\textrm{, o }p<p_{0}\textrm{ o }p\neq p_{0}
\end{eqnarray*}
El estad\'istico de prueba se construye con el mejor estimador de la proporci\'on verdadera, $\hat{p}$, con el estad\'istico de prueba $z$, que se distribuye normal est\'andar.

El procedimiento es
\begin{itemize}
\item[1) ] Hip\'otesis nula: $H_{0}:p=p_{0}$
\item[2) ] Hip\'otesis alternativa

\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:p>p_{0}$ & $p\neq p_{0}$\\
$H_{1}:p<p_{0}$ & \\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
z=\frac{\hat{p}-p_{0}}{\sqrt{\frac{pq}{n}}},\hat{p}=\frac{x}{n}
\end{eqnarray}
donde $x$ es el n\'umero de \'exitos en $n$ intentos binomiales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%--------------------------------------------------------------------------------
\subsubsection{Prueba de Hip\'otesis diferencia entre dos Proporciones Binomiales}
%--------------------------------------------------------------------------------

Cuando se tienen dos muestras aleatorias independientes de dos poblaciones binomiales, el objetivo del experimento puede ser la diferencia $\left(p_{1}-p_{2}\right)$ en las proporciones de individuos u objetos que poseen una caracter\'istica especifica en las dos poblaciones. En este caso se pueden utilizar los estimadores de las dos proporciones $\left(\hat{p}_{1}-\hat{p}_{2}\right)$ con error est\'andar dado por
\begin{eqnarray}SE=\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}},\end{eqnarray}
considerando el estad\'istico $z$ con un nivel de significancia $\left(1-\alpha\right)100\%$

La hip\'otesis nula a probarse es de la forma
\begin{itemize}
\item[$H_{0}$: ] $p_{1}=p_{2}$ o equivalentemente $\left(p_{1}-p_{2}\right)=0$, contra una hip\'otesis alternativa $H_{1}$ de una o dos colas.
\end{itemize}

Para estimar el error est\'andar del estad\'istico $z$, se debe de utilizar el hecho de que suponiendo que $H_{0}$ es verdadera, las dos proporciones son iguales a alg\'un valor com\'un, $p$. Para obtener el mejor estimador de $p$ es
\begin{eqnarray}p=\frac{\textrm{n\'umero total de \'exitos}}{\textrm{N\'umero total de pruebas}}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}.\end{eqnarray}

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\left(p_{1}-p_{2}\right)=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(p_{1}-p_{2}\right)>0$ & $H_{1}:\left(p_{1}-p_{2}\right)\neq 0$\\ 
$H_{1}:\left(p_{1}-p_{2}\right)<0$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{p_{1}q_{1}}{n_{1}}+\frac{p_{2}q_{2}}{n_{2}}}}=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)}{\sqrt{\frac{pq}{n_{1}}+\frac{pq}{n_{2}}}},
\end{eqnarray}
donde $\hat{p_{1}}=x_{1}/n_{1}$ y $\hat{p_{2}}=x_{2}/n_{2}$ , dado que el valor com\'un para $p_{1}$ y $p_{2}$ es $p$, entonces $\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$ y por tanto el estad\'istico de prueba es
\begin{eqnarray}
z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\hat{q}}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}.
\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{\alpha}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:p<p_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%----------------------------------------------------------------
\subsection{Muestras Peque\~nas}
%----------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu=\mu_{0}$,
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu>\mu_{0}$ & $H_{1}:\mu\neq \mu_{0}$\\ 
$H_{1}:\mu<\mu0$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
t=\frac{\overline{x}-\mu_{0}}{\sqrt{\frac{s^{2}}{n}}},
\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Diferencia entre dos medias poblacionales: MAI}
%------------------------------------------------------------------------------------

Cuando los tama\~nos de muestra son peque\ ~nos, no se puede asegurar que las medias muestrales sean normales, pero si las poblaciones originales son normales, entonces la distribuci\'on muestral de la diferencia de las medias muestales, $\left(\overline{x}_{1}-\overline{x}_{2}\right)$, ser\'a normal con media $\left(\mu_{1}-\mu_{2}\right)$ y error est\'andar \begin{eqnarray}ES=\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}.\end{eqnarray}


\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\mu_{1}-\mu_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\mu_{1}-\mu_{2}\right)>D_{0}$ & $H_{1}:\left(\mu_{1}-\mu_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}t=\frac{\left(\overline{x}_{1}-\overline{x}_{2}\right)-D_{0}}{\sqrt{\frac{s^{2}_{1}}{n_{1}}+\frac{s^{2}_{2}}{n_{2}}}}\end{eqnarray}


donde \begin{eqnarray}s^{2}=\frac{\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}}{n_{1}+n_{2}-2}.\end{eqnarray}

\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$z>z_{0}$ & \\
$z<-z_{\alpha}$ cuando $H_{1}:\left(\mu_{1}-\mu_{2}\right)<D_{0}$&$z>z_{\alpha/2}$ o $z<-z_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}

Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Diferencia entre dos medias poblacionales: Diferencias Pareadas}
%------------------------------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\mu_{d}=0$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}:\mu_{d}$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\mu_{d}>0$ & $H_{1}:\mu_{d}\neq 0$\\ 
$H_{1}:\mu_{d}<0$&\\
\end{tabular}
\end{center}

\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
t=\frac{\overline{d}}{\sqrt{\frac{s_{d}^{2}}{n}}}
\end{eqnarray}
donde $n$ es el n\'umero de diferencias pareadas, $\overline{d}$ es la media de las diferencias muestrales, y $s_{d}$ es la desviaci\'on est\'andar de las diferencias muestrales.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$t>t_{\alpha}$ & \\
$t<-t_{\alpha}$ cuando $H_{1}:\mu<mu_{0}$&$t>t_{\alpha/2}$ o $t<-t_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
Los valores cr\'iticos de $t$, $t_{-\alpha}$ y $t_{\alpha/2}$ est\'an basados en $\left(n_{1}+n_{2}-2\right)$ grados de libertad.
\end{itemize}
%------------------------------------------------------------------------------------
\subsubsection{Inferencias con respecto a la Varianza Poblacional}
%------------------------------------------------------------------------------------
\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula:} $H_{0}:\sigma^{2}=\sigma^{2}_{0}$
\item[2) ] \textbf{Hip\'otesis Alternativa: } $H_{1}$
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\sigma^{2}>\sigma^{2}_{0}$ & $H_{1}:\sigma^{2}\neq \sigma^{2}_{0}$\\ 
$H_{1}:\sigma^{2}<\sigma^{2}_{0}$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}
\chi^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}_{0}},
\end{eqnarray}
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$\chi^{2}>\chi^{2}_{\alpha}$ & \\
$\chi^{2}<\chi^{2}_{\left(1-\alpha\right)}$ cuando $H_{1}:\chi^{2}<\chi^{2}_{0}$&$\chi^{2}>\chi^{2}_{\alpha/2}$ o $\chi^{2}<\chi^{2}_{\left(1-\alpha/2\right)}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}

Los valores cr\'iticos de $\chi^{2}$,est\'an basados en $\left(n_{1}+\right)$ grados de libertad.

\end{itemize}

%------------------------------------------------------------------------------------
\subsubsection{Comparaci\'on de dos varianzas poblacionales}
%------------------------------------------------------------------------------------

\begin{itemize}
\item[1) ] \textbf{Hip\'otesis Nula} $H_{0}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)=D_{0}$,\medskip

donde $D_{0}$ es el valor, la diferencia, espec\'ifico que se desea probar. En algunos casos se querr\'a demostrar que no hay diferencia alguna, es decir $D_{0}=0$.

\item[2) ] \textbf{Hip\'otesis Alternativa}
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)>D_{0}$ & $H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)\neq D_{0}$\\ 
$H_{1}:\left(\sigma^{2}_{1}-\sigma^{2}_{2}\right)<D_{0}$&\\
\end{tabular}
\end{center}
\item[3) ] Estad\'istico de prueba:
\begin{eqnarray}F=\frac{s_{1}^{2}}{s_{2}^{2}}\end{eqnarray}
donde $s_{1}^{2}$ es la varianza muestral m\'as grande.
\item[4) ] Regi\'on de rechazo: rechazar $H_{0}$ cuando
\begin{center}
\begin{tabular}{cc}\hline
\textbf{Prueba de una Cola} & \textbf{Prueba de dos colas}\\\hline
$F>F_{\alpha}$ & $F>F_{\alpha/2}$\\
 cuando $p<\alpha$&\\
\end{tabular}
\end{center}
\end{itemize}

%---------------------------------------------------------
\subsection{Estimaci\'on por intervalos}
%---------------------------------------------------------

Recordemos que $S^{2}$ es un estimador insesgado de $\sigma^{2}$, entonces se tiene la siguiente definici\'on 
\begin{Def}
Sean $\hat{\theta}_{1}$ y $\hat{\theta}_{2}$ dos estimadores insesgados de $\theta$, par\'ametro poblacional. Si $\sigma_{\hat{\theta}_{1}}^{2}<\sigma_{\hat{\theta}_{2}}^{2}$, decimos que $\hat{\theta}_{1}$ un estimador m\'as eficaz de $\theta$ que $\hat{\theta}_{2}$.
\end{Def}

Algunas observaciones que es preciso realizar
\begin{Note}
\begin{enumerate}
\item[a) ]Para poblaciones normales, $\overline{X}$ y $\tilde{X}$ son estimadores insesgados de $\mu$, pero con $\sigma_{\overline{X}}^{2}<\sigma_{\tilde{X}_{2}}^{2}$.
%\end{Note}

%\begin{Note}
\item[b) ]Para las estimaciones por intervalos de $\theta$, un intervalo de la forma $\hat{\theta}_{L}<\theta<\hat{\theta}_{U}$,  $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ dependen del valor de $\hat{\theta}$.
\item[c) ]Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, entonces $\hat{\theta}\rightarrow\mu$.
%\end{Note}
\end{enumerate}
\end{Note}


\begin{Note}
Para $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, si $n\rightarrow\infty$, %entonces $\hat{\theta}\rightarrow\mu$.
\end{Note}


\begin{enumerate}
\item[d) ]Para $\hat{\theta}$ se determinan $\hat{\theta}_{L}$ y $\hat{\theta}_{U}$ de modo tal que 
\begin{eqnarray}
P\left\{\hat{\theta}_{L}<\hat{\theta}<\hat{\theta}_{U}\right\}=1-\alpha,
\end{eqnarray}
con $\alpha\in\left(0,1\right)$. Es decir, $\theta\in\left(\hat{\theta}_{L},\hat{\theta}_{U}\right)$ es un intervalo de confianza del $100\left(1-\alpha\right)\%$.

\item[e) ] De acuerdo con el TLC se espera que la distribuci\'on muestral de $\overline{X}$ se distribuye aproximadamente normal con media $\mu_{X}=\mu$ y desviaci\'on est\'andar $\sigma_{\overline{X}}=\frac{\sigma}{\sqrt{n}}$.

\end{enumerate}

Para $Z_{\alpha/2}$ se tiene $P\left\{-Z_{\alpha/2}<Z<Z_{\alpha/2}\right\}=1-\alpha$, donde $Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}$. Entonces
\begin{eqnarray}P\left\{-Z_{\alpha/2}<\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha/2}\right\}=1-\alpha,\end{eqnarray} es equivalente a 
\begin{eqnarray}P\left\{\overline{X}-Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}<\mu<\overline{X}+Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.\end{eqnarray}

\begin{enumerate}
\item[f) ]Si $\overline{X}$ es la media muestral de una muestra de tama\~no $n$ de una poblaci\'on con varianza conocida $\sigma^{2}$, el intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\mu$ es \begin{eqnarray}\mu\in\left(\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\overline{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right).\end{eqnarray}

\item[g) ] Para muestras peque\~nas de poblaciones no normales, no se puede esperar que el grado de confianza sea preciso.
\item[h) ] Para $n\geq30$, con distribuci\'on de forma no muy sesgada, se pueden tener buenos resultados.
\end{enumerate}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a a $z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$, error entre $\overline{X}$ y $\mu$.
\end{Teo}

\begin{Teo}
Si $\overline{X}$ es un estimador de $\mu$, podemos tener $100\left(1-\alpha\right)\%$  de confianza en que el error no exceder\'a una cantidad $e$ cuando el tama\~no de la muestra es \begin{eqnarray}n=\left(\frac{z_{\alpha/2}\sigma}{e}\right)^{2}.\end{eqnarray}
\end{Teo}
\begin{Note}
Para intervalos unilaterales
\begin{eqnarray}P\left\{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}<Z_{\alpha}\right\}=1-\alpha\end{eqnarray}
\end{Note}

equivalentemente
\begin{eqnarray}P\left\{\mu<\overline{X}+Z_{\alpha}\frac{\sigma}{\sqrt{n}}\right\}=1-\alpha.\end{eqnarray}
Si $\overline{X}$ es la media de una muestra aleatoria de tama\~no $n$  a partir de una poblaci\'on con varianza $\sigma^{2}$, los l\'imites de confianza unilaterales del   $100\left(1-\alpha\right)\%$  de confianza para $\mu$ est\'an dados por
\begin{itemize}
\item[a) ] L\'imite unilateral superior: $\overline{x}+z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item[b) ] L\'imite unilateral inferior: $\overline{x}-z_{\alpha}\frac{\sigma}{\sqrt{n}}$
\item[c) ] Para $\sigma$ desconocida recordar que $T=\frac{\overline{x}-\mu}{s/\sqrt{n}}\sim t_{n-1}$, donde $s$ es la desviaci\'on est\'andar de la muestra. Entonces
\begin{eqnarray}
P\left\{-t_{\alpha/2}<T<t_{\alpha/2}\right\}=1-\alpha,\textrm{equivalentemente}\\
P\left\{\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}<\mu<\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right\}=1-\alpha.
\end{eqnarray}

\item[d) ] Un intervalo de confianza del $100\left(1-\alpha\right)\%$  de confianza para $\mu$, $\sigma^{2}$ desconocida y poblaci\'on normal es
 
 \begin{eqnarray}\mu\in\left(\overline{x}-t_{\alpha/2}\frac{s}{\sqrt{n}},\overline{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}\right)\end{eqnarray}, 
 
 donde $t_{\alpha/2}$ es una $t$-student con $\nu=n-1$ grados de libertad.
\item[e) ] Los l\'imites unilaterales para $\mu$ con $\sigma$ desconocida son $\overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}$ y $\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}$.

\item[f) ] Cuando la poblaci\'on no es normal, $\sigma$ desconocida y $n\geq30$, $\sigma$ se puede reemplazar por $s$ para obtener el intervalo de confianza para muestras grandes:
\begin{eqnarray}\overline{X}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.\end{eqnarray}

\item[g) ] El estimador de $\overline{X}$ de $\mu$,  $\sigma$ desconocida, la varianza de $\sigma_{\overline{X}}^{2}=\frac{\sigma^{2}}{n}$, el error est\'andar de $\overline{X}$ es $\sigma/\sqrt{n}$.

\item[h) ] Si $\sigma$ es desconocida y la poblaci\'on es normal, $s\rightarrow\sigma$ y se incluye el error est\'andar $s/\sqrt{n}$, entonces \begin{eqnarray}\overline{x}\pm t_{\alpha/2}\frac{s}{\sqrt{n}}.\end{eqnarray}
\end{itemize}

%---------------------------------------------------------
\subsubsection{Intervalos de confianza sobre la varianza}
%---------------------------------------------------------

Supongamos que  $X$ se distribuye normal $\left(\mu,\sigma^{2}\right)$, desconocidas. Sea $X_{1},X_{2},\ldots,X_{n}$ muestra aleatoria de tama\~no $n$ , $s^{2}$ la varianza muestral.

Se sabe que $X^{2}=\frac{\left(n-1\right)s^{2}}{\sigma^{2}}$ se distribuye $\chi^{2}_{n-1}$ grados de libertad. Su intervalo de confianza es
\begin{eqnarray}
\begin{array}{l}
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\chi^{2}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\chi^{2}_{1-\frac{\alpha}{2},n-1}\leq\frac{\left(n-1\right)s^{2}}{\sigma^{2}}\leq\chi^{2}_{\frac{\alpha}{2},n-1}\right\}=1-\alpha\\
P\left\{\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}}\leq\sigma^{2}\leq\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right\}=1-\alpha,
\end{array}
\end{eqnarray}
es decir

\begin{eqnarray}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right],
\end{eqnarray}
los intervalos unilaterales son
\begin{eqnarray}
\sigma^{2}\in\left[\frac{\left(n-1\right)s^{2}}{\chi^{2}_{\frac{\alpha}{2},n-1}},\infty\right],
\end{eqnarray}
y
\begin{eqnarray}
\sigma^{2}\in\left[-\infty,\frac{\left(n-1\right)s^{2}}{\chi^{2}_{1-\frac{\alpha}{2},n-1}}\right].
\end{eqnarray}

%---------------------------------------------------------
\subsubsection{Intervalos de confianza para proporciones}
%---------------------------------------------------------

Supongamos que se tienen una muestra de tama\~no $n$ de una poblaci\'on grande pero finita, y supongamos que $X$, $X\leq n$, pertenecen a la clase de inter\'es, entonces \begin{eqnarray}\hat{p}=\frac{\overline{X}}{n},\end{eqnarray} es el estimador puntual de la proporci\'on de la poblaci\'on que pertenece a dicha clase. $n$ y $p$ son los par\'ametros de la distribuci\'on binomial, entonces \begin{eqnarray}\hat{p}\sim N\left(p,\frac{p\left(1-p\right)}{n}\right)\end{eqnarray} aproximadamente si $p$ es distinto de $0$ y $1$; o si $n$ es suficientemente grande. Entonces
\begin{eqnarray}
Z=\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\sim N\left(0,1\right),\textrm{ aproximadamente.}
\end{eqnarray}
 
Entonces
\begin{eqnarray}
\begin{array}{l}
1-\alpha=P\left\{-z_{\alpha/2}\leq\frac{\hat{p}-p}{\sqrt{\frac{p\left(1-p\right)}{n}}}\leq z_{\alpha/2}\right\}\\
=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{p\left(1-p\right)}{n}}\right\}
\end{array}
\end{eqnarray}
con $\sqrt{\frac{p\left(1-p\right)}{n}}$ error est\'andar del estimador puntual $p$. Una soluci\'on para determinar el intervalo de confianza del par\'ametro $p$ (desconocido) es

\begin{eqnarray}
1-\alpha=P\left\{\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\leq p\leq \hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right\}
\end{eqnarray}
entonces los intervalos de confianza, tanto unilaterales como de dos colas son: 
\begin{itemize}
\item[a) ] $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$,

\item[b) ] $p\in \left(-\infty,\hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\right)$,

\item[c) ] $p\in \left(\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}},\infty\right)$;

\end{itemize}
para minimizar el error est\'andar, se propone que el tama\~no de la muestra sea \begin{eqnarray}n= \left(\frac{z_{\alpha/2}}{E}\right)^{2}p\left(1-p\right),\end{eqnarray} donde $$E=\mid p-\hat{p}\mid.$$


%---------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas conocidas}
%---------------------------------------------------------

Sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza conocida $\sigma_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza conocida $\sigma_{2}^{2}$. Se busca encontrar un intervalo de confianza de $100\left(1-\alpha\right)\%$ de la diferencia entre medias $\mu_{1}$ y $\mu_{2}$. Sean $X_{11},X_{12},\ldots,X_{1n_{1}}$ muestra aleatoria de $n_{1}$ observaciones de $X_{1}$, y sean $X_{21},X_{22},\ldots,X_{2n_{2}}$ muestra aleatoria de $n_{2}$ observaciones de $X_{2}$.\medskip

Sean $\overline{X}_{1}$ y $\overline{X}_{2}$, medias muestrales, entonces el estad\'sitico 
\begin{eqnarray}
Z=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\sim N\left(0,1\right),\end{eqnarray}
si $X_{1}$ y $X_{2}$ son normales o aproximadamente normales si se aplican las condiciones del Teorema de L\'imite Central respectivamente. Entonces se tiene
\begin{eqnarray}
1-\alpha&=& P\left\{-Z_{\alpha/2}\leq Z\leq Z_{\alpha/2}\right\}=P\left\{-Z_{\alpha/2}\leq \frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}}\leq Z_{\alpha/2}\right\}\\
&=&P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\leq \mu_{1}-\mu_{2}\leq \left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right\}.
\end{eqnarray}

Entonces los intervalos de confianza unilaterales y de dos colas al $\left(1-\alpha\right)\%$ de confianza son 

\begin{itemize}
\item[a) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right],\end{eqnarray}

\item[b) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in \left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\right],\end{eqnarray}

\item[c) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in \left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-Z_{\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}},\infty\right].\end{eqnarray}
\end{itemize}

\begin{Note}
Si $\sigma_{1}$ y $\sigma_{2}$ son conocidas, o por lo menos se conoce una aproximaci\'on, y los tama\~nos de las muestras $n_{1}$ y $n_{2}$ son iguales, $n_{1}=n_{2}=n$, se puede determinar el tama\~no de la muestra para que el error al estimar $\mu_{1}-\mu_{2}$ usando $\overline{X}_{1}-\overline{X}_{2}$ sea menor que $E$ (valor del error deseado) al $\left(1-\alpha\right)\%$ de confianza. El tama\~no $n$ de la muestra requerido para cada muestra es
\begin{eqnarray}
n=\left(\frac{Z_{\alpha/2}}{E}\right)^{2}\left(\sigma_{1}^{2}+\sigma_{2}^{2}\right).
\end{eqnarray}
\end{Note}

%------------------------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas desconocidas e iguales}
%------------------------------------------------------------------------------------------------------------------

\begin{itemize}
\item[a) ] Si $n_{1},n_{2}\geq30$ se pueden utilizar los intervalos de la distribuci\'on normal para varianza conocida

\item[b) ] Si $n_{1},n_{2}$ son muestras peque\~nas, supongase que las poblaciones para $X_{1}$ y $X_{2}$ son normales con varianzas desconocidas y con base en el intervalo de confianza para distribuciones $t$-student
\end{itemize}


Supongamos que $X_{1}$ es una variable aleatoria con media $\mu_{1}$ y varianza $\sigma_{1}^{2}$, $X_{2}$ es una variable aleatoria con media $\mu_{2}$ y varianza $\sigma_{2}^{2}$. Todos los par\'ametros son desconocidos. Sin embargo sup\'ongase que es razonable considerar que $\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}$.\medskip

Nuevamente sean $X_{1}$ y $X_{2}$ variables aleatorias independientes. $X_{1}$ con media desconocida $\mu_{1}$ y varianza muestral $S_{1}^{2}$; y $X_{2}$ con media desconocida $\mu_{2}$ y varianza muestral $S_{2}^{2}$. Dado que $S_{1}^{2}$ y $S_{2}^{2}$ son estimadores de $\sigma_{1}^{2}$, se propone el estimador $S$ de $\sigma^{2}$ como 

\begin{eqnarray}
S_{p}^{2}=\frac{\left(n_{1}-1\right)S_{1}^{2}+\left(n_{2}-1\right)S_{2}^{2}}{n_{1}+n_{2}-2},
\end{eqnarray}
entonces, el estad\'istico para $\mu_{1}-\mu_{2}$ es

\begin{eqnarray}
t_{\nu}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}},
\end{eqnarray}
donde $t_{\nu}$ es una $t$ de student con $\nu=n_{1}+n_{2}-2$ grados de libertad.\medskip

Por lo tanto

\begin{eqnarray}
\begin{array}{l}
1-\alpha=P\left\{-t_{\alpha/2,\nu}\leq t\leq t_{\alpha/2,\nu}\right\}\\
=P\left\{\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\leq t\leq\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right\},
\end{array}
\end{eqnarray}

luego, los intervalos de confianza del $\left(1-\alpha\right)\%$ para $\mu_{1}-|mu_{2}$ son 
\begin{itemize}
\item[a) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right].\end{eqnarray}


\item[b) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in\left[-\infty,\left(\overline{X}_{1}-\overline{X}_{2}\right)+ t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right].\end{eqnarray}

\item[c) ] \begin{eqnarray}\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)- t_{\alpha/2,\nu}S_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}},\infty\right].\end{eqnarray}
\end{itemize}


%------------------------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para dos muestras: Varianzas desconocidas diferentes}
%------------------------------------------------------------------------------------------------------------------

Si no se tiene certeza de que $\sigma_{1}^{2}=\sigma_{2}^{2}$, se propone el estad\'istico
\begin{eqnarray}
t^{*}=\frac{\left(\overline{X}_{1}-\overline{X}_{2}\right)-\left(\mu_{1}-\mu_{2}\right)}{\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}},
\end{eqnarray}
que se distribuye $t$-student con $\nu$ grados de libertad, donde

\begin{eqnarray}
\nu=\frac{\left(\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}\right)^{2}}{\frac{S_{1}^{2}/n_{1}}{n_{1}+1}+\frac{S_{2}^{2}/n_{2}}{n_{2}+1}}-2.
\end{eqnarray}


Entonces el intervalo de confianza de aproximadamente el $100\left(1-\alpha\right)\%$ para $\mu_{1}-\mu_{2}$ con $\sigma_{1}^{2}\neq\sigma_{2}^{2}$ es
\begin{eqnarray}
\mu_{1}-\mu_{2}\in\left[\left(\overline{X}_{1}-\overline{X}_{2}\right)-t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}},\left(\overline{X}_{1}-\overline{X}_{2}\right)+t_{\alpha/2,\nu}\sqrt{\frac{S_{1}^{2}}{n_{1}}+\frac{S_{2}^{2}}{n_{2}}}\right].
\end{eqnarray}

%------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para raz\'on de Varianzas}
%------------------------------------------------------------------------------

Supongamos que se toman dos muestras aleatorias independientes de las dos poblaciones de inter\'es. Sean $X_{1}$ y $X_{2}$ variables normales independientes con medias desconocidas $\mu_{1}$ y $\mu_{2}$ y varianzas desconocidas $\sigma_{1}^{2}$ y $\sigma_{2}^{2}$ respectivamente. Se busca un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\sigma_{1}^{2}/\sigma_{2}^{2}$. Supongamos $n_{1}$ y $n_{2}$ muestras aleatorias de $X_{1}$ y $X_{2}$ y sean $S_{1}^{2}$ y $S_{2}^{2}$ varianzas muestralres. Se sabe que 
\begin{eqnarray}F=\frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}},\end{eqnarray}
se distribuye $F$ con $n_{2}-1$ y $n_{1}-1$ grados de libertad.


Por lo tanto
\begin{eqnarray}
\begin{array}{l}
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq F\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha,\\
P\left\{F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{S_{2}^{2}/\sigma_{2}^{2}}{S_{1}^{2}/\sigma_{1}^{2}}\leq F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha,
\end{array}
\end{eqnarray}
luego entonces
\begin{eqnarray}
P\left\{\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}\leq \frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right\}=1-\alpha.
\end{eqnarray}
en consecuencia

\begin{eqnarray}
\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\in \left[\frac{S_{1}^{2}}{S_{2}^{2}}F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}, \frac{S_{1}^{2}}{S_{2}^{2}}F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}\right],
\end{eqnarray}
donde
\begin{eqnarray}
F_{1-\frac{\alpha}{2},n_{2}-1,n_{1}-1}=\frac{1}{F_{\frac{\alpha}{2},n_{2}-1,n_{1}-1}}.
\end{eqnarray}

%------------------------------------------------------------------------------------------------
\subsubsection{Intervalos de confianza para diferencia de proporciones}
%------------------------------------------------------------------------------------------------


Sean dos proporciones de inter\'es $p_{1}$ y $p_{2}$. Se busca un intervalo para $p_{1}-p_{2}$ al $100\left(1-\alpha\right)\%$. Sean dos muestras independientes de tama\~no $n_{1}$ y $n_{2}$ de poblaciones infinitas de modo que $X_{1}$ y $X_{2}$ variables aleatorias binomiales independientes con par\'ametros $\left(n_{1},p_{1}\right)$ y $\left(n_{2},p_{2}\right)$.  $X_{1}$ y $X_{2}$ son  el n\'umero de observaciones que pertenecen a la clase de inter\'es correspondientes. Entonces $\hat{p}_{1}=\frac{X_{1}}{n_{1}}$ y $\hat{p}_{2}=\frac{X_{2}}{n_{2}}$ son estimadores de $p_{1}$ y $p_{2}$ respectivamente. Supongamos que se cumple la aproximaci\'on  normal a la binomial, entonces

\begin{eqnarray}
Z=\frac{\left(\hat{p}_{1}-\hat{p}_{2}\right)-\left(p_{1}-p_{2}\right)}{\sqrt{\frac{p_{1}\left(1-p_{1}\right)}{n_{1}}-\frac{p_{2}\left(1-p_{2}\right)}{n_{2}}}}\sim N\left(0,1\right)\textrm{aproximadamente}
\end{eqnarray}
por tanto

\begin{eqnarray}
\left(\hat{p}_{1}-\hat{p}_{2}\right)-Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}+\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}\leq p_{1}-p_{2}\leq\left(\hat{p}_{1}-\hat{p}_{2}\right)+Z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}_{1}\left(1-\hat{p}_{1}\right)}{n_{1}}-\frac{\hat{p}_{2}\left(1-\hat{p}_{2}\right)}{n_{2}}}
\end{eqnarray}



%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{An\'alisis de Regresion Lineal (RL)}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>


En muchos problemas hay dos o m\'as variables relacionadas, para medir el grado de relaci\'on se utiliza el \textbf{an\'alisis de regresi\'on}. Supongamos que se tiene una \'unica variable dependiente, $y$, y varias  variables independientes, $x_{1},x_{2},\ldots,x_{n}$. La variable $y$ es una varaible aleatoria, y las variables independientes pueden ser distribuidas independiente o conjuntamente.  A la relaci\'on entre estas variables se le denomina modelo regresi\'on de $y$ en $x_{1},x_{2},\ldots,x_{n}$, por ejemplo $y=\phi\left(x_{1},x_{2},\ldots,x_{n}\right)$, lo que se busca es una funci\'on que mejor aproxime a $\phi\left(\cdot\right)$.



%------------------------------------------------------------------
\subsection{Regresi\'on Lineal Simple (RLS)}
%------------------------------------------------------------------

Supongamos que de momento solamente se tienen una variable independiente $x$, para la variable de respuesta $y$. Y supongamos que la relaci\'on que hay entre $x$ y $y$ es una l\'inea recta, y que para cada observaci\'on de $x$, $y$ es una variable aleatoria. El valor esperado de $y$ para cada valor de $x$ es
\begin{eqnarray*}
E\left(y|x\right)=\beta_{0}+\beta_{1}x,
\end{eqnarray*}
$\beta_{0}$ es la ordenada al or\'igen y  $\beta_{1}$ la pendiente de la recta en cuesti\'on, ambas constantes desconocidas. 

Supongamos que cada observaci\'on $y$ se puede describir por el modelo
\begin{eqnarray}\label{Modelo.Regresion.Original}
y=\beta_{0}+\beta_{1}x+\epsilon
\end{eqnarray}

donde $\epsilon$ es un error aleatorio con media cero y varianza $\sigma^{2}$. Para cada valor $y_{i}$ se tiene $\epsilon_{i}$ variables aleatorias no correlacionadas, cuando se incluyen en el modelo \ref{Modelo.Regresion.Original}, este se le llama \textit{modelo de regresi\'on lineal simple}. Suponga que se tienen $n$ pares de observaciones $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{n},y_{n}\right)$,  estos datos pueden utilizarse para estimar los valores de $\beta_{0}$ y $\beta_{1}$. Esta estimaci\'on realiza por el \textbf{m\'etodos de m\'inimos cuadrados}.

Entonces la ecuaci\'on (\ref{Modelo.Regresion.Original}) se puede reescribir como
\begin{eqnarray}\label{Modelo.Regresion.dos.Original}
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i},\textrm{ para }i=1,2,\ldots,n.
\end{eqnarray}
Si consideramos la suma de los cuadrados de los errores aleatorios, es decir, el cuadrado de la diferencia entre las observaciones con la recta de regresi\'on 
\begin{eqnarray}
L=\sum_{i=1}^{n}\epsilon^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}.
\end{eqnarray}

Para obtener los estimadores por m\'inimos cuadrados de $\beta_{0}$ y $\beta_{1}$,  $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, es preciso calcular las derivadas parciales con respecto a $\beta_{0}$ y $\beta_{1}$,  igualar a cero y  resolver el sistema de ecuaciones lineales resultante:
\begin{eqnarray*}
\frac{\partial L}{\partial \beta_{0}}=0\textrm{, }\frac{\partial L}{\partial \beta_{1}}=0.
\end{eqnarray*}
Evaluando en $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$, se tiene 

\begin{eqnarray*}
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)&=&0,\\
-2\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)x_{i}&=&0,
\end{eqnarray*}
simplificando
\begin{eqnarray*}
n\hat{\beta}_{0}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}&=&\sum_{i=1}^{n}y_{i},\\
\hat{\beta}_{0}\sum_{i=1}^{n}x_{i}+\hat{\beta}_{1}\sum_{i=1}^{n}x_{i}^{2}&=&\sum_{i=1}^{n}x_{i}y_{i}.
\end{eqnarray*}

Las ecuaciones anteriores se les denominan \textit{ecuaciones normales de m\'inimos cuadrados} con soluci\'on
\begin{eqnarray}\label{Ec.Normales.Min.Cuadrados}
\hat{\beta}_{0}&=&\overline{y}-\hat{\beta}_{1}\overline{x},\\
\hat{\beta}_{1}&=&\frac{\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}y_{i}\right)\left(\sum_{i=1}^{n}x_{i}\right)}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}},
\end{eqnarray}
entonces el modelo de regresi\'on lineal simple ajustado es
\begin{eqnarray}
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x.
\end{eqnarray}

Se intrduce la siguiente notaci\'on
\begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2},\\
S_{xy}&=&\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)=\sum_{i=1}^{n}x_{i}y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right);
\end{eqnarray}
y por tanto

\begin{eqnarray}
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}.
\end{eqnarray}
%------------------------------------------------------------------
\subsection{Propiedades de los Estimadores $\hat{\beta}_{0}$ y $\hat{\beta}_{1}$}
%------------------------------------------------------------------

Las propiedades estad\'isticas de los estimadores de m\'inimos cuadrados son \'utiles para evaluar la suficiencia del modelo. Dado que $\hat{\beta}_{0}$ y  $\hat{\beta}_{1}$ son combinaciones lineales de las variables aleatorias $y_{i}$, tambi\'en resultan ser variables aleatorias.

A saber
\begin{eqnarray*}
E\left(\hat{\beta}_{1}\right)&=&E\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}y_{i}\left(x_{i}-\overline{x}\right)\right)=\frac{1}{S_{xx}}E\left(\sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\left(x_{i}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}}\left[\beta_{0}E\left(\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)\right)+E\left(\beta_{1}\sum_{k=1}^{n}x_{k}\left(x_{k}-\overline{x}\right)\right)+E\left(\sum_{k=1}^{n}\epsilon_{k}\left(x_{k}-\overline{x}\right)\right)\right]\\
&=&\frac{1}{S_{xx}}\beta_{1}S_{xx}=\beta_{1}.
\end{eqnarray*}

Por lo tanto 
\begin{equation}\label{Esperanza.Beta.1.Original}
E\left(\hat{\beta}_{1}\right)=\beta_{1},
\end{equation}
Es decir, $\hat{\beta_{1}}$ es un estimador insesgado. Ahora calculemos la varianza:
\begin{eqnarray*}
V\left(\hat{\beta}_{1}\right)&=&V\left(\frac{S_{xy}}{S_{xx}}\right)=\frac{1}{S_{xx}^{2}}V\left(\sum_{k=1}^{n}y_{k}\left(x_{k}-\overline{x}\right)\right)=\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}V\left(y_{k}\left(x_{k}-\overline{x}\right)\right)\\
&=&\frac{1}{S_{xx}^{2}}\sum_{k=1}^{n}\sigma^{2}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}^{2}}\sum_{k=1}^{n}\left(x_{k}-\overline{x}\right)^{2}=\frac{\sigma^{2}}{S_{xx}},
\end{eqnarray*}

por lo tanto

\begin{equation}\label{Varianza.Beta.1.Original}
V\left(\hat{\beta}_{1}\right)=\frac{\sigma^{2}}{S_{xx}}.
\end{equation}

Entonces tenemos la siguiente proposici\'on: 
\begin{Prop}
\begin{eqnarray}
E\left(\hat{\beta}_{0}\right)&=&\beta_{0},\\
V\left(\hat{\beta}_{0}\right)&=&\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right),\\
Cov\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)&=&-\frac{\sigma^{2}\overline{x}}{S_{xx}}.
\end{eqnarray}
\end{Prop}
Para estimar $\sigma^{2}$ es preciso definir la diferencia entre la observaci\'on $y_{k}$, y el valor predecido $\hat{y}_{k}$, es decir
\begin{eqnarray*}
e_{k}=y_{k}-\hat{y}_{k},\textrm{ se le denomina \textbf{residuo}.}
\end{eqnarray*}
La suma de los cuadrados de los errores de los reisduos, \textit{suma de cuadrados del error}:
\begin{eqnarray}
SC_{E}=\sum_{k=1}^{n}e_{k}^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{eqnarray}
sustituyendo $\hat{y}_{k}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}$ se obtiene
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}y_{k}^{2}-n\overline{y}^{2}-\hat{\beta}_{1}S_{xy}=S_{yy}-\hat{\beta}_{1}S_{xy},\\
E\left(SC_{E}\right)&=&\left(n-2\right)\sigma^{2},\textrm{ por lo tanto,}\\
\hat{\sigma}^{2}&=&\frac{SC_{E}}{n-2}=\mathbf{MC_{E}}\textrm{ es un estimador insesgado de }\sigma^{2}.
\end{eqnarray}

%------------------------------------------------------------------
\subsection{Prueba de Hip\'otesis en RLS}
%------------------------------------------------------------------

Para evaluar la suficiencia del modelo de regresi\'on lineal simple, es necesario lleva a cabo una prueba de hip\'otesis respecto de los par\'ametros del modelo as\'i como de la construcci\'on de intervalos de confianza. Para poder realizar la prueba de hip\'otesis sobre la pendiente y la ordenada al or\'igen de la recta de regresi\'on es necesario hacer el supuesto de que el error $\epsilon_{i}$ se distribuye normalmente, es decir $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$. Suponga que se desea probar la hip\'otesis de que la pendiente es igual a una constante, $\beta_{0,1}$ las hip\'otesis Nula y Alternativa son:

\begin{center}
\begin{itemize}
\item[$H_{0}$: ] $\beta_{1}=\beta_{1,0}$,
\item[$H_{1}$: ]$\beta_{1}\neq\beta_{1,0}$.
\end{itemize}
\end{center}

donde dado que las $\epsilon_{i} \sim N\left(0,\sigma^{2}\right)$, se tiene que $y_{i}$ son variables aleatorias normales $N\left(\beta_{0}+\beta_{1}x_{1},\sigma^{2}\right)$. De las ecuaciones (\ref{Ec.Normales.Min.Cuadrados}) se desprende que $\hat{\beta}_{1}$ es combinaci\'on lineal de variables aleatorias normales independientes, es decir, $\hat{\beta}_{1}\sim N\left(\beta_{1},\sigma^{2}/S_{xx}\right)$, recordar las ecuaciones (\ref{Esperanza.Beta.1.Original}) y (\ref{Varianza.Beta.1.Original}).
Entonces se tiene que el estad\'istico de prueba apropiado es
\begin{equation}\label{Estadistico.Beta.1.Original}
t_{0}=\frac{\hat{\beta}_{1}-\hat{\beta}_{1,0}}{\sqrt{MC_{E}/S_{xx}}}
\end{equation}
que se distribuye $t$ con $n-2$ grados de libertad bajo $H_{0}:\beta_{1}=\beta_{1,0}$. Se rechaza $H_{0}$ si 
\begin{equation}\label{Zona.Rechazo.Beta.1}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

Para $\beta_{0}$ se puede proceder de manera an\'aloga para
\begin{itemize}
\item[$H_{0}:$] $\beta_{0}=\beta_{0,0}$,
\item[$H_{1}:$] $\beta_{0}\neq\beta_{0,0}$,
\end{itemize}
con $\hat{\beta}_{0}\sim N\left(\beta_{0},\sigma^{2}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)\right)$, por lo tanto
\begin{equation}\label{Estadistico.Beta.0.Original}
t_{0}=\frac{\hat{\beta}_{0}-\beta_{0,0}}{MC_{E}\left[\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right]},
\end{equation}
con el que rechazamos la hip\'otesis nula si
\begin{equation}\label{Zona.Rechazo.Beta.0.Original}
t_{0}|>t_{\alpha/2,n-2}.
\end{equation}

\begin{itemize}
\item No rechazar $H_{0}:\beta_{1}=0$ es equivalente a decir que no hay relaci\'on lineal entre $x$ y $y$.
\item Alternativamente, si $H_{0}:\beta_{1}=0$ se rechaza, esto implica que $x$ explica la variabilidad de $y$, es decir, podr\'ia significar que la l\'inea recta esel modelo adecuado.
\end{itemize}
El procedimiento de prueba para $H_{0}:\beta_{1}=0$ puede realizarse de la siguiente manera:
\begin{eqnarray*}
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\\
S_{yy}&=&\sum_{k=1}^{n}\left(y_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}+\hat{y}_{k}-\overline{y}\right)^{2}=\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)+\left(y_{k}-\hat{y}_{k}\right)\right]^{2}\\
&=&\sum_{k=1}^{n}\left[\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\left(y_{k}-\hat{y}_{k}\right)^{2}\right]\\
&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+2\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\\
\end{eqnarray*}

\begin{eqnarray*}
&&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\sum_{k=1}^{n}\overline{y}\left(y_{k}-\hat{y}_{k}\right)=\sum_{k=1}^{n}\hat{y}_{k}\left(y_{k}-\hat{y}_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)\\
&=&\sum_{k=1}^{n}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{k}\right)\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=\sum_{k=1}^{n}\hat{\beta}_{0}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\
&+&\sum_{k=1}^{n}\hat{\beta}_{1}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)
=\hat{\beta}_{0}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)\\&+&\hat{\beta}_{1}\sum_{k=1}^{n}x_{k}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)-\overline{y}\sum_{k=1}^{n}\left(y_{k}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{k}\right)=0+0+0=0.
\end{eqnarray*}

Por lo tanto, efectivamente se tiene
\begin{equation}\label{Suma.Total.Cuadrados.Original}
S_{yy}=\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}+\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2},
\end{equation}
donde se hacen las definiciones
\begin{eqnarray}
SC_{E}&=&\sum_{k=1}^{n}\left(\hat{y}_{k}-\overline{y}\right)^{2}\cdots\textrm{Suma de Cuadrados del Error}\\
SC_{R}&=&\sum_{k=1}^{n}\left(y_{k}-\hat{y}_{k}\right)^{2}\cdots\textrm{ Suma de Regresi\'on de Cuadrados}
\end{eqnarray}
Por lo tanto la ecuaci\'on (\ref{Suma.Total.Cuadrados.Original}) se puede reescribir como: 
\begin{equation}\label{Suma.Total.Cuadrados.Dos.Original}
S_{yy}=SC_{R}+SC_{E},
\end{equation}
recordemos que $SC_{E}=S_{yy}-\hat{\beta}_{1}S_{xy}$:
\begin{eqnarray*}
S_{yy}&=&SC_{R}+\left( S_{yy}-\hat{\beta}_{1}S_{xy}\right),\\
S_{xy}&=&\frac{1}{\hat{\beta}_{1}}SC_{R}.
\end{eqnarray*}
$S_{xy}$ tiene $n-1$ grados de libertad y $SC_{R}$ y $SC_{E}$ tienen 1 y $n-2$ grados de libertad respectivamente.

\begin{Prop}
\begin{equation}
E\left(SC_{R}\right)=\sigma^{2}+\beta_{1}S_{xx},
\end{equation}
adem\'as, $SC_{E}$ y $SC_{R}$ son independientes.
\end{Prop}
Recordemos que $\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}}$. Para $H_{0}:\beta_{1}=0$ verdadera,
\begin{eqnarray*}
F_{0}=\frac{SC_{R}/1}{SC_{E}/(n-2)}=\frac{MC_{R}}{MC_{E}},
\end{eqnarray*}
se distribuye $F_{1,n-2}$, y se rechazar\'ia $H_{0}$ si $F_{0}>F_{\alpha,1,n-2}$. El procedimiento de prueba de hip\'otesis puede presentarse como la tabla de an\'alisis de varianza siguiente

\begin{center}
\begin{tabular}{lcccc}\hline
Fuente de & Suma de  &  Grados de  & Media  & $F_{0}$ \\ 
 variaci\'on & Cuadrados & Libertad & Cuadr\'atica & \\\hline
 Regresi\'on & $SC_{R}$ & 1 & $MC_{R}$  & $MC_{R}/MC_{E}$\\
 Error Residual & $SC_{E}$ & $n-2$ & $MC_{E}$ & \\\hline
 Total & $S_{yy}$ & $n-1$ & & \\\hline
\end{tabular} 
\end{center}

La prueba para la significaci\'on de la regresi\'on puede desarrollarse bas\'andose en la expresi\'on (\ref{Estadistico.Beta.1.Original}), con $\hat{\beta}_{1,0}=0$, es decir,
\begin{equation}\label{Estadistico.Beta.1.Cero}
t_{0}=\frac{\hat{\beta}_{1}}{\sqrt{MC_{E}/S_{xx}}}.
\end{equation}

Elevando al cuadrado ambos t\'erminos:
\begin{eqnarray*}
t_{0}^{2}=\frac{\hat{\beta}_{1}^{2}S_{xx}}{MC_{E}}=\frac{\hat{\beta}_{1}S_{xy}}{MC_{E}}=\frac{MC_{R}}{MC_{E}}.
\end{eqnarray*}
Observar que $t_{0}^{2}=F_{0}$, por tanto la prueba que se utiliza para $t_{0}$ es la misma que para $F_{0}$.
%----------------------------------------------------------------
\subsection{Estimaci\'on de Intervalos en RLS}
%----------------------------------------------------------------

Adem\'as de la estimaci\'on puntual para los par\'ametros $\beta_{1}$ y $\beta_{0}$, es posible obtener estimaciones del intervalo de confianza de estos par\'ametros. El ancho de estos intervalos de confianza es una medida de la calidad total de la recta de regresi\'on. Si los $\epsilon_{k}$ se distribuyen normal e independientemente, entonces
\begin{eqnarray*}
\begin{array}{ccc}
\frac{\left(\hat{\beta}_{1}-\beta_{1}\right)}{\sqrt{\frac{MC_{E}}{S_{xx}}}}&y &\frac{\left(\hat{\beta}_{0}-\beta_{0}\right)}{\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}}
\end{array}
\end{eqnarray*}
se distribuyen $t$ con $n-2$ grados de libertad. Por tanto un intervalo de confianza de $100\left(1-\alpha\right)\%$ para $\beta_{1}$ est\'a dado por
\begin{eqnarray}
\hat{\beta}_{1}-t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}\leq \beta_{1}\leq\hat{\beta}_{1}+t_{\alpha/2,n-2}\sqrt{\frac{MC_{E}}{S_{xx}}}.
\end{eqnarray}
De igual manera, para $\beta_{0}$ un intervalo de confianza al $100\left(1-\alpha\right)\%$ es
\begin{eqnarray}
\begin{array}{l}
\hat{\beta}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}\leq\beta_{0}\leq\hat{\beta}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left(\frac{1}{n}+\frac{\overline{x}^{2}}{S_{xx}}\right)}
\end{array}
\end{eqnarray}

%------------------------------------------------------------------
\subsection{Predicci\'on}
%------------------------------------------------------------------

Supongamos que se tiene un valor $x_{0}$ de inter\'es, entonces la estimaci\'on puntual de este nuevo valor
\begin{equation}
\hat{y}_{0}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}.
\end{equation}

Esta nueva observaci\'on es independiente de las utilizadas para obtener el modelo de regresi\'on, por tanto, el intervalo en torno a la recta de regresi\'on es inapropiado, puesto que se basa \'unicamente en los datos empleados para ajustar el modelo de regresi\'on. El intervalo de confianza en torno a la recta de regresi\'on se refiere a la respuesta media verdadera $x=x_{0}$, no a observaciones futuras. Sea $y_{0}$ la observaci\'on futura en $x=x_{0}$, y sea $\hat{y}_{0}$ dada en la ecuaci\'on anterior, el estimador de $y_{0}$. Si se define la variable aleatoria $$w=y_{0}-\hat{y}_{0},$$ esta se distribuye normalmente con media cero y varianza 

\begin{eqnarray}\label{Eq.Varianza.w}
V\left(w\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]\end{eqnarray}
dado que $y_{0}$ es independiente de $\hat{y}_{0}$, por lo tanto el intervalo de predicci\'on al nivel $\alpha$ para futuras observaciones $x_{0}$ es


\begin{eqnarray}\label{Intervalo.Confianza.w}
\hat{y}_{0}-t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}\leq y_{0}\leq \hat{y}_{0}+t_{\alpha/2,n-2}\sqrt{MC_{E}\left[1+\frac{1}{n}+\frac{\left(x-x_{0}\right)^2}{S_{xx}}\right]}.
\end{eqnarray}




%------------------------------------------------------------------
\subsection{Prueba de falta de ajuste}
%------------------------------------------------------------------

Es com\'un encontrar que el modelo ajustado no satisface totalmente el modelo necesario para los datos, en este caso es preciso saber qu\'e tan bueno es el modelo propuesto. Para esto se propone la siguiente prueba de hip\'otesis:
\begin{itemize}
\item[$H_{0}:$ ]El modelo propuesto se ajusta adecuademente a los datos.
\item[$H_{1}:$ ]El modelo NO se ajusta a los datos.
\end{itemize}
La prueba implica dividir la suma de cuadrados del eror o del residuo en las siguientes dos componentes:
\begin{eqnarray}\label{Suma.cuadrdos.errores}
SC_{E}=SC_{EP}+SC_{FDA}
\end{eqnarray}

donde $SC_{EP}$ es la suma de cuadrados atribuibles al error puro, y $SC_{FDA}$ es la suma de cuadrados atribuible a la falta de ajuste del modelo.

%------------------------------------------------------------------
\subsection{Coeficiente de Determinaci\'on}
%------------------------------------------------------------------


La cantidad
\begin{equation}\label{Coeficiente.Determinacion}
R^{2}=\frac{SC_{R}}{S_{yy}}=1-\frac{SC_{E}}{S_{yy}},
\end{equation}
se denomina coeficiente de determinaci\'on y se utiliza para saber si el modelo de regresi\'on es suficiente o no. Se puede demostrar que $0\leq R^{2}\leq1$, una manera de interpretar este valor es que si $R^{2}=k$, entonces el modelo de regresi\'on explica el $k*100\%$ de la variabilidad en los datos.
$R^{2}$ . Este coeficiente tiene las siguientes propiedades
\begin{itemize}
\item No mide la magnitud de la pendiente de la recta de regresi\'on.
\item Un valor grande de $R^{2}$ no implica una pendiente empinada.
\item No mide la suficiencia del modelo.
\item Valores grandes de $R^{2}$ no implican necesariamente que el modelo de regresi\'on proporcionar\'a predicciones precisas para futuras observaciones.
\end{itemize}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Regresi\'on Log\'istica}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->


%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Introducci\'on}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>


La regresi\'on log\'istica es una t\'ecnica de modelado estad\'istico ampliamente utilizada en an\'alisis de datos cuando el objetivo es predecir la probabilidad de un resultado binario, es decir, cuando la variable dependiente o respuesta tiene dos posibles categor\'ias, como "\'exito/fallo" o "s\'i/no". Esta t\'ecnica se emplea en una variedad de disciplinas, como la biomedicina, ciencias sociales, marketing y m\'as, para resolver problemas donde la variable respuesta es discreta o categ\'orica.\medskip

A diferencia de la regresi\'on lineal, que asume una relaci\'on lineal entre las variables independientes y la variable dependiente y que produce valores en un rango continuo, la regresi\'on log\'istica est\'a dise\~nada para manejar situaciones donde la respuesta es categ\'orica. En su forma m\'as com\'un, la regresi\'on log\'istica binaria, el modelo predice la probabilidad de que un evento ocurra en funci\'on de una o m\'as variables independientes. Este tipo de regresi\'on toma la forma de un modelo no lineal, debido a la naturaleza discreta de la variable dependiente.\medskip


La regresi\'on lineal busca modelar la relaci\'on entre una variable dependiente continua $Y$ y una o m\'as variables independientes $X_1, X_2, \ldots, X_n$ mediante una ecuaci\'on de la forma:
\begin{eqnarray}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon,
\end{eqnarray}
donde $\beta_0, \beta_1, \ldots, \beta_n$ son los coeficientes del modelo y $\epsilon$ es el t\'ermino de error. La regresi\'on log\'istica, en cambio, modela la probabilidad de que un evento ocurra (por ejemplo, \'exito vs. fracaso) utilizando la funci\'on log\'istica. La variable dependiente $Y$ es binaria, tomando valores de 0 o 1. La ecuaci\'on de la regresi\'on log\'istica es:
\begin{eqnarray}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n,
\end{eqnarray}
donde $p$ es la probabilidad de que $Y=1$. La funci\'on log\'istica es:
\begin{eqnarray}
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}.
\end{eqnarray}

La regresi\'on log\'istica se utiliza en una variedad de campos para problemas de clasificaci\'on binaria, tales como:
\begin{itemize}
    \item \textbf{Medicina}: Predicci\'on de la presencia o ausencia de una enfermedad.
    \item \textbf{Marketing}: Determinaci\'on de la probabilidad de que un cliente compre un producto.
    \item \textbf{Finanzas}: Evaluaci\'on del riesgo de cr\'edito, es decir, si un cliente va a incumplir o no con un pr\'estamo.
    \item \textbf{Seguridad}: Detecci\'on de fraudes o intrusiones.
\end{itemize}

%------------------------------------------------------
\subsection{Implementaci\'on B\'asica en R}
%------------------------------------------------------

Para implementar una regresi\'on log\'istica en R, primero es necesario instalar y cargar los paquetes necesarios. Aqu\'i se muestra un ejemplo b\'asico de implementaci\'on:

\begin{itemize}
    \item Descargue e instale R desde \texttt{https://cran.r-project.org/}.
    \item Descargue e instale RStudio desde \texttt{https://rstudio.com/products/rstudio/download/}.
\end{itemize}

\begin{verbatim}
# Instalaci\'on del paquete necesario
install.packages("stats")

# Carga del paquete
library(stats)

# Ejemplo de conjunto de datos
data <- data.frame(
  outcome = c(1, 0, 1, 0, 1, 1, 0, 1, 0, 0),
  predictor = c(2.3, 1.9, 3.1, 2.8, 3.6, 2.4, 2.1, 3.3, 2.2, 1.7)
)

# Ajuste del modelo de regresi\'on log\'istica
model <- glm(outcome ~ predictor, data = data, family = binomial)

# Resumen del modelo
summary(model)
\end{verbatim}

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Conceptos B\'asicos}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

La regresi\'on log\'istica es una t\'ecnica de modelado estad\'istico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en funci\'on de una o m\'as variables independientes.  Un modelo de regresi\'on log\'istica describe c\'omo una variable dependiente binaria $Y$ (que puede tomar los valores $0$ o $1$) est\'a relacionada con una o m\'as variables independientes $X_1, X_2, \ldots, X_n$. A diferencia de la regresi\'on lineal, que predice un valor continuo, la regresi\'on log\'istica predice una probabilidad que puede ser interpretada como la probabilidad de que $Y=1$ dado un conjunto de valores para $X_1, X_2, \ldots, X_n$.

%----------------------------------------------------------------
\subsection{Regresi\'on Lineal}
%----------------------------------------------------------------

La regresi\'on lineal es utilizada para predecir el valor de una variable dependiente continua en funci\'on de una o m\'as variables independientes. El modelo de regresi\'on lineal tiene la forma:
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\end{equation}
donde:
\begin{itemize}
    \item[a) ] $Y$ es la variable dependiente.
    \item[b) ] $\beta_0$ es la intersecci\'on con el eje $Y$ o t\'ermino constante.
    \item[c) ] $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes que representan la relaci\'on entre las variables independientes y la variable dependiente.
    \item[d) ] $X_1, X_2, \ldots, X_n$ son las variables independientes.
    \item[e) ] $\epsilon$ es el t\'ermino de error, que representa la desviaci\'on de los datos observados de los valores predichos por el modelo.
\end{itemize}

El objetivo de la regresi\'on lineal es encontrar los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos. Este m\'etodo se conoce como m\'inimos cuadrados ordinarios (OLS, por sus siglas en ingl\'es). La funci\'on de costo a minimizar es:
\begin{equation}
J\left(\beta_0, \beta_1, \ldots, \beta_n\right) = \sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2
\end{equation}
donde:
\begin{itemize}
    \item[a) ] $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on.
    \item[b) ] $\hat{y}_i$ es el valor predicho por el modelo para la $i$-\'esima observaci\'on, dado por:
    \begin{equation}
    \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in}
    \end{equation}
\end{itemize}

Para encontrar los valores \'optimos de los coeficientes, se toman las derivadas parciales de la funci\'on de costo con respecto a cada coeficiente y se igualan a cero:
\begin{equation}
\frac{\partial J}{\partial \beta_j} = 0 \quad \text{para } j = 0, 1, \ldots, n
\end{equation}

Resolviendo este sistema de ecuaciones, se obtienen los valores de los coeficientes que minimizan la funci\'on de costo.
%----------------------------------------------------------------
\subsection{Regresi\'on Log\'istica}
%----------------------------------------------------------------

La deducci\'on de la f\'ormula de la regresi\'on log\'istica comienza con la necesidad de modelar la probabilidad de un evento binario. Queremos encontrar una funci\'on que relacione las variables independientes con la probabilidad de que la variable dependiente tome el valor $1$. La probabilidad de que el evento ocurra, $P(Y=1)$, se denota como $p$. La probabilidad de que el evento no ocurra, $P(Y=0)$, es $1-p$. Los \textbf{odds} (chances) de que ocurra el evento se definen como:
\begin{equation}
\text{odds} = \frac{p}{1-p}
\end{equation}
Los odds indican cu\'antas veces m\'as probable es que ocurra el evento frente a que no ocurra. Para simplificar el modelado de los \textit{odds}, se aplica el logaritmo natural, obteniendo la funci\'on \textbf{logit}:
\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right)
\end{equation}
La transformaci\'on logit es \'util porque convierte el rango de la probabilidad (0, 1) al rango de n\'umeros reales $\left(-\infty, \infty\right)$. La idea clave de la regresi\'on log\'istica es modelar la transformaci\'on logit de la probabilidad como una combinaci\'on lineal de las variables independientes:
\begin{equation}\label{Ec.logit}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{equation}
Aqu\'i, $\beta_0$ es el t\'ermino constante y $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes asociados con las variables independientes $X_1, X_2, \ldots, X_n$. Para expresar $p$ en funci\'on de una combinaci\'on lineal de las variables independientes, invertimos la transformaci\'on logit. Partimos de la ecuaci\'on \ref{Ec.logit}, aplicando la funci\'on exponencial en ambos lados:
\begin{eqnarray}
\frac{p}{1-p} = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}
\end{eqnarray}
Despejando $p$:
\begin{eqnarray}
p = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}
\end{eqnarray}

La expresi\'on final que obtenemos es conocida como la \textbf{funci\'on log\'istica}:
\begin{equation}\label{Eq.Logit1}
p = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n\right)}}
\end{equation}
Esta funci\'on describe c\'omo las variables independientes se relacionan con la probabilidad de que el evento de inter\'es ocurra. Los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ se estiman a partir de los datos utilizando el m\'etodo de m\'axima verosimilitud.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{M\'etodo de M\'axima Verosimilitud}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

Para estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ en la regresi\'on log\'istica, utilizamos el m\'etodo de m\'axima verosimilitud. La idea es encontrar los valores de los coeficientes que maximicen la probabilidad de observar los datos dados. Esta probabilidad se expresa mediante la funci\'on de verosimilitud $L$. La funci\'on de verosimilitud $L(\beta_0, \beta_1, \ldots, \beta_n)$ para un conjunto de $n$ observaciones se define como el producto de las probabilidades de las observaciones dadas las variables independientes:

\begin{equation}\label{Eq.Verosimilitud}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{equation}

donde:
\begin{itemize}
    \item[a) ] $p_i$ es la probabilidad predicha de que $Y_i = 1$,
    \item[b) ] $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on.
\end{itemize}

Trabajar directamente con esta funci\'on de verosimilitud puede ser complicado debido al producto de muchas probabilidades, especialmente si $n$ es grande. Para simplificar los c\'alculos, se utiliza el logaritmo de la funci\'on de verosimilitud, conocido como la funci\'on de \textit{log-verosimilitud}. El uso del logaritmo simplifica significativamente la diferenciaci\'on y maximizaci\'on de la funci\'on. La funci\'on de log-verosimilitud se define como:

\begin{equation}\label{Funcion.LogVerosimilitud}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right].
\end{equation}

Aqu\'i, $\log$ representa el logaritmo natural. Esta transformaci\'on es v\'alida porque el logaritmo es una funci\'on mon\'otona creciente, lo que significa que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud original. En la regresi\'on log\'istica, de acuerdo a la ecuaci\'on \ref{Eq.Logit1} la probabilidad $p_i$ est\'a dada por la funci\'on log\'istica:

\begin{equation}\label{Eq.Logit1.Multi}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{equation}

Sustituyendo esta expresi\'on en la funci\'on de log-verosimilitud (\ref{Funcion.LogVerosimilitud}), obtenemos:

\begin{eqnarray}\label{Eq.LogVerosimilitud.VV}
\begin{array}{l}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log \left( \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) + \right. \\
 \left. (1 - y_i) \log \left( 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}} \right) \right]
\end{array}
\end{eqnarray}

Simplificando esta expresi\'on,

\begin{eqnarray}
\log \left( \frac{1}{1 + e^{-z}} \right) &=& -\log(1 + e^{-z})\textrm{, por tanto}\\
\log \left( 1 - \frac{1}{1 + e^{-z}} \right) &=& \log \left( \frac{e^{-z}}{1 + e^{-z}} \right) = -z - \log(1 + e^{-z})
\end{eqnarray}

Aplicando las ecuaciones anteriores, la ecuaci\'on \ref{Eq.LogVerosimilitud.VV} se convierte en:

\begin{eqnarray*}
\begin{array}{l}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (-\log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})})) + \right. \nonumber \\
 \quad \left. (1 - y_i) \left( -(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}) \right) \right]
\end{array}
\end{eqnarray*}

Simplificando a\'un m\'as, obtenemos:

\begin{eqnarray}\label{Eq.LogVerosimilitud.Final}
\begin{array}{r}
\log L(\beta_0, \beta_1, \ldots, \beta_n) =\sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) -\log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{array}
\end{eqnarray}

%-------------------------------------------------
\subsection{Notaci\'on Matricial}
%-------------------------------------------------

Para simplificar a\'un m\'as la notaci\'on, podemos utilizar notaci\'on matricial. Definimos la matriz $\mathbf{X}$ de tama\~no $n \times (k+1)$ y el vector de coeficientes $\boldsymbol{\beta}$ de tama\~no $(k+1) \times 1$ como sigue:

\begin{equation}\label{Eq.Matricial1}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

Entonces, la expresi\'on para la funci\'on de log-verosimilitud, ecuaci\'on \ref{Eq.LogVerosimilitud.Final}, es:

\begin{equation}\label{Eq.LogLikelihood1.Matricial}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

donde $\mathbf{X}_i$ es la $i$-\'esima fila de la matriz $\mathbf{X}$.  Esta notaci\'on matricial simplifica la implementaci\'on y la derivaci\'on de los estimadores de los coeficientes en la regresi\'on log\'istica. Utilizando el  m\'etodo de Newton-Raphson, es posible encontrar los coeficientes $\beta_j$ que maximizan la funci\'on de log-verosimilitud. Para maximizar la funci\'on de log-verosimilitud, derivamos esta funci\'on con respecto a cada uno de los coeficientes $\beta_j$ y encontramos los puntos cr\'iticos. La derivada parcial de la funci\'on de log-verosimilitud con respecto a $\beta_j$ es:

\begin{eqnarray}\label{Primer.Derivada.LV}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\mathbf{X}_i \boldsymbol{\beta}}}{1 + e^{\mathbf{X}_i \boldsymbol{\beta}}} \right]
\end{eqnarray}

Simplificando, esta derivada se puede expresar como:

\begin{eqnarray}\label{Eq.PrimeraDerivada}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i),\textrm{ donde }p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}
\end{eqnarray}

Para encontrar los coeficientes que maximizan la log-verosimilitud, se requiere resolver el sistema de ecuaciones 
\begin{eqnarray}\label{Eq.Sol.ML}
\frac{\partial \log L(\boldsymbol{\beta})}{\partial \beta_j} = 0 \textrm{ para todos los }j = 0, 1, \ldots, k. 
\end{eqnarray}
Este sistema de ecuaciones no tiene una soluci\'on anal\'itica cerrada, por lo que se propone resolver utilizando el m\'etodo  de Newton-Raphson.

%---------------------------------------------------------------
\subsection{M\'etodo de Newton-Raphson}
%---------------------------------------------------------------

El m\'etodo de Newton-Raphson es un algoritmo iterativo que se utiliza para encontrar las ra\'ices de una funci\'on. En el contexto de la regresi\'on log\'istica, se utiliza para maximizar la funci\'on de log-verosimilitud encontrando los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$. Este m\'etodo se basa en una aproximaci\'on de segundo orden de la funci\'on objetivo. Dado un valor inicial de los coeficientes $\boldsymbol{\beta}^{(0)}$, se actualiza iterativamente el valor de los coeficientes utilizando la f\'ormula:

\begin{equation}\label{Eq.Criterio0}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

donde:
\begin{itemize}
    \item[a) ] $\boldsymbol{\beta}^{(t)}$ es el vector de coeficientes en la $t$-\'esima iteraci\'on.
    \item[b) ] $\nabla \log L(\boldsymbol{\beta}^{(t)})$ es el gradiente de la funci\'on de log-verosimilitud con respecto a los coeficientes $\boldsymbol{\beta}$:

\begin{equation}\label{Eq.Gradiente1}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades, recordar la ecuaci\'on (\ref{Eq.PrimeraDerivada}).

    \item $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\boldsymbol{\beta}^{(t)}$:
\begin{equation}\label{Eq.Hessiana1}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

\end{itemize}

En resumen:

\begin{Algthm}\label{Algoritmo1}
El algoritmo Newton-Raphson para la regresi\'on log\'istica se puede resumir en los siguientes pasos:
\begin{itemize}
\item[i) ] Inicializar el vector de coeficientes $\boldsymbol{\beta}^{(0)}$ (por ejemplo, con ceros o valores peque\~nos aleatorios).
\item[ii) ] Calcular el gradiente $\nabla \log L(\boldsymbol{\beta}^{(t)})$ y la matriz Hessiana $\mathbf{H}(\boldsymbol{\beta}^{(t)})$ en la iteraci\'on $t$.
\item[iii) ] Actualizar los coeficientes utilizando la f\'ormula:
\begin{equation}\label{Eq.Criterio1}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \mathbf{H}(\boldsymbol{\beta}^{(t)}) \right]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}
\item[iv) ] Repetir los pasos ii) y iii) hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{itemize}
\end{Algthm}

En resumen, el m\'etodo de Newton-Raphson permite encontrar los coeficientes que maximizan la funci\'on de log-verosimilitud de manera eficiente. 

\subsection{Notas finales}

En el contexto de la regresi\'on log\'istica, los vectores $X_1, X_2, \ldots, X_n$ representan las variables independientes. Cada $X_j$ es un vector columna que contiene los valores de la variable independiente $j$ para cada una de las $n$ observaciones. Es decir,

\begin{equation}
X_j = \begin{bmatrix}
x_{1j} \\
x_{2j} \\
\vdots \\
x_{nj}
\end{bmatrix}
\end{equation}

Para simplificar la notaci\'on y los c\'alculos, a menudo combinamos todos los vectores de variables independientes en una \'unica matriz de dise\~no $\mathbf{X}$ de tama\~no $n \times (k+1)$, donde $n$ es el n\'umero de observaciones y $k+1$ es el n\'umero de variables independientes m\'as el t\'ermino de intercepto. La primera columna de $\mathbf{X}$ corresponde a un vector de unos para el t\'ermino de intercepto, y las dem\'as columnas corresponden a los valores de las variables independientes:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}
\end{equation}
revisar la ecuaci\'on \ref{Eq.Matricial1}. De esta forma, el modelo logit puede ser escrito de manera compacta utilizando la notaci\'on matricial:

\begin{equation}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \mathbf{X} \boldsymbol{\beta}
\end{equation}

donde $\boldsymbol{\beta}$ es el vector de coeficientes:

\begin{equation}
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
\end{equation}

As\'i, la probabilidad $p$ se puede expresar como:

\begin{equation}\label{Eq.Logit2}
p = \frac{1}{1 + e^{-\mathbf{X} \boldsymbol{\beta}}}
\end{equation}

Comparar la ecuaci\'on anterior con la ecuaci\'on \ref{Eq.Logit1}. Esta notaci\'on matricial simplifica la implementaci\'on y la derivaci\'on de los estimadores de los coeficientes en la regresi\'on log\'istica. Para estimar los coeficientes $\boldsymbol{\beta}$ en la regresi\'on log\'istica, se utiliza el m\'etodo de m\'axima verosimilitud. La funci\'on de verosimilitud $L(\boldsymbol{\beta})$ se define como el producto de las probabilidades de las observaciones dadas las variables independientes, recordemos la ecuaci\'on \ref{Eq.Verosimilitud}:

\begin{eqnarray}
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray}


donde $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on, y $p_i$ es la probabilidad predicha de que $Y_i = 1$.  La funci\'on de log-verosimilitud, que es m\'as f\'acil de maximizar, se obtiene tomando el logaritmo natural de la funci\'on de verosimilitud (\ref{Eq.LogLikelihood1}):

\begin{equation}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}

Sustituyendo $p_i = \frac{1}{1 + e^{-\mathbf{X}_i \boldsymbol{\beta}}}$, donde $\mathbf{X}_i$ es la $i$-\'esima fila de la matriz de dise\~no $\mathbf{X}$, obtenemos:

\begin{equation}\label{Eq.LogLikelihood2}
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i (\mathbf{X}_i \boldsymbol{\beta}) - \log(1 + e^{\mathbf{X}_i \boldsymbol{\beta}}) \right]
\end{equation}

Para encontrar los valores de $\boldsymbol{\beta}$ que maximizan la funci\'on de log-verosimilitud, se utiliza un algoritmo iterativo como el m\'etodo de Newton-Raphson. Este m\'etodo requiere calcular el gradiente y la matriz Hessiana de la funci\'on de log-verosimilitud.


El gradiente de la funci\'on de log-verosimilitud con respecto a $\boldsymbol{\beta}$ es (\ref{Eq.Gradiente1} y \ref{Eq.Gradiente2}):

\begin{equation}
\nabla \log L(\boldsymbol{\beta}) = \mathbf{X}^T (\mathbf{y} - \mathbf{p})
\end{equation}

donde $\mathbf{y}$ es el vector de valores observados y $\mathbf{p}$ es el vector de probabilidades predichas.

La matriz Hessiana de la funci\'on de log-verosimilitud es (\ref{Eq.Hessiana1} y \ref{Eq.Hessiana2}):

\begin{equation}
\mathbf{H}(\boldsymbol{\beta}) = -\mathbf{X}^T \mathbf{W} \mathbf{X}
\end{equation}

donde $\mathbf{W}$ es una matriz diagonal de pesos con elementos $w_i = p_i (1 - p_i)$.

El m\'etodo de Newton-Raphson actualiza los coeficientes $\boldsymbol{\beta}$ de la siguiente manera:

\begin{equation}\label{Eq.Criterio3}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - [\mathbf{H}(\boldsymbol{\beta}^{(t)})]^{-1} \nabla \log L(\boldsymbol{\beta}^{(t)})
\end{equation}

Iterando este proceso hasta que la diferencia entre $\boldsymbol{\beta}^{(t+1)}$ y $\boldsymbol{\beta}^{(t)}$ sea menor que un umbral predefinido (\ref{Eq.Criterio0} y  \ref{Eq.Criterio1}), se obtienen los estimadores de m\'axima verosimilitud para los coeficientes de la regresi\'on log\'istica.

%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>
\section{Validaci\'on del Modelo}
%<>==<><>==<><>==<><>==<><>==<><>==<><>==<><>==<>

Una vez que se han estimado los coeficientes del modelo de regresi\'on log\'istica, es importante validar el modelo para asegurarse de que proporciona predicciones precisas.
%---------------------------------------------------------
\subsection{Curva ROC y AUC}
%---------------------------------------------------------
La curva ROC (Receiver Operating Characteristic) es una herramienta gr\'afica utilizada para evaluar el rendimiento de un modelo de clasificaci\'on binaria. El \'area bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases. 
La curva ROC (Receiver Operating Characteristic) es una representaci\'on gr\'afica de la sensibilidad (verdaderos positivos) frente a 1 - especificidad (falsos positivos). El \'area bajo la curva (AUC) mide la capacidad del modelo para distinguir entre las clases.

\begin{eqnarray*}
\text{Sensibilidad} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{Especificidad} &=& \frac{\text{TN}}{\text{TN} + \text{FP}}
\end{eqnarray*}



%---------------------------------------------------------
\subsection{Matriz de Confusi\'on}
%---------------------------------------------------------
La matriz de confusi\'on es una tabla que resume el rendimiento de un modelo de clasificaci\'on al comparar las predicciones del modelo con los valores reales. Los t\'erminos en la matriz de confusi\'on incluyen verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos. 
La matriz de confusi\'on es una tabla que muestra el rendimiento del modelo comparando las predicciones con los valores reales. Los t\'erminos incluyen:
\begin{itemize}
    \item \textbf{Verdaderos Positivos (TP)}: Predicciones correctas de la clase positiva.
    \item \textbf{Falsos Positivos (FP)}: Predicciones incorrectas de la clase positiva.
    \item \textbf{Verdaderos Negativos (TN)}: Predicciones correctas de la clase negativa.
    \item \textbf{Falsos Negativos (FN)}: Predicciones incorrectas de la clase negativa.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicci\'on Positiva} & \textbf{Predicci\'on Negativa} \\
\hline
\textbf{Real Positiva} & TP & FN \\
\hline
\textbf{Real Negativa} & FP & TN \\
\hline
\end{tabular}
\caption{Matriz de Confusi\'on}
\label{tab:confusion_matrix}
\end{table}
%---------------------------------------------------------
\subsection{Precisi\'on, Recall y F1-Score}
%---------------------------------------------------------
\begin{eqnarray*}
\text{Precisi\'on} &=& \frac{\text{TP}}{\text{TP} + \text{FP}} \\
\text{Recall} &=& \frac{\text{TP}}{\text{TP} + \text{FN}} \\
\text{F1-Score} &=& 2 \cdot \frac{\text{Precisi\'on} \cdot \text{Recall}}{\text{Precisi\'on} + \text{Recall}}
\end{eqnarray*}
%---------------------------------------------------------
\subsection{Log-Loss}
%---------------------------------------------------------
La p\'erdida logar\'itmica (Log-Loss) mide la precisi\'on de las probabilidades predichas. La f\'ormula es:
\begin{eqnarray*}
\text{Log-Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}
donde $y_i$ son los valores reales y $p_i$ son las probabilidades predichas.
%---------------------------------------------------------
\subsection{K-Fold Cross-Validation}
%---------------------------------------------------------
La validaci\'on cruzada es una t\'ecnica para evaluar la capacidad de generalizaci\'on de un modelo. Existen varios tipos de validaci\'on cruzada:


En K-Fold Cross-Validation, los datos se dividen en K subconjuntos. El modelo se entrena K veces, cada vez utilizando K-1 subconjuntos para el entrenamiento y el subconjunto restante para la validaci\'on.

\begin{eqnarray*}
\text{Error Medio} = \frac{1}{K} \sum_{k=1}^{K} \text{Error}_k
\end{eqnarray*}
%---------------------------------------------------------
\subsection{Leave-One-Out Cross-Validation (LOOCV)}
%---------------------------------------------------------
En LOOCV, cada observaci\'on se usa una vez como conjunto de validaci\'on y las restantes como conjunto de entrenamiento. Este m\'etodo es computacionalmente costoso pero \'util para conjuntos de datos peque\~nos.

%<>==><==<>==><==<>==><==<>==><==<>==><==<>==><==
\section{Ajuste y Sobreajuste del Modelo}
%<>==><==<>==><==<>==><==<>==><==<>==><==<>==><==

El ajuste adecuado del modelo es crucial para evitar el sobreajuste (\textit{overfitting}) y el subajuste (\textit{underfitting)}. Ambos problemas impactan negativamente en la capacidad del modelo para generalizar a datos nuevos.

\begin{itemize}

\item Sobreajuste: El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando no solo los patrones reales, sino también el ruido y las peculiaridades del conjunto de datos. Esto resulta en un modelo que funciona extremadamente bien en los datos de entrenamiento, pero que falla al ser evaluado con datos nuevos. Los síntomas típicos del sobreajuste incluyen una \textbf{alta precisión en el conjunto de entrenamiento} y \textbf{una baja precisión en el conjunto de validación o prueba}.

Algunos factores que pueden llevar al sobreajuste incluyen:
\begin{itemize} 
\item[a) ] \textbf{Modelos excesivamente complejos}: Un modelo con demasiados parámetros o características puede adaptarse demasiado a los datos. 
\item[b) ] \textbf{Conjuntos de datos pequeños}: Cuando los datos de entrenamiento no son suficientes para capturar la variedad de situaciones posibles.
 \item[c) ] \textbf{Falta de regularización}: Si no se utiliza ninguna técnica de regularización, el modelo puede ajustarse de manera demasiado precisa a los datos. 
 \end{itemize}

\item Subajuste:  El subajuste ocurre cuando el modelo no es capaz de capturar los patrones subyacentes de los datos, generalmente porque es demasiado simple o porque el entrenamiento no ha sido suficiente. En este caso, tanto en el conjunto de entrenamiento como en el de validación se observan errores elevados, lo que indica que el modelo no está aprendiendo correctamente.

Las causas comunes del subajuste incluyen: 
\begin{itemize}
\item[a) ] \textbf{Modelos demasiado simples}: Modelos con pocos parámetros o de baja complejidad, como la regresión lineal para problemas no lineales. 
\item[b) ] \textbf{Insuficiente entrenamiento}: El modelo no ha sido entrenado adecuadamente, lo que puede requerir más iteraciones de entrenamiento o ajustes en los hiperparámetros. 
\item[c) ] \textbf{Datos insuficientemente procesados}: El preprocesamiento incorrecto o insuficiente de los datos puede impedir que el modelo identifique patrones importantes. \end{itemize}
\end{itemize}

Existen varias estrategias para prevenir el sobreajuste, incluyendo la selección de un modelo adecuado, el uso de más datos y la aplicación de técnicas de regularización. A continuación, se describen algunas de las técnicas más comunes:

\begin{itemize}

\item Regularizaci\'on) La regularización es una técnica que añade un término de penalización a la función de costo, con el fin de reducir la complejidad del modelo. Las dos formas más comunes de regularización son:

\begin{itemize} 
\item[a) ] \textbf{Regresión Lasso (L1)}: Esta técnica añade una penalización proporcional al valor absoluto de los coeficientes del modelo. La regularización Lasso tiende a reducir algunos coeficientes a cero, lo que lleva a la selección automática de características. \begin{eqnarray*} 
\text{Función de Costo L1} = \sum_{i=1}^{n} (y_i - \hat{y}i)^2 + \lambda \sum{j=1}^{p} |\beta_j| 
\end{eqnarray*} 

\item[b) ] \textbf{Regresión Ridge (L2)}: Esta técnica penaliza el cuadrado de los coeficientes del modelo. A diferencia de Lasso, Ridge no fuerza los coeficientes a ser exactamente cero, sino que los reduce. 
\begin{eqnarray*} 
\text{Función de Costo L2} = \sum_{i=1}^{n} (y_i - \hat{y}i)^2 + \lambda \sum{j=1}^{p} \beta_j^2 
\end{eqnarray*} 
\end{itemize}

\item Elastic Net: Combina las penalizaciones L1 y L2, con un término de penalización ajustable para controlar la importancia de cada uno. 

\item Validación Cruzada: Otra técnica importante para prevenir el sobreajuste es la validación cruzada, que ayuda a estimar el rendimiento del modelo en datos no observados. El método más común es la validación cruzada $K-fold$, donde los datos se dividen en $K$ subconjuntos, y el modelo se entrena $K$ veces, utilizando $K-1$ subconjuntos para el entrenamiento y el restante para la validación.

\item Aumentar el Tamaño del Conjunto de Datos: Una estrategia efectiva, aunque a menudo difícil de implementar, es aumentar el tamaño del conjunto de datos. Más datos permiten al modelo capturar mejor los patrones subyacentes y generalizar de manera más efectiva, disminuyendo el riesgo de sobreajuste.

\item Reducción de Características (Feature Selection): Reducir el número de características (variables) irrelevantes o redundantes puede simplificar el modelo y mejorar su capacidad de generalización. Técnicas como la selección de características basada en su importancia o la eliminación de características altamente correlacionadas pueden ayudar a evitar el sobreajuste.
\end{itemize}




%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Preparaci\'on de Datos y Selecci\'on de Variables}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->


\section{Introducci\'on}

La preparaci\'on de datos y la selecci\'on de variables son pasos cruciales en el proceso de modelado estad\'istico. Un modelo bien preparado y con las variables adecuadas puede mejorar significativamente la precisi\'on y la interpretabilidad del modelo. Este cap\'itulo proporciona una revisi\'on detallada de las t\'ecnicas de limpieza de datos, tratamiento de datos faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.

\section{Importancia de la Preparaci\'on de Datos}

La calidad de los datos es fundamental para el \'exito de cualquier an\'alisis estad\'istico. Los datos sin limpiar pueden llevar a modelos inexactos y conclusiones err\'oneas. La preparaci\'on de datos incluye varias etapas:
\begin{itemize}
    \item Limpieza de datos
    \item Tratamiento de datos faltantes
    \item Codificaci\'on de variables categ\'oricas
    \item Selecci\'on y transformaci\'on de variables
\end{itemize}

\section{Limpieza de Datos}

La limpieza de datos es el proceso de detectar y corregir (o eliminar) los datos incorrectos, incompletos o irrelevantes. Este proceso incluye:
\begin{itemize}
    \item Eliminaci\'on de duplicados
    \item Correcci\'on de errores tipogr\'aficos
    \item Consistencia de formato
    \item Tratamiento de valores extremos (outliers)
\end{itemize}

\section{Tratamiento de Datos Faltantes}

Los datos faltantes son un problema com\'un en los conjuntos de datos y pueden afectar la calidad de los modelos. Hay varias estrategias para manejar los datos faltantes:
\begin{itemize}
    \item \textbf{Eliminaci\'on de Datos Faltantes}: Se eliminan las filas o columnas con datos faltantes.
    \item \textbf{Imputaci\'on}: Se reemplazan los valores faltantes con estimaciones, como la media, la mediana o la moda.
    \item \textbf{Modelos Predictivos}: Se utilizan modelos predictivos para estimar los valores faltantes.
\end{itemize}

\subsection{Imputaci\'on de la Media}

Una t\'ecnica com\'un es reemplazar los valores faltantes con la media de la variable. Esto se puede hacer de la siguiente manera:
\begin{eqnarray*}
x_i = \begin{cases} 
      x_i & \text{si } x_i \text{ no es faltante} \\
      \bar{x} & \text{si } x_i \text{ es faltante}
   \end{cases}
\end{eqnarray*}
donde $\bar{x}$ es la media de la variable.

\section{Codificaci\'on de Variables Categ\'oricas}

Las variables categ\'oricas deben ser convertidas a un formato num\'erico antes de ser usadas en un modelo de regresi\'on log\'istica. Hay varias t\'ecnicas para codificar variables categ\'oricas:

\subsection{Codificaci\'on One-Hot}

La codificaci\'on one-hot crea una columna binaria para cada categor\'ia. Por ejemplo, si tenemos una variable categ\'orica con tres categor\'ias (A, B, C), se crean tres columnas:
\begin{eqnarray*}
\text{A} &=& [1, 0, 0] \\
\text{B} &=& [0, 1, 0] \\
\text{C} &=& [0, 0, 1]
\end{eqnarray*}

\subsection{Codificaci\'on Ordinal}

La codificaci\'on ordinal asigna un valor entero \'unico a cada categor\'ia, preservando el orden natural de las categor\'ias. Por ejemplo:
\begin{eqnarray*}
\text{Bajo} &=& 1 \\
\text{Medio} &=& 2 \\
\text{Alto} &=& 3
\end{eqnarray*}

\section{Selecci\'on de Variables}

La selecci\'on de variables es el proceso de elegir las variables m\'as relevantes para el modelo. Existen varias t\'ecnicas para la selecci\'on de variables:

\subsection{M\'etodos de Filtrado}

Los m\'etodos de filtrado seleccionan variables basadas en criterios estad\'isticos, como la correlaci\'on o la chi-cuadrado. Algunas t\'ecnicas comunes incluyen:
\begin{itemize}
    \item \textbf{An\'alisis de Correlaci\'on}: Se seleccionan variables con alta correlaci\'on con la variable dependiente y baja correlaci\'on entre ellas.
    \item \textbf{Pruebas de Chi-cuadrado}: Se utilizan para variables categ\'oricas para determinar la asociaci\'on entre la variable independiente y la variable dependiente.
\end{itemize}

\subsection{M\'etodos de Wrapper}

Los m\'etodos de wrapper eval\'uan m\'ultiples combinaciones de variables y seleccionan la combinaci\'on que optimiza el rendimiento del modelo. Ejemplos incluyen:
\begin{itemize}
    \item \textbf{Selecci\'on hacia Adelante}: Comienza con un modelo vac\'io y agrega variables una por una, seleccionando la variable que mejora m\'as el modelo en cada paso.
    \item \textbf{Selecci\'on hacia Atr\'as}: Comienza con todas las variables y elimina una por una, removiendo la variable que tiene el menor impacto en el modelo en cada paso.
    \item \textbf{Selecci\'on Paso a Paso}: Combina la selecci\'on hacia adelante y hacia atr\'as, agregando y eliminando variables seg\'un sea necesario.
\end{itemize}

\subsection{M\'etodos Basados en Modelos}

Los m\'etodos basados en modelos utilizan t\'ecnicas de regularizaci\'on como Lasso y Ridge para seleccionar variables. Estas t\'ecnicas a\~naden un t\'ermino de penalizaci\'on a la funci\'on de costo para evitar el sobreajuste.

\subsubsection{Regresi\'on Lasso}

La regresi\'on Lasso (Least Absolute Shrinkage and Selection Operator) a\~nade una penalizaci\'on $L_1$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on que controla la cantidad de penalizaci\'on.

\subsubsection{Regresi\'on Ridge}

La regresi\'on Ridge a\~nade una penalizaci\'on $L_2$ a la funci\'on de costo:
\begin{eqnarray*}
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\end{eqnarray*}
donde $\lambda$ es el par\'ametro de regularizaci\'on.

\section{Implementaci\'on en R}

\subsection{Limpieza de Datos}

Para ilustrar la limpieza de datos en R, considere el siguiente conjunto de datos:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, NA, 5),
  var2 = c("A", "B", "A", "B", "A"),
  var3 = c(10, 15, 10, 20, 25)
)

# Eliminaci\'on de filas con datos faltantes
data_clean <- na.omit(data)

# Imputaci\'on de la media
data$var1[is.na(data$var1)] <- mean(data$var1, na.rm = TRUE)
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Para codificar variables categ\'oricas, utilice la funci\'on `model.matrix`:
\begin{verbatim}
data <- data.frame(
  var1 = c(1, 2, 3, 4, 5),
  var2 = c("A", "B", "A", "B", "A")
)

# Codificaci\'on one-hot
data_onehot <- model.matrix(~ var2 - 1, data = data)
\end{verbatim}

\subsection{Selecci\'on de Variables}

Para la selecci\'on de variables, utilice el paquete `caret`:
\begin{verbatim}
library(caret)

# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$var1, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]

# Modelo de regresi\'on log\'istica
model <- train(var1 ~ ., data = dataTrain, method = "glm", family = "binomial")

# Selecci\'on de variables
model <- step(model, direction = "both")
summary(model)
\end{verbatim}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Diagn\'ostico del Modelo y Ajuste de Par\'ametros}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

\section{Introducci\'on}

El diagn\'ostico del modelo y el ajuste de par\'ametros son pasos esenciales para mejorar la precisi\'on y la robustez de los modelos de regresi\'on log\'istica. Este cap\'itulo se enfoca en las t\'ecnicas para diagnosticar problemas en los modelos y en m\'etodos para ajustar los par\'ametros de manera \'optima.

\section{Diagn\'ostico del Modelo}

El diagn\'ostico del modelo implica evaluar el rendimiento del modelo y detectar posibles problemas, como el sobreajuste, la multicolinealidad y la influencia de puntos de datos individuales.

\subsection{Residuos}

Los residuos son las diferencias entre los valores observados y los valores predichos por el modelo. El an\'alisis de residuos puede revelar patrones que indican problemas con el modelo.

\begin{eqnarray*}
\text{Residuo}_i = y_i - \hat{y}_i
\end{eqnarray*}

\subsubsection{Residuos Estudiantizados}

Los residuos estudiantizados se ajustan por la variabilidad del residuo y se utilizan para detectar outliers.

\begin{eqnarray*}
r_i = \frac{\text{Residuo}_i}{\hat{\sigma} \sqrt{1 - h_i}}
\end{eqnarray*}
donde $h_i$ es el leverage del punto de datos.

\subsection{Influencia}

La influencia mide el impacto de un punto de datos en los coeficientes del modelo. Los puntos con alta influencia pueden distorsionar el modelo.

\subsubsection{Distancia de Cook}

La distancia de Cook es una medida de la influencia de un punto de datos en los coeficientes del modelo.

\begin{eqnarray*}
D_i = \frac{r_i^2}{p} \cdot \frac{h_i}{1 - h_i}
\end{eqnarray*}
donde $p$ es el n\'umero de par\'ametros en el modelo.

\subsection{Multicolinealidad}

La multicolinealidad ocurre cuando dos o m\'as variables independientes est\'an altamente correlacionadas. Esto puede inflar las varianzas de los coeficientes y hacer que el modelo sea inestable.

\subsubsection{Factor de Inflaci\'on de la Varianza (VIF)}

El VIF mide cu\'anto se inflan las varianzas de los coeficientes debido a la multicolinealidad.

\begin{eqnarray*}
\text{VIF}_j = \frac{1}{1 - R_j^2}
\end{eqnarray*}
donde $R_j^2$ es el coeficiente de determinaci\'on de la regresi\'on de la variable $j$ contra todas las dem\'as variables.

\section{Ajuste de Par\'ametros}

El ajuste de par\'ametros implica seleccionar los valores \'optimos para los hiperpar\'ametros del modelo. Esto puede mejorar el rendimiento y prevenir el sobreajuste.

\subsection{Grid Search}

El grid search es un m\'etodo exhaustivo para ajustar los par\'ametros. Se define una rejilla de posibles valores de par\'ametros y se eval\'ua el rendimiento del modelo para cada combinaci\'on.

\subsection{Random Search}

El random search selecciona aleatoriamente combinaciones de valores de par\'ametros dentro de un rango especificado. Es menos exhaustivo que el grid search, pero puede ser m\'as eficiente.

\subsection{Bayesian Optimization}

La optimizaci\'on bayesiana utiliza modelos probabil\'isticos para seleccionar iterativamente los valores de par\'ametros m\'as prometedores.

\section{Implementaci\'on en R}

\subsection{Diagn\'ostico del Modelo}

\begin{verbatim}
# Cargar el paquete necesario
library(car)

# Residuos estudentizados
dataTrain$resid <- rstudent(model)
hist(dataTrain$resid, breaks = 20, main = "Residuos Estudentizados")

# Distancia de Cook
dataTrain$cook <- cooks.distance(model)
plot(dataTrain$cook, type = "h", main = "Distancia de Cook")

# Factor de Inflaci\'on de la Varianza
vif_values <- vif(model)
print(vif_values)
\end{verbatim}

\subsection{Ajuste de Par\'ametros}

\begin{verbatim}
# Grid Search con caret
control <- trainControl(method = "cv", number = 10)
tune_grid <- expand.grid(.alpha = c(0, 0.5, 1), .lambda = seq(0.01, 0.1, by = 0.01))

model_tune <- train(var1 ~ ., data = dataTrain, method = "glmnet", 
                    trControl = control, tuneGrid = tune_grid)

print(model_tune)
\end{verbatim}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Interpretaci\'on de los Resultados}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}

Interpretar correctamente los resultados de un modelo de regresi\'on log\'istica es esencial para tomar decisiones informadas. Este cap\'itulo se centra en la interpretaci\'on de los coeficientes del modelo, las odds ratios, los intervalos de confianza y la significancia estad\'istica.

\section{Coeficientes de Regresi\'on Log\'istica}

Los coeficientes de regresi\'on log\'istica representan la relaci\'on entre las variables independientes y la variable dependiente en t\'erminos de log-odds. 

\subsection{Interpretaci\'on de los Coeficientes}

Cada coeficiente $\beta_j$ en el modelo de regresi\'on log\'istica se interpreta como el cambio en el log-odds de la variable dependiente por unidad de cambio en la variable independiente $X_j$.

\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}

\subsection{Signo de los Coeficientes}

\begin{itemize}
    \item \textbf{Coeficiente Positivo}: Un coeficiente positivo indica que un aumento en la variable independiente est\'a asociado con un aumento en el log-odds de la variable dependiente.
    \item \textbf{Coeficiente Negativo}: Un coeficiente negativo indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en el log-odds de la variable dependiente.
\end{itemize}

\section{Odds Ratios}

Las odds ratios proporcionan una interpretaci\'on m\'as intuitiva de los coeficientes de regresi\'on log\'istica. La odds ratio para una variable independiente $X_j$ se calcula como $e^{\beta_j}$.

\subsection{C\'alculo de las Odds Ratios}

\begin{eqnarray*}
\text{OR}_j = e^{\beta_j}
\end{eqnarray*}

\subsection{Interpretaci\'on de las Odds Ratios}

\begin{itemize}
    \item \textbf{OR > 1}: Un OR mayor que 1 indica que un aumento en la variable independiente est\'a asociado con un aumento en las odds de la variable dependiente.
    \item \textbf{OR < 1}: Un OR menor que 1 indica que un aumento en la variable independiente est\'a asociado con una disminuci\'on en las odds de la variable dependiente.
    \item \textbf{OR = 1}: Un OR igual a 1 indica que la variable independiente no tiene efecto sobre las odds de la variable dependiente.
\end{itemize}

\section{Intervalos de Confianza}

Los intervalos de confianza proporcionan una medida de la incertidumbre asociada con los estimadores de los coeficientes. Un intervalo de confianza del 95\% para un coeficiente $\beta_j$ indica que, en el 95\% de las muestras, el intervalo contendr\'a el valor verdadero de $\beta_j$.

\subsection{C\'alculo de los Intervalos de Confianza}

Para calcular un intervalo de confianza del 95\% para un coeficiente $\beta_j$, utilizamos la f\'ormula:
\begin{eqnarray*}
\beta_j \pm 1.96 \cdot \text{SE}(\beta_j)
\end{eqnarray*}
donde $\text{SE}(\beta_j)$ es el error est\'andar de $\beta_j$.

\section{Significancia Estad\'istica}

La significancia estad\'istica se utiliza para determinar si los coeficientes del modelo son significativamente diferentes de cero. Esto se eval\'ua mediante pruebas de hip\'otesis.

\subsection{Prueba de Hip\'otesis}

Para cada coeficiente $\beta_j$, la hip\'otesis nula $H_0$ es que $\beta_j = 0$. La hip\'otesis alternativa $H_a$ es que $\beta_j \neq 0$.

\subsection{P-valor}

El p-valor indica la probabilidad de obtener un coeficiente tan extremo como el observado, asumiendo que la hip\'otesis nula es verdadera. Un p-valor menor que el nivel de significancia $\alpha$ (t\'ipicamente 0.05) indica que podemos rechazar la hip\'otesis nula.

\section{Implementaci\'on en R}

\subsection{C\'alculo de Coeficientes y Odds Ratios}

\begin{verbatim}
# Cargar el paquete necesario
library(broom)

# Entrenar el modelo de regresi\'on log\'istica
model <- glm(var1 ~ ., data = dataTrain, family = "binomial")

# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}

\subsection{Intervalos de Confianza}

\begin{verbatim}
# Intervalos de confianza para los coeficientes
confint(model)

# Intervalos de confianza para las odds ratios
exp(confint(model))
\end{verbatim}

\subsection{P-valores y Significancia Estad\'istica}

\begin{verbatim}
# Resumen del modelo con p-valores
summary(model)
\end{verbatim}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Regresi\'on Log\'istica Multinomial y An\'alisis de Supervivencia}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}

La regresi\'on log\'istica multinomial y el an\'alisis de supervivencia son extensiones de la regresi\'on log\'istica binaria. Este cap\'itulo se enfoca en las t\'ecnicas y aplicaciones de estos m\'etodos avanzados.

\section{Regresi\'on Log\'istica Multinomial}

La regresi\'on log\'istica multinomial se utiliza cuando la variable dependiente tiene m\'as de dos categor\'ias.

\subsection{Modelo Multinomial}

El modelo de regresi\'on log\'istica multinomial generaliza el modelo binario para manejar m\'ultiples categor\'ias. La probabilidad de que una observaci\'on pertenezca a la categor\'ia $k$ se expresa como:

\begin{eqnarray*}
P(Y = k) = \frac{e^{\beta_{0k} + \beta_{1k} X_1 + \ldots + \beta_{nk} X_n}}{\sum_{j=1}^{K} e^{\beta_{0j} + \beta_{1j} X_1 + \ldots + \beta_{nj} X_n}}
\end{eqnarray*}

\subsection{Estimaci\'on de Par\'ametros}

Los coeficientes del modelo multinomial se estiman utilizando m\'axima verosimilitud, similar a la regresi\'on log\'istica binaria.

\section{An\'alisis de Supervivencia}

El an\'alisis de supervivencia se utiliza para modelar el tiempo hasta que ocurre un evento de inter\'es, como la muerte o la falla de un componente.

\subsection{Funci\'on de Supervivencia}

La funci\'on de supervivencia $S(t)$ describe la probabilidad de que una observaci\'on sobreviva m\'as all\'a del tiempo $t$:

\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}

\subsection{Modelo de Riesgos Proporcionales de Cox}

El modelo de Cox es un modelo de regresi\'on semiparam\'etrico utilizado para analizar datos de supervivencia:

\begin{eqnarray*}
h(t|X) = h_0(t) e^{\beta_1 X_1 + \ldots + \beta_p X_p}
\end{eqnarray*}
donde $h(t|X)$ es la tasa de riesgo en el tiempo $t$ dado el vector de covariables $X$ y $h_0(t)$ es la tasa de riesgo basal.

\section{Implementaci\'on en R}

\subsection{Regresi\'on Log\'istica Multinomial}

\begin{verbatim}
# Cargar el paquete necesario
library(nnet)

# Entrenar el modelo de regresi\'on log\'istica multinomial
model_multinom <- multinom(var1 ~ ., data = dataTrain)

# Resumen del modelo
summary(model_multinom)
\end{verbatim}

\subsection{An\'alisis de Supervivencia}

\begin{verbatim}
# Cargar el paquete necesario
library(survival)

# Crear el objeto de supervivencia
surv_object <- Surv(time = data$time, event = data$status)

# Ajustar el modelo de Cox
model_cox <- coxph(surv_object ~ var1 + var2, data = data)

# Resumen del modelo
summary(model_cox)
\end{verbatim}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Implementaci\'on de Regresi\'on Log\'istica en Datos Reales}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

\section{Introducci\'on}

Implementar un modelo de regresi\'on log\'istica en datos reales implica varias etapas, desde la limpieza de datos hasta la evaluaci\'on y validaci\'on del modelo. Este cap\'itulo presenta un ejemplo pr\'actico de la implementaci\'on de un modelo de regresi\'on log\'istica utilizando un conjunto de datos real.

\section{Conjunto de Datos}

Para este ejemplo, utilizaremos un conjunto de datos disponible p\'ublicamente que contiene informaci\'on sobre clientes bancarios. El objetivo es predecir si un cliente suscribir\'a un dep\'osito a plazo fijo.

\section{Preparaci\'on de Datos}

\subsection{Carga y Exploraci\'on de Datos}

Primero, cargamos y exploramos el conjunto de datos para entender su estructura y contenido.

\begin{verbatim}
# Cargar el paquete necesario
library(dplyr)

# Cargar el conjunto de datos
data <- read.csv("bank.csv")

# Explorar los datos
str(data)
summary(data)
\end{verbatim}

\subsection{Limpieza de Datos}

El siguiente paso es limpiar los datos, lo que incluye tratar los valores faltantes y eliminar las duplicidades.

\begin{verbatim}
# Eliminar duplicados
data <- data %>% distinct()

# Imputar valores faltantes (si existen)
data <- data %>% mutate_if(is.numeric, ~ifelse(is.na(.), mean(., na.rm = TRUE), .))
\end{verbatim}

\subsection{Codificaci\'on de Variables Categ\'oricas}

Convertimos las variables categ\'oricas en variables num\'ericas utilizando la codificaci\'on one-hot.

\begin{verbatim}
# Codificaci\'on one-hot de variables categ\'oricas
data <- data %>% mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))
\end{verbatim}

\section{Divisi\'on de Datos}

Dividimos los datos en conjuntos de entrenamiento y prueba.

\begin{verbatim}
# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(123)
trainIndex <- createDataPartition(data$y, p = .8, list = FALSE, times = 1)
dataTrain <- data[trainIndex,]
dataTest <- data[-trainIndex,]
\end{verbatim}

\section{Entrenamiento del Modelo}

Entrenamos un modelo de regresi\'on log\'istica utilizando el conjunto de entrenamiento.

\begin{verbatim}
# Entrenar el modelo de regresi\'on log\'istica
model <- glm(y ~ ., data = dataTrain, family = "binomial")

# Resumen del modelo
summary(model)
\end{verbatim}

\section{Evaluaci\'on del Modelo}

Evaluamos el rendimiento del modelo utilizando el conjunto de prueba.

\begin{verbatim}
# Predicciones en el conjunto de prueba
predictions <- predict(model, dataTest, type = "response")

# Convertir probabilidades a etiquetas
predicted_labels <- ifelse(predictions > 0.5, 1, 0)

# Matriz de confusi\'on
confusionMatrix(predicted_labels, dataTest$y)
\end{verbatim}

\section{Interpretaci\'on de los Resultados}

Interpretamos los coeficientes del modelo y las odds ratios.

\begin{verbatim}
# Coeficientes del modelo
coef(model)

# Odds ratios
exp(coef(model))
\end{verbatim}


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Resumen y Proyecto Final}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Resumen de Conceptos Clave}

En este curso, hemos cubierto una variedad de conceptos y t\'ecnicas esenciales para la regresi\'on log\'istica. Los conceptos clave incluyen:

\begin{itemize}
    \item \textbf{Fundamentos de Probabilidad y Estad\'istica}: Comprensi\'on de distribuciones de probabilidad, medidas de tendencia central y dispersi\'on, inferencia estad\'istica y pruebas de hip\'otesis.
    \item \textbf{Regresi\'on Log\'istica}: Modelo de regresi\'on log\'istica binaria y multinomial, interpretaci\'on de coeficientes y odds ratios, m\'etodos de estimaci\'on y validaci\'on.
    \item \textbf{Preparaci\'on de Datos}: Limpieza de datos, tratamiento de valores faltantes, codificaci\'on de variables categ\'oricas y selecci\'on de variables.
    \item \textbf{Evaluaci\'on del Modelo}: Curva ROC, AUC, matriz de confusi\'on, precisi\'on, recall, F1-score y validaci\'on cruzada.
    \item \textbf{Diagn\'ostico del Modelo}: An\'alisis de residuos, influencia, multicolinealidad y ajuste de par\'ametros.
    \item \textbf{An\'alisis de Supervivencia}: Modelos de supervivencia, funci\'on de supervivencia y modelos de riesgos proporcionales de Cox.
\end{itemize}

\section{Buenas Pr\'acticas}

Al implementar modelos de regresi\'on log\'istica, es importante seguir buenas pr\'acticas para garantizar la precisi\'on y la robustez de los modelos. Algunas buenas pr\'acticas incluyen:

\begin{itemize}
    \item \textbf{Exploraci\'on y Preparaci\'on de Datos}: Realizar un an\'alisis exploratorio exhaustivo y preparar los datos adecuadamente antes de construir el modelo.
    \item \textbf{Evaluaci\'on y Validaci\'on del Modelo}: Utilizar m\'etricas adecuadas para evaluar el rendimiento del modelo y validar el modelo utilizando t\'ecnicas como la validaci\'on cruzada.
    \item \textbf{Interpretaci\'on de Resultados}: Interpretar correctamente los coeficientes del modelo y las odds ratios, y comunicar los resultados de manera clara y concisa.
    \item \textbf{Revisi\'on y Ajuste del Modelo}: Diagnosticar problemas en el modelo y ajustar los par\'ametros para mejorar el rendimiento.
\end{itemize}

\section{Proyecto Final}

Para aplicar los conceptos y t\'ecnicas aprendidos en este curso, te proponemos realizar un proyecto final utilizando un conjunto de datos de tu elecci\'on. El proyecto debe incluir las siguientes etapas:

\subsection{Selecci\'on del Conjunto de Datos}

Elige un conjunto de datos relevante que contenga una variable dependiente binaria o multinomial y varias variables independientes.

\subsection{Exploraci\'on y Preparaci\'on de Datos}

Realiza un an\'alisis exploratorio de los datos y prepara los datos para el modelado. Esto incluye la limpieza de datos, el tratamiento de valores faltantes y la codificaci\'on de variables categ\'oricas.

\subsection{Entrenamiento y Evaluaci\'on del Modelo}

Entrena un modelo de regresi\'on log\'istica utilizando el conjunto de datos preparado y eval\'ua su rendimiento utilizando m\'etricas apropiadas.

\subsection{Interpretaci\'on de Resultados}

Interpreta los coeficientes del modelo y las odds ratios, y proporciona una explicaci\'on clara de los resultados.

\subsection{Presentaci\'on del Proyecto}

Presenta tu proyecto en un informe detallado que incluya la descripci\'on del conjunto de datos, los pasos de preparaci\'on y modelado, los resultados del modelo y las conclusiones.



%==<>====<>====<>====<>====<>====<>====<>====<>====<>====<>====
\part{SEGUNDA PARTE: ANALISIS DE SUPERVIVENCIA}
%==<>====<>====<>====<>====<>====<>====<>====<>====<>====<>====

\chapter{Introducci\'on al An\'alisis de Supervivencia}

\section{Conceptos B\'asicos}
El an\'alisis de supervivencia es una rama de la estad\'istica que se ocupa del an\'alisis del tiempo que transcurre hasta que ocurre un evento de inter\'es, com\'unmente referido como "tiempo de falla". Este campo es ampliamente utilizado en medicina, biolog\'ia, ingenier\'ia, ciencias sociales, y otros campos.

\section{Definici\'on de Eventos y Tiempos}
En el an\'alisis de supervivencia, un "evento" se refiere a la ocurrencia de un evento espec\'ifico, como la muerte, la falla de un componente, la reca\'ida de una enfermedad, etc. El "tiempo de supervivencia" es el tiempo que transcurre desde un punto de inicio definido hasta la ocurrencia del evento.

\section{Censura}
La censura ocurre cuando la informaci\'on completa sobre el tiempo hasta el evento no est\'a disponible para todos los individuos en el estudio. Hay tres tipos principales de censura:
\begin{itemize}
    \item \textbf{Censura a la derecha:} Ocurre cuando el evento de inter\'es no se ha observado para algunos sujetos antes del final del estudio.
    \item \textbf{Censura a la izquierda:} Ocurre cuando el evento de inter\'es ocurri\'o antes del inicio del periodo de observaci\'on.
    \item \textbf{Censura por intervalo:} Ocurre cuando el evento de inter\'es se sabe que ocurri\'o en un intervalo de tiempo, pero no se conoce el momento exacto.
\end{itemize}

\section{Funci\'on de Supervivencia}
La funci\'on de supervivencia, $S(t)$, se define como la probabilidad de que un individuo sobreviva m\'as all\'a de un tiempo $t$. Matem\'aticamente, se expresa como:
\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}
donde $T$ es una variable aleatoria que representa el tiempo hasta el evento. La funci\'on de supervivencia tiene las siguientes propiedades:
\begin{itemize}
    \item $S(0) = 1$: Esto indica que al inicio (tiempo $t=0$), la probabilidad de haber experimentado el evento es cero, por lo tanto, la supervivencia es del 100%.
    \item $\lim_{t \to \infty} S(t) = 0$: A medida que el tiempo tiende al infinito, la probabilidad de que cualquier individuo a\'un no haya experimentado el evento tiende a cero.
    \item $S(t)$ es una funci\'on no creciente: Esto significa que a medida que el tiempo avanza, la probabilidad de supervivencia no aumenta.
\end{itemize}

\section{Funci\'on de Densidad de Probabilidad}
La funci\'on de densidad de probabilidad $f(t)$ describe la probabilidad de que el evento ocurra en un instante de tiempo espec\'ifico. Se define como:
\begin{eqnarray*}
f(t) = \frac{dF(t)}{dt}
\end{eqnarray*}
donde $F(t)$ es la funci\'on de distribuci\'on acumulada, $F(t) = P(T \leq t)$. La relaci\'on entre $S(t)$ y $f(t)$ es:
\begin{eqnarray*}
f(t) = -\frac{dS(t)}{dt}
\end{eqnarray*}

\section{Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, tambi\'en conocida como funci\'on de tasa de fallas o hazard rate, se define como la tasa instant\'anea de ocurrencia del evento en el tiempo $t$, dado que el individuo ha sobrevivido hasta el tiempo $t$. Matem\'aticamente, se expresa como:
\begin{eqnarray*}
\lambda(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{eqnarray*}
Esto se puede reescribir usando $f(t)$ y $S(t)$ como:
\begin{eqnarray*}
\lambda(t) = \frac{f(t)}{S(t)}
\end{eqnarray*}

\section{Relaci\'on entre Funci\'on de Supervivencia y Funci\'on de Riesgo}
La funci\'on de supervivencia y la funci\'on de riesgo est\'an relacionadas a trav\'es de la siguiente ecuaci\'on:
\begin{eqnarray*}
S(t) = \exp\left(-\int_0^t \lambda(u) \, du\right)
\end{eqnarray*}
Esta f\'ormula se deriva del hecho de que la funci\'on de supervivencia es la probabilidad acumulativa de no haber experimentado el evento hasta el tiempo $t$, y $\lambda(t)$ es la tasa instant\'anea de ocurrencia del evento.

La funci\'on de riesgo tambi\'en puede ser expresada como:
\begin{eqnarray*}
\lambda(t) = -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Deducci\'on de la Funci\'on de Supervivencia}
La relaci\'on entre la funci\'on de supervivencia y la funci\'on de riesgo se puede deducir integrando la funci\'on de riesgo:
\begin{eqnarray*}
S(t) &=& \exp\left(-\int_0^t \lambda(u) \, du\right) \\
\log S(t) &=& -\int_0^t \lambda(u) \, du \\
\frac{d}{dt} \log S(t) &=& -\lambda(t) \\
\lambda(t) &=& -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Ejemplo de C\'alculo}
Supongamos que tenemos una muestra de tiempos de supervivencia $T_1, T_2, \ldots, T_n$. Podemos estimar la funci\'on de supervivencia emp\'irica como:
\begin{eqnarray*}
\hat{S}(t) = \frac{\text{N\'umero de individuos que sobreviven m\'as all\'a de } t}{\text{N\'umero total de individuos en riesgo en } t}
\end{eqnarray*}
y la funci\'on de riesgo emp\'irica como:
\begin{eqnarray*}
\hat{\lambda}(t) = \frac{\text{N\'umero de eventos en } t}{\text{N\'umero de individuos en riesgo en } t}
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis de supervivencia es una herramienta poderosa para analizar datos de tiempo hasta evento. Entender los conceptos b\'asicos como la funci\'on de supervivencia y la funci\'on de riesgo es fundamental para el an\'alisis m\'as avanzado.


\chapter{Funci\'on de Supervivencia y Funci\'on de Riesgo}
\section{Introducci\'on}
Este cap\'itulo profundiza en la definici\'on y propiedades de la funci\'on de supervivencia y la funci\'on de riesgo, dos conceptos fundamentales en el an\'alisis de supervivencia. Entender estas funciones y su relaci\'on es crucial para modelar y analizar datos de tiempo hasta evento.

\section{Funci\'on de Supervivencia}
La funci\'on de supervivencia, $S(t)$, describe la probabilidad de que un individuo sobreviva m\'as all\'a de un tiempo $t$. Formalmente, se define como:
\begin{eqnarray*}
S(t) = P(T > t)
\end{eqnarray*}
donde $T$ es una variable aleatoria que representa el tiempo hasta el evento.

\subsection{Propiedades de la Funci\'on de Supervivencia}
La funci\'on de supervivencia tiene varias propiedades importantes:
\begin{itemize}
    \item $S(0) = 1$: Indica que la probabilidad de haber experimentado el evento en el tiempo 0 es cero.
    \item $\lim_{t \to \infty} S(t) = 0$: A medida que el tiempo tiende al infinito, la probabilidad de supervivencia tiende a cero.
    \item $S(t)$ es una funci\'on no creciente: A medida que el tiempo avanza, la probabilidad de supervivencia no aumenta.
\end{itemize}

\subsection{Derivaci\'on de $S(t)$}
Si la funci\'on de densidad de probabilidad $f(t)$ del tiempo de supervivencia $T$ es conocida, la funci\'on de supervivencia puede derivarse como:
\begin{eqnarray*}
S(t) &=& P(T > t) \\
     &=& 1 - P(T \leq t) \\
     &=& 1 - F(t) \\
     &=& 1 - \int_0^t f(u) \, du
\end{eqnarray*}
donde $F(t)$ es la funci\'on de distribuci\'on acumulada.

\subsection{Ejemplo de C\'alculo de $S(t)$}
Consideremos un ejemplo donde el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con tasa $\lambda$. La funci\'on de densidad de probabilidad $f(t)$ es:
\begin{eqnarray*}
f(t) = \lambda e^{-\lambda t}, \quad t \geq 0
\end{eqnarray*}
La funci\'on de distribuci\'on acumulada $F(t)$ es:
\begin{eqnarray*}
F(t) = \int_0^t \lambda e^{-\lambda u} \, du = 1 - e^{-\lambda t}
\end{eqnarray*}
Por lo tanto, la funci\'on de supervivencia $S(t)$ es:
\begin{eqnarray*}
S(t) = 1 - F(t) = e^{-\lambda t}
\end{eqnarray*}

\section{Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, proporciona la tasa instant\'anea de ocurrencia del evento en el tiempo $t$, dado que el individuo ha sobrevivido hasta el tiempo $t$. Matem\'aticamente, se define como:
\begin{eqnarray*}
\lambda(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t}
\end{eqnarray*}

\subsection{Relaci\'on entre $\lambda(t)$ y $f(t)$}
La funci\'on de riesgo se puede relacionar con la funci\'on de densidad de probabilidad $f(t)$ y la funci\'on de supervivencia $S(t)$ de la siguiente manera:
\begin{eqnarray*}
\lambda(t) &=& \frac{f(t)}{S(t)}
\end{eqnarray*}

\subsection{Derivaci\'on de $\lambda(t)$}
La derivaci\'on de $\lambda(t)$ se basa en la definici\'on condicional de la probabilidad:
\begin{eqnarray*}
\lambda(t) &=& \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t} \\
           &=& \lim_{\Delta t \to 0} \frac{\frac{P(t \leq T < t + \Delta t \text{ y } T \geq t)}{P(T \geq t)}}{\Delta t} \\
           &=& \lim_{\Delta t \to 0} \frac{\frac{P(t \leq T < t + \Delta t)}{P(T \geq t)}}{\Delta t} \\
           &=& \frac{f(t)}{S(t)}
\end{eqnarray*}

\section{Relaci\'on entre Funci\'on de Supervivencia y Funci\'on de Riesgo}
La funci\'on de supervivencia y la funci\'on de riesgo est\'an estrechamente relacionadas. La relaci\'on se expresa mediante la siguiente ecuaci\'on:
\begin{eqnarray*}
S(t) = \exp\left(-\int_0^t \lambda(u) \, du\right)
\end{eqnarray*}

\subsection{Deducci\'on de la Relaci\'on}
Para deducir esta relaci\'on, consideramos la derivada logar\'itmica de la funci\'on de supervivencia:
\begin{eqnarray*}
S(t) &=& \exp\left(-\int_0^t \lambda(u) \, du\right) \\
\log S(t) &=& -\int_0^t \lambda(u) \, du \\
\frac{d}{dt} \log S(t) &=& -\lambda(t) \\
\lambda(t) &=& -\frac{d}{dt} \log S(t)
\end{eqnarray*}

\section{Interpretaci\'on de la Funci\'on de Riesgo}
La funci\'on de riesgo, $\lambda(t)$, se interpreta como la tasa instant\'anea de ocurrencia del evento por unidad de tiempo, dado que el individuo ha sobrevivido hasta el tiempo $t$. Es una medida local del riesgo de falla en un instante espec\'ifico.

\subsection{Ejemplo de C\'alculo de $\lambda(t)$}
Consideremos nuevamente el caso donde el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con tasa $\lambda$. La funci\'on de densidad de probabilidad $f(t)$ es:
\begin{eqnarray*}
f(t) = \lambda e^{-\lambda t}
\end{eqnarray*}
La funci\'on de supervivencia $S(t)$ es:
\begin{eqnarray*}
S(t) = e^{-\lambda t}
\end{eqnarray*}
La funci\'on de riesgo $\lambda(t)$ se calcula como:
\begin{eqnarray*}
\lambda(t) &=& \frac{f(t)}{S(t)} \\
           &=& \frac{\lambda e^{-\lambda t}}{e^{-\lambda t}} \\
           &=& \lambda
\end{eqnarray*}
En este caso, $\lambda(t)$ es constante y igual a $\lambda$, lo que es una caracter\'istica de la distribuci\'on exponencial.

\section{Funciones de Riesgo Acumulada y Media Residual}
La funci\'on de riesgo acumulada $H(t)$ se define como:
\begin{eqnarray*}
H(t) = \int_0^t \lambda(u) \, du
\end{eqnarray*}
Esta funci\'on proporciona la suma acumulada de la tasa de riesgo hasta el tiempo $t$.

La funci\'on de vida media residual $e(t)$ se define como la esperanza del tiempo de vida restante dado que el individuo ha sobrevivido hasta el tiempo $t$:
\begin{eqnarray*}
e(t) = \mathbb{E}[T - t \mid T > t] = \int_t^\infty S(u) \, du
\end{eqnarray*}

\section{Ejemplo de C\'alculo de Funci\'on de Riesgo Acumulada y Vida Media Residual}
Consideremos nuevamente la distribuci\'on exponencial con tasa $\lambda$. La funci\'on de riesgo acumulada $H(t)$ es:
\begin{eqnarray*}
H(t) &=& \int_0^t \lambda \, du \\
     &=& \lambda t
\end{eqnarray*}

La funci\'on de vida media residual $e(t)$ es:
\begin{eqnarray*}
e(t) &=& \int_t^\infty e^{-\lambda u} \, du \\
     &=& \left[ \frac{-1}{\lambda} e^{-\lambda u} \right]_t^\infty \\
     &=& \frac{1}{\lambda} e^{-\lambda t} \\
     &=& \frac{1}{\lambda}
\end{eqnarray*}
En este caso, la vida media residual es constante e igual a $\frac{1}{\lambda}$, otra caracter\'istica de la distribuci\'on exponencial.

\section{Conclusi\'on}
La funci\'on de supervivencia y la funci\'on de riesgo son herramientas fundamentales en el an\'alisis de supervivencia. Entender su definici\'on, propiedades, y la relaci\'on entre ellas es esencial para modelar y analizar correctamente los datos de tiempo hasta evento. Las funciones de riesgo acumulada y vida media residual proporcionan informaci\'on adicional sobre la din\'amica del riesgo a lo largo del tiempo.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Estimador de Kaplan-Meier}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}
El estimador de Kaplan-Meier, tambi\'en conocido como la funci\'on de supervivencia emp\'irica, es una herramienta no param\'etrica para estimar la funci\'on de supervivencia a partir de datos censurados. Este m\'etodo es especialmente \'util cuando los tiempos de evento est\'an censurados a la derecha.

\section{Definici\'on del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier se define como:
\begin{eqnarray*}
\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $t_i$ es el tiempo del $i$-\'esimo evento,
    \item $d_i$ es el n\'umero de eventos que ocurren en $t_i$,
    \item $n_i$ es el n\'umero de individuos en riesgo justo antes de $t_i$.
\end{itemize}

\section{Propiedades del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier tiene las siguientes propiedades:
\begin{itemize}
    \item Es una funci\'on escalonada que disminuye en los tiempos de los eventos observados.
    \item Puede manejar datos censurados a la derecha.
    \item Proporciona una estimaci\'on no param\'etrica de la funci\'on de supervivencia.
\end{itemize}

\subsection{Funci\'on Escalonada}
La funci\'on escalonada del estimador de Kaplan-Meier significa que $\hat{S}(t)$ permanece constante entre los tiempos de los eventos y disminuye en los tiempos de los eventos. Matem\'aticamente, si $t_i$ es el tiempo del $i$-\'esimo evento, entonces:
\begin{eqnarray*}
\hat{S}(t) = \hat{S}(t_i) \quad \text{para} \ t_i \leq t < t_{i+1}
\end{eqnarray*}

\subsection{Manejo de Datos Censurados}
El estimador de Kaplan-Meier maneja datos censurados a la derecha al ajustar la estimaci\'on de la funci\'on de supervivencia s\'olo en los tiempos en que ocurren eventos. Si un individuo es censurado antes de experimentar el evento, no contribuye a la disminuci\'on de $\hat{S}(t)$ en el tiempo de censura. Esto asegura que la censura no sesga la estimaci\'on de la supervivencia.

\subsection{Estimaci\'on No Param\'etrica}
El estimador de Kaplan-Meier es no param\'etrico porque no asume ninguna forma espec\'ifica para la distribuci\'on de los tiempos de supervivencia. En cambio, utiliza la informaci\'on emp\'irica disponible para estimar la funci\'on de supervivencia.

\section{Deducci\'on del Estimador de Kaplan-Meier}
La deducci\'on del estimador de Kaplan-Meier se basa en el principio de probabilidad condicional. Consideremos un conjunto de tiempos de supervivencia observados $t_1, t_2, \ldots, t_k$ con eventos en cada uno de estos tiempos. El estimador de la probabilidad de supervivencia m\'as all\'a del tiempo $t$ es el producto de las probabilidades de sobrevivir m\'as all\'a de cada uno de los tiempos de evento observados hasta $t$.

\subsection{Probabilidad Condicional}
La probabilidad de sobrevivir m\'as all\'a de $t_i$, dado que el individuo ha sobrevivido justo antes de $t_i$, es:
\begin{eqnarray*}
P(T > t_i \mid T \geq t_i) = 1 - \frac{d_i}{n_i}
\end{eqnarray*}
donde $d_i$ es el n\'umero de eventos en $t_i$ y $n_i$ es el n\'umero de individuos en riesgo justo antes de $t_i$.

\subsection{Producto de Probabilidades Condicionales}
La probabilidad de sobrevivir m\'as all\'a de un tiempo $t$ cualquiera, dada la secuencia de tiempos de evento, es el producto de las probabilidades condicionales de sobrevivir m\'as all\'a de cada uno de los tiempos de evento observados hasta $t$. As\'i, el estimador de Kaplan-Meier se obtiene como:
\begin{eqnarray*}
\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)
\end{eqnarray*}

\section{Ejemplo de C\'alculo}
Supongamos que tenemos los siguientes tiempos de supervivencia observados para cinco individuos: 2, 3, 5, 7, 8. Supongamos adem\'as que tenemos censura a la derecha en el tiempo 10. Los tiempos de evento y el n\'umero de individuos en riesgo justo antes de cada evento son:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
Tiempo ($t_i$) & Eventos ($d_i$) & En Riesgo ($n_i$) \\
\hline
2 & 1 & 5 \\
3 & 1 & 4 \\
5 & 1 & 3 \\
7 & 1 & 2 \\
8 & 1 & 1 \\
\hline
\end{tabular}
\caption{Ejemplo de c\'alculo del estimador de Kaplan-Meier}
\end{table}

Usando estos datos, el estimador de Kaplan-Meier se calcula como:
\begin{eqnarray*}
\hat{S}(2) &=& 1 - \frac{1}{5} = 0.8 \\
\hat{S}(3) &=& 0.8 \times \left(1 - \frac{1}{4}\right) = 0.8 \times 0.75 = 0.6 \\
\hat{S}(5) &=& 0.6 \times \left(1 - \frac{1}{3}\right) = 0.6 \times 0.6667 = 0.4 \\
\hat{S}(7) &=& 0.4 \times \left(1 - \frac{1}{2}\right) = 0.4 \times 0.5 = 0.2 \\
\hat{S}(8) &=& 0.2 \times \left(1 - \frac{1}{1}\right) = 0.2 \times 0 = 0 \\
\end{eqnarray*}

\section{Intervalos de Confianza para el Estimador de Kaplan-Meier}
Para calcular intervalos de confianza para el estimador de Kaplan-Meier, se puede usar la transformaci\'on logar\'itmica y la aproximaci\'on normal. Un intervalo de confianza aproximado para $\log(-\log(\hat{S}(t)))$ se obtiene como:
\begin{eqnarray*}
\log(-\log(\hat{S}(t))) \pm z_{\alpha/2} \sqrt{\frac{1}{d_i(n_i - d_i)}}
\end{eqnarray*}
donde $z_{\alpha/2}$ es el percentil correspondiente de la distribuci\'on normal est\'andar.

\section{Transformaci\'on Logar\'itmica Inversa}
La transformaci\'on logar\'itmica inversa se utiliza para obtener los l\'imites del intervalo de confianza para $S(t)$:
\begin{eqnarray*}
\hat{S}(t) = \exp\left(-\exp\left(\log(-\log(\hat{S}(t))) \pm z_{\alpha/2} \sqrt{\frac{1}{d_i(n_i - d_i)}}\right)\right)
\end{eqnarray*}

\section{C\'alculo Detallado de Intervalos de Confianza}
Para un c\'alculo m\'as detallado de los intervalos de confianza, consideremos un tiempo espec\'ifico $t_j$. La varianza del estimador de Kaplan-Meier en $t_j$ se puede estimar usando Greenwood's formula:
\begin{eqnarray*}
\text{Var}(\hat{S}(t_j)) = \hat{S}(t_j)^2 \sum_{t_i \leq t_j} \frac{d_i}{n_i(n_i - d_i)}
\end{eqnarray*}
El intervalo de confianza aproximado para $\hat{S}(t_j)$ es entonces:
\begin{eqnarray*}
\hat{S}(t_j) \pm z_{\alpha/2} \sqrt{\text{Var}(\hat{S}(t_j))}
\end{eqnarray*}

\section{Ejemplo de Intervalo de Confianza}
Supongamos que en el ejemplo anterior queremos calcular el intervalo de confianza para $\hat{S}(3)$. Primero, calculamos la varianza:
\begin{eqnarray*}
\text{Var}(\hat{S}(3)) &=& \hat{S}(3)^2 \left( \frac{1}{5 \times 4} + \frac{1}{4 \times 3} \right) \\
                       &=& 0.6^2 \left( \frac{1}{20} + \frac{1}{12} \right) \\
                       &=& 0.36 \left( 0.05 + 0.0833 \right) \\
                       &=& 0.36 \times 0.1333 \\
                       &=& 0.048
\end{eqnarray*}
El intervalo de confianza es entonces:
\begin{eqnarray*}
0.6 \pm 1.96 \sqrt{0.048} = 0.6 \pm 1.96 \times 0.219 = 0.6 \pm 0.429
\end{eqnarray*}
Por lo tanto, el intervalo de confianza para $\hat{S}(3)$ es aproximadamente $(0.171, 1.029)$. Dado que una probabilidad no puede exceder 1, ajustamos el intervalo a $(0.171, 1.0)$.

\section{Interpretaci\'on del Estimador de Kaplan-Meier}
El estimador de Kaplan-Meier proporciona una estimaci\'on emp\'irica de la funci\'on de supervivencia que es f\'acil de interpretar y calcular. Su capacidad para manejar datos censurados lo hace especialmente \'util en estudios de supervivencia.

\section{Conclusi\'on}
El estimador de Kaplan-Meier es una herramienta poderosa para estimar la funci\'on de supervivencia en presencia de datos censurados. Su c\'alculo es relativamente sencillo y proporciona una estimaci\'on no param\'etrica robusta de la supervivencia a lo largo del tiempo. La interpretaci\'on adecuada de este estimador y su intervalo de confianza asociado es fundamental para el an\'alisis de datos de supervivencia.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Comparaci\'on de Curvas de Supervivencia}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}
Comparar curvas de supervivencia es crucial para determinar si existen diferencias significativas en las tasas de supervivencia entre diferentes grupos. Las pruebas de hip\'otesis, como el test de log-rank, son herramientas comunes para esta comparaci\'on.

\section{Test de Log-rank}
El test de log-rank se utiliza para comparar las curvas de supervivencia de dos o m\'as grupos. La hip\'otesis nula es que no hay diferencia en las funciones de riesgo entre los grupos.

\subsection{F\'ormula del Test de Log-rank}
El estad\'istico del test de log-rank se define como:
\begin{eqnarray*}
\chi^2 = \frac{\left(\sum_{i=1}^k (O_i - E_i)\right)^2}{\sum_{i=1}^k V_i}
\end{eqnarray*}
donde:
\begin{itemize}
    \item $O_i$ es el n\'umero observado de eventos en el grupo $i$.
    \item $E_i$ es el n\'umero esperado de eventos en el grupo $i$.
    \item $V_i$ es la varianza del n\'umero de eventos en el grupo $i$.
\end{itemize}

\subsection{C\'alculo de $E_i$ y $V_i$}
El n\'umero esperado de eventos $E_i$ y la varianza $V_i$ se calculan como:
\begin{eqnarray*}
E_i &=& \frac{d_i \cdot n_i}{n} \\
V_i &=& \frac{d_i \cdot (n - d_i) \cdot n_i \cdot (n - n_i)}{n^2 \cdot (n - 1)}
\end{eqnarray*}
donde:
\begin{itemize}
    \item $d_i$ es el n\'umero total de eventos en el grupo $i$.
    \item $n_i$ es el n\'umero de individuos en riesgo en el grupo $i$.
    \item $n$ es el n\'umero total de individuos en todos los grupos.
\end{itemize}

\section{Ejemplo de C\'alculo del Test de Log-rank}
Supongamos que tenemos dos grupos con los siguientes datos de eventos:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Grupo & Tiempo ($t_i$) & Eventos ($O_i$) & En Riesgo ($n_i$) \\
\hline
1 & 2 & 1 & 5 \\
1 & 4 & 1 & 4 \\
2 & 3 & 1 & 4 \\
2 & 5 & 1 & 3 \\
\hline
\end{tabular}
\caption{Ejemplo de datos para el test de log-rank}
\end{table}

Calculemos $E_i$ y $V_i$ para cada grupo:

\begin{eqnarray*}
E_1 &=& \frac{2 \cdot 5}{9} + \frac{2 \cdot 4}{8} = \frac{10}{9} + \frac{8}{8} = 1.11 + 1 = 2.11 \\
V_1 &=& \frac{2 \cdot 7 \cdot 5 \cdot 4}{81 \cdot 8} = \frac{2 \cdot 7 \cdot 5 \cdot 4}{648} = \frac{280}{648} = 0.432 \\
E_2 &=& \frac{2 \cdot 4}{9} + \frac{2 \cdot 3}{8} = \frac{8}{9} + \frac{6}{8} = 0.89 + 0.75 = 1.64 \\
V_2 &=& \frac{2 \cdot 7 \cdot 4 \cdot 4}{81 \cdot 8} = \frac{2 \cdot 7 \cdot 4 \cdot 4}{648} = \frac{224}{648} = 0.346 \\
\end{eqnarray*}

El estad\'istico de log-rank se calcula como:
\begin{eqnarray*}
\chi^2 &=& \frac{\left((1 - 2.11) + (1 - 1.64)\right)^2}{0.432 + 0.346} \\
       &=& \frac{\left(-1.11 - 0.64\right)^2}{0.778} \\
       &=& \frac{3.04}{0.778} \\
       &=& 3.91
\end{eqnarray*}

El valor p se puede obtener comparando $\chi^2$ con una distribuci\'on $\chi^2$ con un grado de libertad (dado que estamos comparando dos grupos).

\section{Interpretaci\'on del Test de Log-rank}
Un valor p peque\~no (generalmente menos de 0.05) indica que hay una diferencia significativa en las curvas de supervivencia entre los grupos. Un valor p grande sugiere que no hay suficiente evidencia para rechazar la hip\'otesis nula de que las curvas de supervivencia son iguales.

\section{Pruebas Alternativas}
Adem\'as del test de log-rank, existen otras pruebas para comparar curvas de supervivencia, como el test de Wilcoxon (Breslow), que da m\'as peso a los eventos en tiempos tempranos.

\section{Conclusi\'on}
El test de log-rank es una herramienta esencial para comparar curvas de supervivencia entre diferentes grupos. Su c\'alculo se basa en la diferencia entre los eventos observados y esperados en cada grupo, y su interpretaci\'on puede ayudar a identificar diferencias significativas en la supervivencia.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Modelos de Riesgos Proporcionales de Cox}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}
El modelo de riesgos proporcionales de Cox, propuesto por David Cox en 1972, es una de las herramientas m\'as utilizadas en el an\'alisis de supervivencia. Este modelo permite evaluar el efecto de varias covariables en el tiempo hasta el evento, sin asumir una forma espec\'ifica para la distribuci\'on de los tiempos de supervivencia.

\section{Definici\'on del Modelo de Cox}
El modelo de Cox se define como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta^T X)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $\lambda(t \mid X)$ es la funci\'on de riesgo en el tiempo $t$ dado el vector de covariables $X$.
    \item $\lambda_0(t)$ es la funci\'on de riesgo basal en el tiempo $t$.
    \item $\beta$ es el vector de coeficientes del modelo.
    \item $X$ es el vector de covariables.
\end{itemize}

\section{Supuesto de Proporcionalidad de Riesgos}
El modelo de Cox asume que las razones de riesgo entre dos individuos son constantes a lo largo del tiempo. Matem\'aticamente, si $X_i$ y $X_j$ son las covariables de dos individuos, la raz\'on de riesgos se expresa como:
\begin{eqnarray*}
\frac{\lambda(t \mid X_i)}{\lambda(t \mid X_j)} = \frac{\lambda_0(t) \exp(\beta^T X_i)}{\lambda_0(t) \exp(\beta^T X_j)} = \exp(\beta^T (X_i - X_j))
\end{eqnarray*}

\section{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud parcial. La funci\'on de verosimilitud parcial se define como:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^k \frac{\exp(\beta^T X_i)}{\sum_{j \in R(t_i)} \exp(\beta^T X_j)}
\end{eqnarray*}
donde $R(t_i)$ es el conjunto de individuos en riesgo en el tiempo $t_i$.

\subsection{Funci\'on de Log-Verosimilitud Parcial}
La funci\'on de log-verosimilitud parcial es:
\begin{eqnarray*}
\log L(\beta) = \sum_{i=1}^k \left(\beta^T X_i - \log \sum_{j \in R(t_i)} \exp(\beta^T X_j)\right)
\end{eqnarray*}

\subsection{Derivadas Parciales y Maximizaci\'on}
Para encontrar los estimadores de m\'axima verosimilitud, resolvemos el sistema de ecuaciones obtenido al igualar a cero las derivadas parciales de $\log L(\beta)$ con respecto a $\beta$:
\begin{eqnarray*}
\frac{\partial \log L(\beta)}{\partial \beta} = \sum_{i=1}^k \left(X_i - \frac{\sum_{j \in R(t_i)} X_j \exp(\beta^T X_j)}{\sum_{j \in R(t_i)} \exp(\beta^T X_j)}\right) = 0
\end{eqnarray*}

\section{Interpretaci\'on de los Coeficientes}
Cada coeficiente $\beta_i$ representa el logaritmo de la raz\'on de riesgos asociado con un incremento unitario en la covariable $X_i$. Un valor positivo de $\beta_i$ indica que un aumento en $X_i$ incrementa el riesgo del evento, mientras que un valor negativo indica una reducci\'on del riesgo.

\section{Evaluaci\'on del Modelo}
El modelo de Cox se eval\'ua utilizando varias t\'ecnicas, como el an\'alisis de residuos de Schoenfeld para verificar el supuesto de proporcionalidad de riesgos, y el uso de curvas de supervivencia estimadas para evaluar la bondad de ajuste.

\subsection{Residuos de Schoenfeld}
Los residuos de Schoenfeld se utilizan para evaluar la proporcionalidad de riesgos. Para cada evento en el tiempo $t_i$, el residuo de Schoenfeld para la covariable $X_j$ se define como:
\begin{eqnarray*}
r_{ij} = X_{ij} - \hat{X}_{ij}
\end{eqnarray*}
donde $\hat{X}_{ij}$ es la covariable ajustada.

\subsection{Curvas de Supervivencia Ajustadas}
Las curvas de supervivencia ajustadas se obtienen utilizando la funci\'on de riesgo basal estimada y los coeficientes del modelo. La funci\'on de supervivencia ajustada se define como:
\begin{eqnarray*}
\hat{S}(t \mid X) = \hat{S}_0(t)^{\exp(\beta^T X)}
\end{eqnarray*}
donde $\hat{S}_0(t)$ es la funci\'on de supervivencia basal estimada.

\section{Ejemplo de Aplicaci\'on del Modelo de Cox}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Supongamos que los datos se ajustan a un modelo de Cox y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.02, \quad \hat{\beta}_{sexo} = -0.5, \quad \hat{\beta}_{tratamiento} = 1.2
\end{eqnarray*}

La funci\'on de riesgo ajustada se expresa como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(0.02 \cdot \text{edad} - 0.5 \cdot \text{sexo} + 1.2 \cdot \text{tratamiento})
\end{eqnarray*}

\section{Conclusi\'on}
El modelo de riesgos proporcionales de Cox es una herramienta poderosa para analizar datos de supervivencia con m\'ultiples covariables. Su flexibilidad y la falta de suposiciones fuertes sobre la distribuci\'on de los tiempos de supervivencia lo hacen ampliamente aplicable en diversas disciplinas.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Diagn\'ostico y Validaci\'on de Modelos de Cox}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}
Una vez ajustado un modelo de Cox, es crucial realizar diagn\'osticos y validaciones para asegurar que el modelo es apropiado y que los supuestos subyacentes son v\'alidos. Esto incluye la verificaci\'on del supuesto de proporcionalidad de riesgos y la evaluaci\'on del ajuste del modelo.

\section{Supuesto de Proporcionalidad de Riesgos}
El supuesto de proporcionalidad de riesgos implica que la raz\'on de riesgos entre dos individuos es constante a lo largo del tiempo. Si este supuesto no se cumple, las inferencias hechas a partir del modelo pueden ser incorrectas.

\subsection{Residuos de Schoenfeld}
Los residuos de Schoenfeld se utilizan para evaluar la proporcionalidad de riesgos. Para cada evento en el tiempo $t_i$, el residuo de Schoenfeld para la covariable $X_j$ se define como:
\begin{eqnarray*}
r_{ij} = X_{ij} - \hat{X}_{ij}
\end{eqnarray*}
donde $\hat{X}_{ij}$ es la covariable ajustada. Si los residuos de Schoenfeld no muestran una tendencia sistem\'atica cuando se trazan contra el tiempo, el supuesto de proporcionalidad de riesgos es razonable.

\section{Bondad de Ajuste}
La bondad de ajuste del modelo de Cox se eval\'ua comparando las curvas de supervivencia observadas y ajustadas, y utilizando estad\'isticas de ajuste global.

\subsection{Curvas de Supervivencia Ajustadas}
Las curvas de supervivencia aaustadas se obtienen utilizando la funci\'on de riesgo basal estimada y los coeficientes del modelo. La funci\'on de supervivencia ajustada se define como:
\begin{eqnarray*}
\hat{S}(t \mid X) = \hat{S}_0(t)^{\exp(\beta^T X)}
\end{eqnarray*}
donde $\hat{S}_0(t)$ es la funci\'on de supervivencia basal estimada. Comparar estas curvas con las curvas de Kaplan-Meier para diferentes niveles de las covariables puede proporcionar una validaci\'on visual del ajuste del modelo.

\subsection{Estad\'isticas de Ajuste Global}
Las estad\'isticas de ajuste global, como el test de la desviaci\'on y el test de la bondad de ajuste de Grambsch y Therneau, se utilizan para evaluar el ajuste global del modelo de Cox.

\section{Diagn\'ostico de Influencia}
El diagn\'ostico de influencia identifica observaciones individuales que tienen un gran impacto en los estimados del modelo. Los residuos de devianza y los residuos de martingala se utilizan com\'unmente para este prop\'osito.

\subsection{Residuos de Deviance}
Los residuos de deviance se definen como:
\begin{eqnarray*}
D_i = \text{sign}(O_i - E_i) \sqrt{-2 \left(O_i \log \frac{O_i}{E_i} - (O_i - E_i)\right)}
\end{eqnarray*}
donde $O_i$ es el n\'umero observado de eventos y $E_i$ es el n\'umero esperado de eventos. Observaciones con residuos de deviance grandes en valor absoluto pueden ser influyentes.

\subsection{Residuos de Martingala}
Los residuos de martingala se definen como:
\begin{eqnarray*}
M_i = O_i - E_i
\end{eqnarray*}
donde $O_i$ es el n\'umero observado de eventos y $E_i$ es el n\'umero esperado de eventos. Los residuos de martingala se utilizan para detectar observaciones que no se ajustan bien al modelo.

\section{Ejemplo de Diagn\'ostico}
Consideremos un modelo de Cox ajustado con las covariables edad, sexo y tratamiento. Para diagnosticar la influencia de observaciones individuales, calculamos los residuos de deviance y martingala para cada observaci\'on.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Observaci\'on & Edad & Sexo & Tratamiento & Residuo de Deviance \\
\hline
1 & 50 & 0 & 1 & 1.2 \\
2 & 60 & 1 & 0 & -0.5 \\
3 & 45 & 0 & 1 & -1.8 \\
4 & 70 & 1 & 0 & 0.3 \\
\hline
\end{tabular}
\caption{Residuos de deviance para observaciones individuales}
\end{table}

Observaciones con residuos de deviance grandes en valor absoluto (como la observaci\'on 3) pueden ser influyentes y requieren una revisi\'on adicional.

\section{Conclusi\'on}
El diagn\'ostico y la validaci\'on son pasos cr\'iticos en el an\'slisis de modelos de Cox. Evaluar el supuesto de proporcionalidad de riesgos, la bondad de ajuste y la influencia de observaciones individuales asegura que las inferencias y conclusiones derivadas del modelo sean v\'slidas y fiables.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Modelos Acelerados de Fallos}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

\section{Introducci\'on}
Los modelos acelerados de fallos (AFT) son una alternativa a los modelos de riesgos proporcionales de Cox. En lugar de asumir que las covariables afectan la tasa de riesgo, los modelos AFT asumen que las covariables multiplican el tiempo de supervivencia por una constante.

\section{Definici\'on del Modelo AFT}
Un modelo AFT se expresa como:
\begin{eqnarray*}
T = T_0 \exp(\beta^T X)
\end{eqnarray*}
donde:
\begin{itemize}
    \item $T$ es el tiempo de supervivencia observado.
    \item $T_0$ es el tiempo de supervivencia bajo condiciones basales.
    \item $\beta$ es el vector de coeficientes del modelo.
    \item $X$ es el vector de covariables.
\end{itemize}

\subsection{Transformaci\'on Logar\'itmica}
El modelo AFT se puede transformar logar\'itmicamente para obtener una forma lineal:
\begin{eqnarray*}
\log(T) = \log(T_0) + \beta^T X
\end{eqnarray*}

\section{Estimaci\'on de los Par\'ametros}
Los par\'ametros del modelo AFT se estiman utilizando el m\'etodo de m\'axima verosimilitud. La funci\'on de verosimilitud se define como:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n f(t_i \mid X_i; \beta)
\end{eqnarray*}
donde $f(t_i \mid X_i; \beta)$ es la funci\'on de densidad de probabilidad del tiempo de supervivencia $t_i$ dado el vector de covariables $X_i$ y los par\'ametros $\beta$.

\subsection{Funci\'on de Log-Verosimilitud}
La funci\'on de log-verosimilitud es:
\begin{eqnarray*}
\log L(\beta) = \sum_{i=1}^n \log f(t_i \mid X_i; \beta)
\end{eqnarray*}

\subsection{Maximizaci\'on de la Verosimilitud}
Los estimadores de m\'axima verosimilitud se obtienen resolviendo el sistema de ecuaciones obtenido al igualar a cero las derivadas parciales de $\log L(\beta)$ con respecto a $\beta$:
\begin{eqnarray*}
\frac{\partial \log L(\beta)}{\partial \beta} = 0
\end{eqnarray*}

\section{Distribuciones Comunes en Modelos AFT}
En los modelos AFT, el tiempo de supervivencia $T$ puede seguir varias distribuciones comunes, como la exponencial, Weibull, log-normal y log-log\'istica. Cada una de estas distribuciones tiene diferentes propiedades y aplicaciones.

\subsection{Modelo Exponencial AFT}
En un modelo exponencial AFT, el tiempo de supervivencia $T$ sigue una distribuci\'on exponencial con par\'ametro $\lambda$:
\begin{eqnarray*}
f(t) = \lambda \exp(-\lambda t)
\end{eqnarray*}
La funci\'on de supervivencia es:
\begin{eqnarray*}
S(t) = \exp(-\lambda t)
\end{eqnarray*}
La transformaci\'on logar\'itmica del tiempo de supervivencia es:
\begin{eqnarray*}
\log(T) = \log\left(\frac{1}{\lambda}\right) + \beta^T X
\end{eqnarray*}

\subsection{Modelo Weibull AFT}
En un modelo Weibull AFT, el tiempo de supervivencia $T$ sigue una distribuci\'on Weibull con par\'ametros $\lambda$ y $k$:
\begin{eqnarray*}
f(t) = \lambda k t^{k-1} \exp(-\lambda t^k)
\end{eqnarray*}
La funci\'on de supervivencia es:
\begin{eqnarray*}
S(t) = \exp(-\lambda t^k)
\end{eqnarray*}
La transformaci\'on logar\'itmica del tiempo de supervivencia es:
\begin{eqnarray*}
\log(T) = \log\left(\left(\frac{1}{\lambda}\right)^{1/k}\right) + \frac{\beta^T X}{k}
\end{eqnarray*}

\section{Interpretaci\'on de los Coeficientes}
En los modelos AFT, los coeficientes $\beta_i$ se interpretan como factores multiplicativos del tiempo de supervivencia. Un valor positivo de $\beta_i$ indica que un aumento en la covariable $X_i$ incrementa el tiempo de supervivencia, mientras que un valor negativo indica una reducci\'on del tiempo de supervivencia.

\section{Ejemplo de Aplicaci\'on del Modelo AFT}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Supongamos que los datos se ajustan a un modelo Weibull AFT y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = -0.02, \quad \hat{\beta}_{sexo} = 0.5, \quad \hat{\beta}_{tratamiento} = -1.2
\end{eqnarray*}

La funci\'on de supervivencia ajustada se expresa como:
\begin{eqnarray*}
S(t \mid X) = \exp\left(-\left(\frac{t \exp(-0.02 \cdot \text{edad} + 0.5 \cdot \text{sexo} - 1.2 \cdot \text{tratamiento})}{\lambda}\right)^k\right)
\end{eqnarray*}

\section{Conclusi\'on}
Los modelos AFT proporcionan una alternativa flexible a los modelos de riesgos proporcionales de Cox. Su enfoque en la multiplicaci\'on del tiempo de supervivencia por una constante permite una interpretaci\'on intuitiva y aplicaciones en diversas \'areas.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{An\'alisis Multivariado de Supervivencia}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->

\section{Introducci\'on}
El an\'alisis multivariado de supervivencia extiende los modelos de supervivencia para incluir m\'ultiples covariables, permitiendo evaluar su efecto simult\'aneo sobre el tiempo hasta el evento. Los modelos de Cox y AFT son com\'unmente utilizados en este contexto.

\section{Modelo de Cox Multivariado}
El modelo de Cox multivariado se define como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta^T X)
\end{eqnarray*}
donde $X$ es un vector de covariables.

\subsection{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud parcial, como se discuti\'o anteriormente. La funci\'on de verosimilitud parcial se maximiza para obtener los estimadores de los coeficientes.

\section{Modelo AFT Multivariado}
El modelo AFT multivariado se expresa como:
\begin{eqnarray*}
T = T_0 \exp(\beta^T X)
\end{eqnarray*}

\subsection{Estimaci\'on de los Par\'ametros}
Los par\'ametros $\beta$ se estiman utilizando el m\'etodo de m\'axima verosimilitud, similar al caso univariado. La funci\'on de verosimilitud se maximiza para obtener los estimadores de los coeficientes.

\section{Interacci\'on y Efectos No Lineales}
En el an\'alisis multivariado, es importante considerar la posibilidad de interacciones entre covariables y efectos no lineales. Estos se pueden incluir en los modelos extendiendo las funciones de riesgo o supervivencia.

\subsection{Interacciones}
Las interacciones entre covariables se pueden modelar a\~nadiendo t\'erminos de interacci\'on en el modelo:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2)
\end{eqnarray*}
donde $X_1 X_2$ es el t\'ermino de interacci\'on.

\subsection{Efectos No Lineales}
Los efectos no lineales se pueden modelar utilizando funciones no lineales de las covariables, como polinomios o splines:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(\beta_1 X + \beta_2 X^2)
\end{eqnarray*}

\section{Selecci\'on de Variables}
La selecci\'on de variables es crucial en el an\'alisis multivariado para evitar el sobreajuste y mejorar la interpretabilidad del modelo. M\'etodos como la regresi\'on hacia atr\'as, la regresi\'on hacia adelante y la selecci\'on por criterios de informaci\'on (AIC, BIC) son com\'unmente utilizados.

\subsection{Regresi\'on Hacia Atr\'as}
La regresi\'on hacia atr\'as comienza con todas las covariables en el modelo y elimina iterativamente la covariable menos significativa hasta que todas las covariables restantes sean significativas.

\subsection{Regresi\'on Hacia Adelante}
La regresi\'on hacia adelante comienza con un modelo vac\'io y a\~nade iterativamente la covariable m\'as significativa hasta que no se pueda a\~nadir ninguna covariable adicional significativa.

\subsection{Criterios de Informaci\'on}
Los criterios de informaci\'on, como el AIC (Akaike Information Criterion) y el BIC (Bayesian Information Criterion), se utilizan para seleccionar el modelo que mejor se ajusta a los datos con la menor complejidad posible:
\begin{eqnarray*}
AIC &=& -2 \log L + 2k \\
BIC &=& -2 \log L + k \log n
\end{eqnarray*}
donde $L$ es la funci\'on de verosimilitud del modelo, $k$ es el n\'umero de par\'ametros en el modelo y $n$ es el tama\~no de la muestra.

\section{Ejemplo de An\'alisis Multivariado}
Consideremos un ejemplo con tres covariables: edad, sexo y tratamiento. Ajustamos un modelo de Cox multivariado y obtenemos los siguientes coeficientes:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.03, \quad \hat{\beta}_{sexo} = -0.6, \quad \hat{\beta}_{tratamiento} = 1.5
\end{eqnarray*}

La funci\'on de riesgo ajustada se expresa como:
\begin{eqnarray*}
\lambda(t \mid X) = \lambda_0(t) \exp(0.03 \cdot \text{edad} - 0.6 \cdot \text{sexo} + 1.5 \cdot \text{tratamiento})
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis multivariado de supervivencia permite evaluar el efecto conjunto de m\'ultiples covariables sobre el tiempo hasta el evento. La inclusi\'on de interacciones y efectos no lineales, junto con la selecci\'on adecuada de variables, mejora la precisi\'on y la interpretabilidad de los modelos de supervivencia.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Supervivencia en Datos Complicados}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}
El an\'alisis de supervivencia en datos complicados se refiere a la evaluaci\'on de datos de supervivencia que presentan desaf\'ios adicionales, como la censura por intervalo, datos truncados y datos con m\'ultiples tipos de eventos. Estos escenarios requieren m\'etodos avanzados para un an\'alisis adecuado.

\section{Censura por Intervalo}
La censura por intervalo ocurre cuando el evento de inter\'es se sabe que ocurri\'o dentro de un intervalo de tiempo, pero no se conoce el momento exacto. Esto es com\'un en estudios donde las observaciones se realizan en puntos de tiempo discretos.

\subsection{Modelo para Datos Censurados por Intervalo}
Para datos censurados por intervalo, la funci\'on de verosimilitud se modifica para incluir la probabilidad de que el evento ocurra dentro de un intervalo:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n P(T_i \in [L_i, U_i] \mid X_i; \beta)
\end{eqnarray*}
donde $[L_i, U_i]$ es el intervalo de tiempo durante el cual se sabe que ocurri\'o el evento para el individuo $i$.

\section{Datos Truncados}
Los datos truncados ocurren cuando los tiempos de supervivencia est\'an sujetos a un umbral, y solo se observan los individuos cuyos tiempos de supervivencia superan (o est\'an por debajo de) ese umbral. Existen dos tipos principales de truncamiento: truncamiento a la izquierda y truncamiento a la derecha.

\subsection{Modelo para Datos Truncados}
Para datos truncados a la izquierda, la funci\'on de verosimilitud se ajusta para considerar solo los individuos que superan el umbral de truncamiento:
\begin{eqnarray*}
L(\beta) = \prod_{i=1}^n \frac{f(t_i \mid X_i; \beta)}{1 - F(L_i \mid X_i; \beta)}
\end{eqnarray*}
donde $L_i$ es el umbral de truncamiento para el individuo $i$.

\section{An\'alisis de Competing Risks}
En estudios donde pueden ocurrir m\'ultiples tipos de eventos (competing risks), es crucial modelar adecuadamente el riesgo asociado con cada tipo de evento. La probabilidad de ocurrencia de cada evento compite con las probabilidades de ocurrencia de otros eventos.

\subsection{Modelo de Competing Risks}
Para un an\'alisis de competing risks, la funci\'on de riesgo se descompone en funciones de riesgo espec\'ificas para cada tipo de evento:
\begin{eqnarray*}
\lambda(t) = \sum_{j=1}^m \lambda_j(t)
\end{eqnarray*}
donde $\lambda_j(t)$ es la funci\'on de riesgo para el evento $j$.

\section{M\'etodos de Imputaci\'on}
Los m\'etodos de imputaci\'on se utilizan para manejar datos faltantes o censurados en estudios de supervivencia. La imputaci\'on m\'ultiple es un enfoque com\'un que crea m\'ultiples conjuntos de datos completos imputando valores faltantes varias veces y luego combina los resultados.

\subsection{Imputaci\'on M\'ultiple}
La imputaci\'on m\'ultiple para datos de supervivencia se realiza en tres pasos:
\begin{enumerate}
    \item Imputar los valores faltantes m\'ultiples veces para crear varios conjuntos de datos completos.
    \item Analizar cada conjunto de datos completo por separado utilizando m\'etodos de supervivencia est\'andar.
    \item Combinar los resultados de los an\'alisis separados para obtener estimaciones y varianzas combinadas.
\end{enumerate}

\section{Ejemplo de An\'alisis con Datos Complicados}
Consideremos un estudio con datos censurados por intervalo y competing risks. Ajustamos un modelo para los datos censurados por intervalo y obtenemos los siguientes coeficientes para las covariables edad y tratamiento:
\begin{eqnarray*}
\hat{\beta}_{edad} = 0.04, \quad \hat{\beta}_{tratamiento} = -0.8
\end{eqnarray*}

La funci\'on de supervivencia ajustada se expresa como:
\begin{eqnarray*}
S(t \mid X) = \exp\left(-\left(\frac{t \exp(0.04 \cdot \text{edad} - 0.8 \cdot \text{tratamiento})}{\lambda}\right)^k\right)
\end{eqnarray*}

\section{Conclusi\'on}
El an\'alisis de supervivencia en datos complicados requiere m\'etodos avanzados para manejar censura por intervalo, datos truncados y competing risks. La aplicaci\'on de modelos adecuados y m\'etodos de imputaci\'on asegura un an\'alisis preciso y completo de estos datos complejos.


%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\chapter{Proyecto Final y Revisi\'on}
%<--__--__--__--><--__--__--__--><--__--__--__--><--__--__--__-->
\section{Introducci\'on}
El proyecto final proporciona una oportunidad para aplicar los conceptos y t\'ecnicas aprendidas en el curso de an\'alisis de supervivencia. Este cap\'itulo incluye una gu\'ia para desarrollar un proyecto de an\'alisis de supervivencia y una revisi\'on de los conceptos clave.

\section{Desarrollo del Proyecto}
El proyecto final debe incluir los siguientes componentes:
\begin{enumerate}
    \item Definici\'on del problema: Identificar la pregunta de investigaci\'on y los objetivos del an\'alisis de supervivencia.
    \item Descripci\'on de los datos: Presentar los datos utilizados, incluyendo las covariables y la estructura de los datos.
    \item An\'alisis exploratorio: Realizar un an\'alisis descriptivo de los datos, incluyendo la censura y la distribuci\'on de los tiempos de supervivencia.
    \item Ajuste del modelo: Ajustar modelos de supervivencia adecuados (Kaplan-Meier, Cox, AFT) y evaluar su bondad de ajuste.
    \item Diagn\'ostico del modelo: Realizar diagn\'osticos para evaluar los supuestos del modelo y la influencia de observaciones individuales.
    \item Interpretaci\'on de resultados: Interpretar los coeficientes del modelo y las curvas de supervivencia ajustadas.
    \item Conclusiones: Resumir los hallazgos del an\'alisis y proporcionar recomendaciones basadas en los resultados.
\end{enumerate}

\section{Revisi\'on de Conceptos Clave}
Una revisi\'on de los conceptos clave del an\'alisis de supervivencia incluye:
\begin{itemize}
    \item \textbf{Funci\'on de Supervivencia:} Define la probabilidad de sobrevivir m\'as all\'a de un tiempo espec\'ifico.
    \item \textbf{Funci\'on de Riesgo:} Define la tasa instant\'anea de ocurrencia del evento.
    \item \textbf{Estimador de Kaplan-Meier:} Proporciona una estimaci\'on no param\'etrica de la funci\'on de supervivencia.
    \item \textbf{Test de Log-rank:} Compara curvas de supervivencia entre diferentes grupos.
    \item \textbf{Modelo de Cox:} Eval\'ua el efecto de m\'ultiples covariables sobre el tiempo hasta el evento, asumiendo proporcionalidad de riesgos.
    \item \textbf{Modelos AFT:} Modelan el efecto de las covariables multiplicando el tiempo de supervivencia por una constante.
    \item \textbf{An\'alisis Multivariado:} Considera interacciones y efectos no lineales entre m\'ultiples covariables.
    \item \textbf{Supervivencia en Datos Complicados:} Maneja censura por intervalo, datos truncados y competing risks.
\end{itemize}

\section{Ejemplo de Proyecto Final}
A continuaci\'on se presenta un ejemplo de estructura de un proyecto final de an\'alisis de supervivencia:

\subsection{Definici\'on del Problema}
Analizar el efecto del tratamiento y la edad sobre la supervivencia de pacientes con una enfermedad espec\'ifica.

\subsection{Descripci\'on de los Datos}
Datos de supervivencia de 100 pacientes, con covariables: edad, sexo y tipo de tratamiento. Los tiempos de supervivencia est\'an censurados a la derecha.

\subsection{An\'alisis Exploratorio}
Realizar histogramas y curvas de Kaplan-Meier para explorar la distribuci\'on de los tiempos de supervivencia y la censura.

\subsection{Ajuste del Modelo}
Ajustar un modelo de Cox y un modelo AFT con las covariables edad y tratamiento.

\subsection{Diagn\'ostico del Modelo}
Evaluar la proporcionalidad de riesgos y realizar an\'alisis de residuos para identificar observaciones influyentes.

\subsection{Interpretaci\'on de Resultados}
Interpretar los coeficientes del modelo y las curvas de supervivencia ajustadas para diferentes niveles de las covariables.

\subsection{Conclusiones}
Resumir los hallazgos y proporcionar recomendaciones sobre el efecto del tratamiento y la edad en la supervivencia de los pacientes.

\section{Conclusi\'on}
El proyecto final es una oportunidad para aplicar los conocimientos adquiridos en un contexto pr\'actico. La revisi\'on de los conceptos clave y la aplicaci\'on de t\'ecnicas adecuadas de an\'alisis de supervivencia aseguran un an\'alisis riguroso y significativo.







\end{document}
