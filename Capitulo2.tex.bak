\section{Introducci\'on}

Los fundamentos de probabilidad y estad\'istica son esenciales para comprender y aplicar t\'ecnicas de an\'alisis de datos y modelado estad\'istico, incluyendo la regresi\'on lineal y log\'istica. Este cap\'itulo proporciona una revisi\'on de los conceptos clave en probabilidad y estad\'istica que son relevantes para estos m\'etodos.

\section{Probabilidad}

La probabilidad es una medida de la incertidumbre o el grado de creencia en la ocurrencia de un evento. Los conceptos fundamentales incluyen:

\subsection{Espacio Muestral y Eventos}

El espacio muestral, denotado como $S$, es el conjunto de todos los posibles resultados de un experimento aleatorio. Un evento es un subconjunto del espacio muestral. Por ejemplo, si lanzamos un dado, el espacio muestral es:
\begin{eqnarray*}
S = \{1, 2, 3, 4, 5, 6\}
\end{eqnarray*}
Un evento podr\'ia ser obtener un n\'umero par:
\begin{eqnarray*}
E = \{2, 4, 6\}
\end{eqnarray*}

\subsection{Definiciones de Probabilidad}

Existen varias definiciones de probabilidad, incluyendo la probabilidad cl\'asica, la probabilidad frecuentista y la probabilidad bayesiana.

\subsubsection{Probabilidad Cl\'asica}

La probabilidad cl\'asica se define como el n\'umero de resultados favorables dividido por el n\'umero total de resultados posibles:
\begin{eqnarray*}
P(E) = \frac{|E|}{|S|}
\end{eqnarray*}
donde $|E|$ es el n\'umero de elementos en el evento $E$ y $|S|$ es el n\'umero de elementos en el espacio muestral $S$.

\subsubsection{Probabilidad Frecuentista}

La probabilidad frecuentista se basa en la frecuencia relativa de ocurrencia de un evento en un gran n\'umero de repeticiones del experimento:
\begin{eqnarray*}
P(E) = \lim_{n \to \infty} \frac{n_E}{n}
\end{eqnarray*}
donde $n_E$ es el n\'umero de veces que ocurre el evento $E$ y $n$ es el n\'umero total de repeticiones del experimento.

\subsubsection{Probabilidad Bayesiana}

La probabilidad bayesiana se interpreta como un grado de creencia actualizado a medida que se dispone de nueva informaci\'on. Se basa en el teorema de Bayes:
\begin{eqnarray*}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{eqnarray*}
donde $P(A|B)$ es la probabilidad de $A$ dado $B$, $P(B|A)$ es la probabilidad de $B$ dado $A$, $P(A)$ y $P(B)$ son las probabilidades de $A$ y $B$ respectivamente.

\section{Estad\'istica Bayesiana}

La estad\'istica bayesiana proporciona un enfoque coherente para el an\'alisis de datos basado en el teorema de Bayes. Los conceptos fundamentales incluyen:

\subsection{Prior y Posterior}

\subsubsection{Distribuci\'on Prior}

La distribuci\'on prior (apriori) representa nuestra creencia sobre los par\'ametros antes de observar los datos. Es una distribuci\'on de probabilidad que refleja nuestra incertidumbre inicial sobre los par\'ametros. Por ejemplo, si creemos que un par\'ametro $\theta$ sigue una distribuci\'on normal con media $\mu_0$ y varianza $\sigma_0^2$, nuestra prior ser\'ia:
\begin{eqnarray*}
P(\theta) = \frac{1}{\sqrt{2\pi\sigma_0^2}} e^{-\frac{(\theta-\mu_0)^2}{2\sigma_0^2}}
\end{eqnarray*}

\subsubsection{Verosimilitud}

La verosimilitud (likelihood) es la probabilidad de observar los datos dados los par\'ametros. Es una funci\'on de los par\'ametros $\theta$ dada una muestra de datos $X$:
\begin{eqnarray*}
L(\theta; X) = P(X|\theta)
\end{eqnarray*}
donde $X$ son los datos observados y $\theta$ son los par\'ametros del modelo.

\subsubsection{Distribuci\'on Posterior}

La distribuci\'on posterior (a posteriori) combina la informaci\'on de la prior y la verosimilitud utilizando el teorema de Bayes. Representa nuestra creencia sobre los par\'ametros despu\'es de observar los datos:
\begin{eqnarray*}
P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
\end{eqnarray*}
donde $P(\theta|X)$ es la distribuci\'on posterior, $P(X|\theta)$ es la verosimilitud, $P(\theta)$ es la prior y $P(X)$ es la probabilidad marginal de los datos.

La probabilidad marginal de los datos $P(X)$ se puede calcular como:
\begin{eqnarray*}
P(X) = \int_{\Theta} P(X|\theta)P(\theta) d\theta
\end{eqnarray*}
donde $\Theta$ es el espacio de todos los posibles valores del par\'ametro $\theta$.

\section{Distribuciones de Probabilidad}

Las distribuciones de probabilidad describen c\'omo se distribuyen los valores de una variable aleatoria. Existen distribuciones de probabilidad discretas y continuas.

\subsection{Distribuciones Discretas}

Una variable aleatoria discreta toma un n\'umero finito o contable de valores. Algunas distribuciones discretas comunes incluyen:

\subsubsection{Distribuci\'on Binomial}

La distribuci\'on binomial describe el n\'umero de \'exitos en una serie de ensayos de Bernoulli independientes y con la misma probabilidad de \'exito. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
\end{eqnarray*}
donde $X$ es el n\'umero de \'exitos, $n$ es el n\'umero de ensayos, $p$ es la probabilidad de \'exito en cada ensayo, y $\binom{n}{k}$ es el coeficiente binomial.

La funci\'on generadora de momentos (MGF) para la distribuci\'on binomial es:
\begin{eqnarray*}
M_X(t) = \left( 1 - p + pe^t \right)^n
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria binomial son:
\begin{eqnarray*}
E(X) &=& np \\
\text{Var}(X) &=& np(1-p)
\end{eqnarray*}

\subsubsection{Distribuci\'on de Poisson}

La distribuci\'on de Poisson describe el n\'umero de eventos que ocurren en un intervalo de tiempo fijo o en un \'area fija. La funci\'on de probabilidad es:
\begin{eqnarray*}
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
\end{eqnarray*}
donde $X$ es el n\'umero de eventos, $\lambda$ es la tasa media de eventos por intervalo, y $k$ es el n\'umero de eventos observados.

La funci\'on generadora de momentos (MGF) para la distribuci\'on de Poisson es:
\begin{eqnarray*}
M_X(t) = e^{\lambda (e^t - 1)}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria de Poisson son:
\begin{eqnarray*}
E(X) &=& \lambda \\
\text{Var}(X) &=& \lambda
\end{eqnarray*}

\subsection{Distribuciones Continuas}

Una variable aleatoria continua toma un n\'umero infinito de valores en un intervalo continuo. Algunas distribuciones continuas comunes incluyen:

\subsubsection{Distribuci\'on Normal}

La distribuci\'on normal, tambi\'en conocida como distribuci\'on gaussiana, es una de las distribuciones m\'as importantes en estad\'istica. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{eqnarray*}
donde $x$ es un valor de la variable aleatoria, $\mu$ es la media, y $\sigma$ es la desviaci\'on est\'andar.

La funci\'on generadora de momentos (MGF) para la distribuci\'on normal es:
\begin{eqnarray*}
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria normal son:
\begin{eqnarray*}
E(X) &=& \mu \\
\text{Var}(X) &=& \sigma^2
\end{eqnarray*}

\subsubsection{Distribuci\'on Exponencial}

La distribuci\'on exponencial describe el tiempo entre eventos en un proceso de Poisson. La funci\'on de densidad de probabilidad es:
\begin{eqnarray*}
f(x) = \lambda e^{-\lambda x}
\end{eqnarray*}
donde $x$ es el tiempo entre eventos y $\lambda$ es la tasa media de eventos.

La funci\'on generadora de momentos (MGF) para la distribuci\'on exponencial es:
\begin{eqnarray*}
M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{para } t < \lambda
\end{eqnarray*}

El valor esperado y la varianza de una variable aleatoria exponencial son:
\begin{eqnarray*}
E(X) &=& \frac{1}{\lambda} \\
\text{Var}(X) &=& \frac{1}{\lambda^2}
\end{eqnarray*}

\section{Estad\'istica Descriptiva}

La estad\'istica descriptiva resume y describe las caracter\'isticas de un conjunto de datos. Incluye medidas de tendencia central, medidas de dispersi\'on y medidas de forma.

\subsection{Medidas de Tendencia Central}

Las medidas de tendencia central incluyen la media, la mediana y la moda.

\subsubsection{Media}

La media aritm\'etica es la suma de los valores dividida por el n\'umero de valores:
\begin{eqnarray*}
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{eqnarray*}
donde $x_i$ son los valores de la muestra y $n$ es el tama\~no de la muestra.

\subsubsection{Mediana}

La mediana es el valor medio cuando los datos est\'an ordenados. Si el n\'umero de valores es impar, la mediana es el valor central. Si es par, es el promedio de los dos valores centrales.

\subsubsection{Moda}

La moda es el valor que ocurre con mayor frecuencia en un conjunto de datos.

\subsection{Medidas de Dispersi\'on}

Las medidas de dispersi\'on incluyen el rango, la varianza y la desviaci\'on est\'andar.

\subsubsection{Rango}

El rango es la diferencia entre el valor m\'aximo y el valor m\'inimo de los datos:
\begin{eqnarray*}
Rango = x_{\text{max}} - x_{\text{min}}
\end{eqnarray*}

\subsubsection{Varianza}

La varianza es la media de los cuadrados de las diferencias entre los valores y la media:
\begin{eqnarray*}
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{eqnarray*}

\subsubsection{Desviaci\'on Est\'andar}

La desviaci\'on est\'andar es la ra\'iz cuadrada de la varianza:
\begin{eqnarray*}
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{eqnarray*}

\section{Inferencia Estad\'istica}

La inferencia estad\'istica es el proceso de sacar conclusiones sobre una poblaci\'on a partir de una muestra. Incluye la estimaci\'on de par\'ametros y la prueba de hip\'otesis.

\subsection{Estimaci\'on de Par\'ametros}

La estimaci\'on de par\'ametros implica el uso de datos muestrales para estimar los par\'ametros de una poblaci\'on.

\subsubsection{Estimador Puntual}

Un estimador puntual proporciona un \'unico valor como estimaci\'on de un par\'ametro de la poblaci\'on. Por ejemplo, la media muestral $\bar{x}$ es un estimador puntual de la media poblacional $\mu$. Otros ejemplos de estimadores puntuales son:

\begin{itemize}
    \item \textbf{Mediana muestral ($\tilde{x}$)}: Estimador de la mediana poblacional.
    \item \textbf{Varianza muestral ($s^2$)}: Estimador de la varianza poblacional $\sigma^2$, definido como:
    \begin{eqnarray*}
    s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
    \end{eqnarray*}
    \item \textbf{Desviaci\'on est\'andar muestral ($s$)}: Estimador de la desviaci\'on est\'andar poblacional $\sigma$, definido como:
    \begin{eqnarray*}
    s = \sqrt{s^2}
    \end{eqnarray*}
\end{itemize}

\subsubsection{Propiedades de los Estimadores Puntuales}

Los estimadores puntuales deben cumplir ciertas propiedades deseables, como:

\begin{itemize}
    \item \textbf{Insesgadez}: Un estimador es insesgado si su valor esperado es igual al valor del par\'ametro que estima.
    \begin{eqnarray*}
    E(\hat{\theta}) = \theta
    \end{eqnarray*}
    \item \textbf{Consistencia}: Un estimador es consistente si converge en probabilidad al valor del par\'ametro a medida que el tama\~no de la muestra tiende a infinito.
    \item \textbf{Eficiencia}: Un estimador es eficiente si tiene la varianza m\'as baja entre todos los estimadores insesgados.
\end{itemize}

\subsubsection{Estimador por Intervalo}

Un estimador por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el par\'ametro poblacional con un cierto nivel de confianza. Por ejemplo, un intervalo de confianza para la media es:
\begin{eqnarray*}
\left( \bar{x} - z \frac{\sigma}{\sqrt{n}}, \bar{x} + z \frac{\sigma}{\sqrt{n}} \right)
\end{eqnarray*}
donde $z$ es el valor cr\'itico correspondiente al nivel de confianza deseado, $\sigma$ es la desviaci\'on est\'andar poblacional y $n$ es el tama\~no de la muestra.

\subsection{Prueba de Hip\'otesis}

La prueba de hip\'otesis es un procedimiento para decidir si una afirmaci\'on sobre un par\'ametro poblacional es consistente con los datos muestrales.

\subsubsection{Hip\'otesis Nula y Alternativa}

La hip\'otesis nula ($H_0$) es la afirmaci\'on que se somete a prueba, y la hip\'otesis alternativa ($H_a$) es la afirmaci\'on que se acepta si se rechaza la hip\'otesis nula.

\subsubsection{Nivel de Significancia}

El nivel de significancia ($\alpha$) es la probabilidad de rechazar la hip\'otesis nula cuando es verdadera. Un valor com\'unmente utilizado es $\alpha = 0.05$.

\subsubsection{Estad\'istico de Prueba}

El estad\'istico de prueba es una medida calculada a partir de los datos muestrales que se utiliza para decidir si se rechaza la hip\'otesis nula. Por ejemplo, en una prueba $t$ para la media:
\begin{eqnarray*}
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
\end{eqnarray*}
donde $\bar{x}$ es la media muestral, $\mu_0$ es la media poblacional bajo la hip\'otesis nula, $s$ es la desviaci\'on est\'andar muestral y $n$ es el tama\~no de la muestra.

\subsubsection{P-valor}

El p-valor es la probabilidad de obtener un valor del estad\'istico de prueba al menos tan extremo como el observado, bajo la suposici\'on de que la hip\'otesis nula es verdadera. Si el p-valor es menor que el nivel de significancia $\alpha$, se rechaza la hip\'otesis nula. El p-valor se interpreta de la siguiente manera:

\begin{itemize}
    \item \textbf{P-valor bajo (p < 0.05)}: Evidencia suficiente para rechazar la hip\'otesis nula.
    \item \textbf{P-valor alto (p > 0.05)}: No hay suficiente evidencia para rechazar la hip\'otesis nula.
\end{itemize}

\subsubsection{Tipos de Errores}

En la prueba de hip\'otesis, se pueden cometer dos tipos de errores:

\begin{itemize}
    \item \textbf{Error Tipo I ($\alpha$)}: Rechazar la hip\'otesis nula cuando es verdadera.
    \item \textbf{Error Tipo II ($\beta$)}: No rechazar la hip\'otesis nula cuando es falsa.
\end{itemize}

\subsubsection{Tabla de Errores en la Prueba de Hip\'otesis}

A continuaci\'on se presenta una tabla que muestra los posibles resultados en una prueba de hip\'otesis, incluyendo los falsos positivos (error tipo I) y los falsos negativos (error tipo II):

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Hip\'otesis Nula Verdadera} & \textbf{Hip\'otesis Nula Falsa} \\
\hline
\textbf{Rechazar $H_0$} & Error Tipo I ($\alpha$) & Aceptar $H_a$ \\
\hline
\textbf{No Rechazar $H_0$} & Aceptar $H_0$ & Error Tipo II ($\beta$) \\
\hline
\end{tabular}
\caption{Resultados de la Prueba de Hip\'otesis}
\label{tab:hypothesis_testing}
\end{table}

\section{Referencias y Bibliograf\'ia}

Para profundizar en los conceptos de probabilidad y estad\'istica, se recomiendan las siguientes referencias:

\begin{itemize}
    \item \textbf{Libros}:
    \begin{itemize}
        \item Ross, S. M. (2014). \textit{Introduction to Probability and Statistics for Engineers and Scientists}. Academic Press.
        \item DeGroot, M. H., and Schervish, M. J. (2012). \textit{Probability and Statistics} (4th ed.). Pearson.
        \item Hogg, R. V., McKean, J., and Craig, A. T. (2019). \textit{Introduction to Mathematical Statistics} (8th ed.). Pearson.
    \end{itemize}
    \item \textbf{Art\'iculos y Tutoriales}:
    \begin{itemize}
        \item Wasserman, L. (2004). \textit{All of Statistics: A Concise Course in Statistical Inference}. Springer.
        \item Probability and Statistics Tutorials on Khan Academy: \url{https://www.khanacademy.org/math/statistics-probability}
        \item Online Statistics Education: \url{http://onlinestatbook.com/}
    \end{itemize}
    \item \textbf{Cursos en L\'inea}:
    \begin{itemize}
        \item Coursera: \textit{Statistics with R} by Duke University.
        \item edX: \textit{Data Science: Probability} by Harvard University.
    \end{itemize}
\end{itemize}

Estas referencias proporcionan una base s\'olida para comprender y aplicar los conceptos discutidos en este cap\'itulo.
