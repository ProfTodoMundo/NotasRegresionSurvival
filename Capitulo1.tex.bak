\section{Conceptos B\'asicos de la Regresi\'on Log\'istica}

La regresi\'on log\'istica es una t\'ecnica de modelado estad\'istico utilizada para predecir la probabilidad de un evento binario (es decir, un evento que tiene dos posibles resultados) en funci\'on de una o m\'as variables independientes. Es ampliamente utilizada en diversas disciplinas, como medicina, econom\'ia, biolog\'ia, y ciencias sociales, para analizar y predecir resultados binarios.

Un modelo de regresi\'on log\'istica tiene la forma de una ecuaci\'on que describe c\'omo una variable dependiente binaria $Y$ (que puede tomar los valores $0$ o $1$) est\'a relacionada con una o m\'as variables independientes $X_1, X_2, \ldots, X_n$. A diferencia de la regresi\'on lineal, que predice un valor continuo, la regresi\'on log\'istica predice una probabilidad que puede ser interpretada como la probabilidad de que $Y=1$ dado un conjunto de valores para $X_1, X_2, \ldots, X_n$.

\section{Regresi\'on Lineal}

La regresi\'on lineal es una t\'ecnica de modelado estad\'istico utilizada para predecir el valor de una variable dependiente continua en funci\'on de una o m\'as variables independientes.

\subsection{Modelo Lineal}

El modelo de regresi\'on lineal tiene la forma:
\begin{eqnarray*}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
\end{eqnarray*}
donde:
\begin{itemize}
    \item $Y$ es la variable dependiente.
    \item $\beta_0$ es la intersecci\'on con el eje $Y$ o t\'ermino constante.
    \item $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes que representan la relaci\'on entre las variables independientes y la variable dependiente.
    \item $X_1, X_2, \ldots, X_n$ son las variables independientes.
    \item $\epsilon$ es el t\'ermino de error, que representa la desviaci\'on de los datos observados de los valores predichos por el modelo.
\end{itemize}

\subsection{M\'inimos Cuadrados Ordinarios (OLS)}

El objetivo de la regresi\'on lineal es encontrar los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos. Este m\'etodo se conoce como m\'inimos cuadrados ordinarios (OLS, por sus siglas en ingl\'es).

La funci\'on de costo que se minimiza es:
\begin{eqnarray*}
J\left(\beta_0, \beta_1, \ldots, \beta_n\right) = \sum_{i=1}^{n}\left(y_i - \hat{y}_i\right)^2
\end{eqnarray*}
donde:
\begin{itemize}
    \item $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on.
    \item $\hat{y}_i$ es el valor predecido por el modelo para la $i$-\'esima observaci\'on, dado por:
    \begin{eqnarray*}
    \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in}
    \end{eqnarray*}
\end{itemize}

Para encontrar los valores \'optimos de los coeficientes, se toman las derivadas parciales de la funci\'on de costo con respecto a cada coeficiente y se igualan a cero:
\begin{eqnarray*}
\frac{\partial J}{\partial \beta_j} = 0 \quad \text{para } j = 0, 1, \ldots, n
\end{eqnarray*}

Resolviendo este sistema de ecuaciones, se obtienen los valores de los coeficientes que minimizan la funci\'on de costo.

\section{Regresi\'on Log\'istica}

La deducci\'on de la f\'ormula de la regresi\'on log\'istica comienza con la necesidad de modelar la probabilidad de un evento binario. Queremos encontrar una funci\'on que relacione las variables independientes con la probabilidad de que la variable dependiente tome el valor $1$.

\subsection{Probabilidad y Odds}

La probabilidad de que el evento ocurra, $P(Y=1)$, se denota como $p$. La probabilidad de que el evento no ocurra, $P(Y=0)$, es $1-p$. Los \textit{odds} (chances) de que ocurra el evento se definen como:
\begin{eqnarray*}
\text{odds} = \frac{p}{1-p}
\end{eqnarray*}
Los \textit{odds} nos indican cu\'antas veces m\'as probable es que ocurra el evento frente a que no ocurra.

\subsection{Transformaci\'on Logit}

Para simplificar el modelado de los \textit{odds}, aplicamos el logaritmo natural, obteniendo la funci\'on logit:
\begin{eqnarray*}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right)
\end{eqnarray*}
La transformaci\'on logit es \'util porque convierte el rango de la probabilidad (0, 1) al rango de n\'umeros reales $\left(-\infty, \infty\right)$.

\subsection{Modelo Lineal en el Espacio Logit}

La idea clave de la regresi\'on log\'istica es modelar la transformaci\'on logit de la probabilidad como una combinaci\'on lineal de las variables independientes:
\begin{eqnarray*}
\text{logit}(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}
Aqu\'i, $\beta_0$ es el intercepto y $\beta_1, \beta_2, \ldots, \beta_n$ son los coeficientes asociados con las variables independientes $X_1, X_2, \ldots, X_n$.

\subsection{Invertir la Transformaci\'on Logit}

Para expresar $p$ en funci\'on de una combinaci\'on lineal de las variables independientes, invertimos la transformaci\'on logit. Partimos de la ecuaci\'on:
\begin{eqnarray*}
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\end{eqnarray*}
Aplicamos la exponenciaci\'on a ambos lados:
\begin{eqnarray*}
\frac{p}{1-p} = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}
\end{eqnarray*}
Despejamos $p$:
\begin{eqnarray*}
p = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n}}
\end{eqnarray*}

\subsection{Funci\'on Log\'istica}

La expresi\'on final que obtenemos es conocida como la funci\'on log\'istica:
\begin{eqnarray*}
p = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n\right)}}
\end{eqnarray*}
Esta funci\'on describe c\'omo las variables independientes se relacionan con la probabilidad de que el evento de inter\'es ocurra. Los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$ se estiman a partir de los datos utilizando el m\'etodo de m\'axima verosimilitud.

\section{M\'etodo de M\'axima Verosimilitud}

En la regresi\'on log\'istica, los coeficientes del modelo se estiman utilizando el m\'etodo de m\'axima verosimilitud. Este m\'etodo busca encontrar los valores de los coeficientes que maximicen la probabilidad de observar los datos dados los valores de las variables independientes.

\subsection{Funci\'on de Verosimilitud}

Para un conjunto de $n$ observaciones, la funci\'on de verosimilitud $L$ se define como el producto de las probabilidades individuales de observar cada dato:
\begin{eqnarray*}
L(\beta_0, \beta_1, \ldots, \beta_n) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
\end{eqnarray*}
donde $y_i$ es el valor observado de la variable dependiente para la $i$-\'esima observaci\'on y $p_i$ es la probabilidad predicha de que $Y_i = 1$.

Aqu\'i, $p_i$ es dado por la funci\'on log\'istica:
\begin{eqnarray*}
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in})}}
\end{eqnarray*}

\subsection{Funci\'on de Log-Verosimilitud}

Para simplificar los c\'alculos, trabajamos con el logaritmo de la funci\'on de verosimilitud, conocido como la funci\'on de log-verosimilitud. Tomar el logaritmo convierte el producto en una suma:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{eqnarray*}

Sustituyendo $p_i$:
\begin{eqnarray*}
\log L(\beta_0, \beta_1, \ldots, \beta_n) = \sum_{i=1}^{n} \left[ y_i (\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}) - \log(1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}) \right]
\end{eqnarray*}

\subsection{Maximizaci\'on de la Log-Verosimilitud}

El objetivo es encontrar los valores de $\beta_0, \beta_1, \ldots, \beta_n$ que maximicen la funci\'on de log-verosimilitud. Esto se hace derivando la funci\'on de log-verosimilitud con respecto a cada uno de los coeficientes y encontrando los puntos cr\'iticos.

Para $\beta_j$, la derivada parcial de la funci\'on de log-verosimilitud es:
\begin{eqnarray*}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} \left[ y_i X_{ij} - \frac{X_{ij} e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}}{1 + e^{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_n X_{in}}} \right]
\end{eqnarray*}

Esto se simplifica a:
\begin{eqnarray*}
\frac{\partial \log L}{\partial \beta_j} = \sum_{i=1}^{n} X_{ij} (y_i - p_i)
\end{eqnarray*}

Para maximizar la log-verosimilitud, resolvemos el sistema de ecuaciones $\frac{\partial \log L}{\partial \beta_j} = 0$ para todos los $j$ de 0 a $n$. Este sistema de ecuaciones no tiene una soluci\'on anal\'itica cerrada, por lo que se resuelve num\'ericamente utilizando m\'etodos iterativos como el algoritmo de Newton-Raphson.

\subsection{M\'etodo de Newton-Raphson}

El m\'etodo de Newton-Raphson es un algoritmo iterativo que se utiliza para encontrar las ra\'ices de una funci\'on. En el contexto de la regresi\'on log\'istica, se utiliza para maximizar la funci\'on de log-verosimilitud encontrando los valores de los coeficientes $\beta_0, \beta_1, \ldots, \beta_n$.

El m\'etodo de Newton-Raphson se basa en una aproximaci\'on de segundo orden de la funci\'on objetivo. Dado un valor inicial de los coeficientes $\beta^{(0)}$, se iterativamente actualiza el valor de los coeficientes utilizando la f\'ormula:
\begin{eqnarray*}
\beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
\end{eqnarray*}
donde:
\begin{itemize}
    \item $\beta^{(k)}$ es el vector de coeficientes en la $k$-\'esima iteraci\'on.
    \item $\mathbf{H}(\beta^{(k)})$ es la matriz Hessiana (matriz de segundas derivadas) evaluada en $\beta^{(k)}$.
    \item $\mathbf{g}(\beta^{(k)})$ es el gradiente (vector de primeras derivadas) evaluado en $\beta^{(k)}$.
\end{itemize}

\subsubsection{Gradiente}

El gradiente de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{g}(\beta) = \frac{\partial \log L}{\partial \beta} = \sum_{i=1}^{n} \mathbf{X}_i (y_i - p_i)
\end{eqnarray*}
donde $\mathbf{X}_i$ es el vector de valores de las variables independientes para la $i$-\'esima observaci\'on.

\subsubsection{Hessiana}

La matriz Hessiana de la funci\'on de log-verosimilitud con respecto a los coeficientes $\beta$ es:
\begin{eqnarray*}
\mathbf{H}(\beta) = \frac{\partial^2 \log L}{\partial \beta \partial \beta^T} = -\sum_{i=1}^{n} p_i (1 - p_i) \mathbf{X}_i \mathbf{X}_i^T
\end{eqnarray*}

\subsubsection{Algoritmo Newton-Raphson}

El algoritmo Newton-Raphson para la regresi\'on log\'istica se puede resumir en los siguientes pasos:
\begin{enumerate}
    \item Inicializar el vector de coeficientes $\beta^{(0)}$ (por ejemplo, con ceros o valores peque~nos aleatorios).
    \item Calcular el gradiente $\mathbf{g}(\beta^{(k)})$ y la matriz Hessiana $\mathbf{H}(\beta^{(k)})$ en la iteraci\'on $k$.
    \item Actualizar los coeficientes utilizando la f\'ormula:
    \begin{eqnarray*}
    \beta^{(k+1)} = \beta^{(k)} - \left[ \mathbf{H}(\beta^{(k)}) \right]^{-1} \mathbf{g}(\beta^{(k)})
    \end{eqnarray*}
    \item Repetir los pasos 2 y 3 hasta que la diferencia entre $\beta^{(k+1)}$ y $\beta^{(k)}$ sea menor que un umbral predefinido (criterio de convergencia).
\end{enumerate}

En resumen, el m\'etodo de Newton-Raphson permite encontrar los coeficientes que maximizan la funci\'on de log-verosimilitud de manera eficiente. Este m\'etodo es ampliamente utilizado en software estad\'istico para ajustar modelos de regresi\'on log\'istica.

\section{Implementaci\'on B\'asica en R}

Para implementar una regresi\'on log\'istica en R, primero es necesario instalar y cargar los paquetes necesarios. A continuaci\'on, se proporciona una gu\'ia paso a paso para configurar R y RStudio, as\'i como un ejemplo pr\'actico de implementaci\'on de un modelo de regresi\'on log\'istica.

\subsection{Instalaci\'on y Configuraci\'on de R y RStudio}
\begin{itemize}
    \item Descargue e instale R desde \texttt{https://cran.r-project.org/}. Siga las instrucciones para su sistema operativo (Windows, MacOS, Linux).
    \item Descargue e instale RStudio desde \texttt{https://rstudio.com/products/rstudio/download/}. RStudio es un entorno de desarrollo integrado (IDE) para R que facilita la escritura y ejecuci\'on de c\'odigo R.
\end{itemize}

\subsection{Introducci\'on B\'asica a R}

R es un lenguaje de programaci\'on y entorno de software para an\'alisis estad\'istico y gr\'afico. A continuaci\'on, se presentan algunos conceptos y comandos b\'asicos en R:

\begin{itemize}
    \item \textbf{Asignaci\'on}: En R, se puede asignar valores a variables utilizando el operador `<-`. Por ejemplo:
    \begin{verbatim}
    x <- 5
    y <- 10
    z <- x + y
    \end{verbatim}
    
    \item \textbf{Operaciones Aritm\'eticas}: R puede realizar operaciones aritm\'eticas b\'asicas como suma, resta, multiplicaci\'on y divisi\'on. Por ejemplo:
    \begin{verbatim}
    sum <- x + y
    diff <- x - y
    prod <- x * y
    quot <- x / y
    \end{verbatim}
    
    \item \textbf{Funciones B\'asicas}: R tiene muchas funciones integradas para realizar diversas tareas. Por ejemplo, se puede calcular la media y la desviaci\'on est\'andar de un conjunto de n\'umeros utilizando las funciones \texttt{mean()} y \texttt{sd()}:
    \begin{verbatim}
    numbers <- c(1, 2, 3, 4, 5)
    avg <- mean(numbers)
    sd_val <- sd(numbers)
    \end{verbatim}
\end{itemize}

\subsection{Ejemplo de Regresi\'on Log\'istica en R}

A continuaci\'on, se muestra un ejemplo de c\'omo ajustar un modelo de regresi\'on log\'istica en R utilizando un conjunto de datos simulado. El ejemplo incluye la instalaci\'on del paquete necesario, la carga de datos, el ajuste del modelo, y la interpretaci\'on de los resultados.

\begin{verbatim}
# Instalaci\'on del paquete necesario
install.packages("stats")

# Carga del paquete
library(stats)

# Ejemplo de conjunto de datos
data <- data.frame(
  outcome = c(1, 0, 1, 0, 1, 1, 0, 1, 0, 0),
  predictor = c(2.3, 1.9, 3.1, 2.8, 3.6, 2.4, 2.1, 3.3, 2.2, 1.7)
)

# Ajuste del modelo de regresi\'on log\'istica
model <- glm(outcome ~ predictor, data = data, family = binomial)

# Resumen del modelo
summary(model)
\end{verbatim}

En este ejemplo, se utiliza el conjunto de datos `data` que contiene una variable de resultado binaria `outcome` y una variable predictora continua `predictor`. El modelo de regresi\'on log\'istica se ajusta utilizando la funci\'on \texttt{glm} con la familia binomial. La funci\'on \texttt{summary(model)} proporciona un resumen del modelo ajustado, incluyendo los coeficientes estimados, sus errores est\'andar, valores z, y p-valores.

\begin{itemize}
    \item \textbf{Coeficientes}: Los coeficientes estimados $\beta_0$ y $\beta_1$ indican la direcci\'on y magnitud de la relaci\'on entre las variables predictoras y la probabilidad del resultado.
    \item \textbf{Errores Est\'andar}: Los errores est\'andar proporcionan una medida de la precisi\'on de los coeficientes estimados.
    \item \textbf{Valores z y p-valores}: Los valores z y p-valores se utilizan para evaluar la significancia estad\'istica de los coeficientes. Un p-valor peque~no (generalmente < 0.05) indica que el coeficiente es significativamente diferente de cero.
\end{itemize}

Este es solo un ejemplo b\'asico. En aplicaciones reales, es posible que necesites realizar m\'as an\'alisis y validaciones, como la evaluaci\'on de la bondad de ajuste del modelo, el diagn\'ostico de posibles problemas de multicolinealidad, y la validaci\'on cruzada del modelo.

\section{Referencias y Bibliograf\'ia}

Para profundizar en los conceptos de regresi\'on lineal y log\'istica, as\'i como en el m\'etodo de m\'axima verosimilitud y el algoritmo de Newton-Raphson, se recomiendan las siguientes referencias:

\begin{itemize}
    \item \textbf{Libros}:
    \begin{itemize}
        \item James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). \textit{An Introduction to Statistical Learning: with Applications in R}. Springer.
        \item Hosmer, D. W., Lemeshow, S., and Sturdivant, R. X. (2013). \textit{Applied Logistic Regression} (3rd ed.). Wiley.
        \item Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.
    \end{itemize}
    \item \textbf{Art\'iculos y Tutoriales}:
    \begin{itemize}
        \item Harrell, F. E. (2015). \textit{Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis}. Springer.
        \item R Documentation and Tutorials: \url{https://cran.r-project.org/manuals.html}
        \item Tutorials on R-bloggers: \url{https://www.r-bloggers.com/}
    \end{itemize}
    \item \textbf{Cursos en L\'inea}:
    \begin{itemize}
        \item Coursera: \textit{Machine Learning} by Andrew Ng.
        \item edX: \textit{Data Science and Machine Learning Essentials} by Microsoft.
    \end{itemize}
\end{itemize}

Estas referencias proporcionan una base s\'olida para comprender y aplicar los conceptos discutidos en este cap\'itulo.

